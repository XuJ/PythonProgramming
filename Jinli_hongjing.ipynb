{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算流程\n",
    "\n",
    "## 1. 整理源数据：将公司名(ID)与数据对应起来\n",
    "+ dw.qyxx_basic ： regcap\n",
    "+ qyxx_state_owned_enterprise_background\n",
    "+ \n",
    "\n",
    "\n",
    "## 2. 明确特征的计算函数\n",
    "\n",
    "+ 合并后的df的字段：\n",
    "\n",
    ">a\t----目标企业ID  \n",
    "b\t----投资方ID  \n",
    "c\t----被投资方ID  \n",
    "b_degree\t----投资方度数   \n",
    "c_degree\t----被投资方度数   \n",
    "bc_relation\t----投资方是否是投资关系   \n",
    "b_isperson\t----投资方是否是人  \n",
    "c_isperson  ----被投资方是否是人  \n",
    "a_name\t ----目标企业名称  \n",
    "b_name  ----投资方名称  \n",
    "c_name  ----被投资方名称  \n",
    "b_isSOcompany  ----投资方是否是国企  \n",
    "c_isSOcompany  ----被投资方是否是国企  \n",
    "a_isIPOcompany ----目标企业是否是上市  \n",
    "a_regcap  ----目标企业注册资本  \n",
    "a_realcap  ----目标企业实收资本  \n",
    ">a_zhuanli  ----目标企业专利数  \n",
    "a_shangbiao  ----目标企业商标  \n",
    ">a_ICP  ----目标企业备案信息  \n",
    "a_url  ----目标企业网站  \n",
    ">a_bgxx  ---目标企业变更信息【法定代表人变更次数、股东变更次数、注册资本变更次数、高管变更次数、经营范围变更次数】  \n",
    "a_bgxx_capital ----目标企业注册资本变更详情    \n",
    "a_recruit  ----目标企业招聘信息【招聘人员分布】  \n",
    "a_recruit_industry ----目标企业招聘信息【各行业招聘人员分布】   \n",
    "a_fzjg ---- 目标公司分支机构数   \n",
    ">a_zhaobiao  ----目标企业招标数【一年之内】  \n",
    "a_zhongbiao  ----目标企业中标数【一年之内】  \n",
    "a_namefrag ---- 目标公司字号    \n",
    "a_province_black_num ---- 目标公司所在省份黑企业走总数       \n",
    "a_province_total_num ---- 目标公司所在省份类金融总数    \n",
    "a_common_interests ----目标公司与黑名单库中任意一家企业存在利益一致行动关系的数量    \n",
    "a_common_address ----目标公司与黑名单库中任意一家企业地址相同的数量        \n",
    "b_province ---- 投资方省份    \n",
    "c_province ---- 被投资方省份    \n",
    "b_is_black_company ---- 投资方是否是黑企业    \n",
    "c_is_black_company ---- 被投资方是否是黑企业    \n",
    ">b_regtime  ----投资方注册时间   \n",
    "c_regtime  ----被投资方注册时间   \n",
    "b_ktgg  ----投资方开庭公告数  \n",
    "b_zgcpwsw  ----投资方裁判文书数  \n",
    "b_zgcpwsw_specific ----投资方裁判文书数【非法集资】    \n",
    "b_rmfygg  ----投资方法院公告  \n",
    "b_lending  ----投资方民间借贷数    \n",
    "c_ktgg  ----被投资方开庭公告数  \n",
    "c_zgcpwsw  ----被投资方裁判文书数  \n",
    "c_zgcpwsw_specific ----被投资方裁判文书数【非法集资】    \n",
    "c_rmfygg  ----被投资方法院公告  \n",
    "c_lending  ----被投资方民间借贷数    \n",
    "b_xzcf  ----投资方行政处罚数  \n",
    "c_xzcf  ----被投资方行政处罚数  \n",
    "b_zhixing  ----投资方执行人数  \n",
    "c_zhixing  ----被投资方执行人数  \n",
    "b_dishonesty  ----投资方失信被执行人数  \n",
    "c_dishonesty  ----被投资方失信被执行人数  \n",
    "b_jyyc  ----投资方经营异常数  \n",
    "c_jyyc  ----被投资方经营异常数  \n",
    "b_estatus  ----投资方经营状态  \n",
    "c_estatus  ----被投资方经营状态   \n",
    "b_circxzcf  ----投资方银监会行政处罚数  \n",
    "c_circxzcf  ----被投资方银监会行政处罚数  \n",
    ">b_opescope  ----投资方经营范围  \n",
    "c_opescope  ----被投资方经营范围  \n",
    "b_address  ----投资方地址  \n",
    "c_address  ----被投资方地址   \n",
    "b_is_fzjg ----投资方是否是分支机构    \n",
    "c_is_fzjg ----被投资方是否是分支机构    \n",
    "b_is_new_finance ----投资方是否是新金融企业    \n",
    "c_is_new_finance ----被投资方是否是新金融企业    \n",
    "b_isIPOcompany ----投资方是否是上市公司    \n",
    "c_isIPOcompany ----被投资方是否是上市公司    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 0 : 重新启一个sparksession，包含各种外部依赖（首先需要给定spark.driver.extraClassPath,与 JDBC的jar包，这里假如一个公司名的解析程序）\n",
    "'''\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from collections import OrderedDict\n",
    "\n",
    "spark.stop()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    conf = SparkConf()\n",
    "    conf.setMaster('yarn-client')\n",
    "    conf.set(\"spark.yarn.am.cores\", 15)\n",
    "    conf.set(\"spark.executor.memory\", \"50g\")\n",
    "    conf.set(\"spark.executor.instances\", 20)\n",
    "    conf.set(\"spark.executor.cores\", 10)\n",
    "    conf.set(\"spark.python.worker.memory\", \"3g\")\n",
    "    conf.set(\"spark.default.parallelism\", 3000)\n",
    "    conf.set(\"spark.sql.shuffle.partitions\", 3000)\n",
    "    conf.set(\"spark.broadcast.blockSize\", 1024)\n",
    "    conf.set(\"spark.sql.crossJoin.enabled\", True)\n",
    "    conf.set(\"spark.executor.extraJavaOptions\",\"-XX:+PrintGCDetails -XX:+PrintGCTimeStamps\")    \n",
    "    conf.set(\"spark.submit.pyFiles\", u'hdfs://bbdc6ha/user/antifraud/source/keyword_demo/dafei_keyword.py, hdfs://bbdc6ha/user/antifraud/source/company_type_for_capital_risk/wtyh_company_type_20160816.py')\n",
    "    conf.set(\"spark.files\", u'''hdfs://bbdc6ha/user/antifraud/source/keyword_demo/city,hdfs://bbdc6ha/user/antifraud/source/keyword_demo/1gram.words,hdfs://bbdc6ha/user/antifraud/source/keyword_demo/2gram.words,hdfs://bbdc6ha/user/antifraud/source/keyword_demo/new.work.words,/data5/antifraud/source/GM_release_LR.model''')\n",
    "    \n",
    "    \n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"glf_core_node\") \\\n",
    "        .config(conf = conf) \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCESS !!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "STEP 1：读取基础原数据\n",
    "'''\n",
    "from pyspark.sql import functions as fun\n",
    "from pyspark.sql import types as tp\n",
    "from pyspark.sql import Row\n",
    "import os\n",
    "\n",
    "#数据参数\n",
    "relation_version = '20180622'\n",
    "\n",
    "basic_version = '20180629'\n",
    "zhuanli_version = '20180629'\n",
    "shangbiao_version = '20180629'\n",
    "domain_website_version = '20180629'\n",
    "bgxx_version = '20180629'\n",
    "recruit_version = '20180629'\n",
    "zhaobiao_version = '20180629'\n",
    "zhongbiao_version = '20180629' \n",
    "ktgg_version = '20180629'\n",
    "zgcpwsw_version = '20180629'\n",
    "rmfygg_version = '20180629'\n",
    "xzcf_version = '20180629'\n",
    "zhixing_version = '20180629'\n",
    "dishonesty_version = '20180629'\n",
    "jyyc_version = '20180629'\n",
    "circxzcf_version = '20180629'\n",
    "fzjg_version = '20180629'\n",
    "\n",
    "state_owned_version = '20180117'\n",
    "black_version = '20170901'\n",
    "leijinrong_version = relation_version\n",
    "nf_version = relation_version\n",
    "so_version = state_owned_version\n",
    "\n",
    "#输入样本的参数\n",
    "sample_company_type = 'tar_company'\n",
    "sample_company_name = 'raw_jl_company_list_20180724_1.data'\n",
    "\n",
    "#中间结果版本\n",
    "tid_version = relation_version\n",
    "\n",
    "\n",
    "\n",
    "def add_col(one_col, two_col):\n",
    "    '''\n",
    "    将2个col合成一个元祖\n",
    "    '''\n",
    "    return (one_col, two_col)\n",
    "add_col_udf = fun.udf(add_col, tp.ArrayType(tp.StringType()))\n",
    "\n",
    "def to_dict(col):\n",
    "    '''\n",
    "    转换成字典\n",
    "    '''\n",
    "    return dict(col)\n",
    "to_dict_udf = fun.udf(to_dict, tp.MapType(tp.StringType(), tp.StringType()))\n",
    "\n",
    "#投资方、被投资方经营状态是否为吊销\n",
    "def is_not_revoked(col):\n",
    "    try:\n",
    "        if u'吊销' not in col and u'注销' not in col:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    except:\n",
    "        return 1\n",
    "is_not_revoked_udf = fun.udf(is_not_revoked, tp.IntegerType())\n",
    "\n",
    "def has_keyword(opescope):\n",
    "    '''\n",
    "    关键字匹配\n",
    "    '''\n",
    "    keywords_list_1 = [\n",
    "        u'民间借贷', u'民间融资', u'资产管理', u'养老', u'艺术品', u'生态农业',\n",
    "        u'养生', u'新能源', u'生物科技', u'环保科技', u'资产管理', u'投资管理', \n",
    "        u'基金管理', u'支付业务', u'互联网支付', u'电子支付', u'货币兑换', \n",
    "        u'移动电话支付', u'银行卡收单', u'预付卡受理', u'金融信息服务', \n",
    "        u'电子商务', u'投资咨询', u'投资管理', u'基金募集', u'基金销售',\n",
    "        u'资产管理', u'征信', u'经济贸易咨询', u'财务咨询', u'风险投资', \n",
    "        u'资产经营', u'众筹', u'财富管理',  u'纳米']\n",
    "    keywords_list_2 = [\n",
    "        u'投资', u'咨询', u'贸易', u'租赁', u'保理', u'交易场所', u'小额贷款', \n",
    "        u'担保', u'金融信息服务', u'网络科技', u'信息科技', u'信息技术']\n",
    "\n",
    "    keyword_dict = dict(\n",
    "        zip(keywords_list_1, len(keywords_list_1) * ['k_1']) + \n",
    "        zip(keywords_list_1, len(keywords_list_2) * ['k_2']))\n",
    "    \n",
    "    if opescope is not None:\n",
    "        for each_keyword, keyword_type in keyword_dict.iteritems():\n",
    "            if each_keyword in opescope:\n",
    "                return keyword_type\n",
    "        return 'k_0'\n",
    "    else:\n",
    "        return 'k_0'\n",
    "has_keyword_udf = fun.udf(has_keyword, tp.StringType())\n",
    "\n",
    "def get_change_num(s1, s2):\n",
    "    '''解析注册资本'''\n",
    "    \n",
    "    try:\n",
    "        import re\n",
    "        s1_analysis = re.search('[\\d\\.]+', s1).group()\n",
    "        s2_analysis = re.search('[\\d\\.]+', s2).group()\n",
    "        return round(float(s2_analysis) - float(s1_analysis), 1)\n",
    "    except:\n",
    "        return 0.\n",
    "get_change_num_udf = fun.udf(get_change_num, tp.FloatType())\n",
    "\n",
    "def get_company_namefrag(iterator):\n",
    "    '''\n",
    "    构建DAG；这里因为涉及到加载词典，只能用mapPartition，不然IO开销太大\n",
    "    '''\n",
    "    try:\n",
    "        from dafei_keyword import KeywordExtr\n",
    "        _obj = KeywordExtr(\"city\", \"1gram.words\", \"2gram.words\", \"new.work.words\")\n",
    "        keyword_list = []\n",
    "        for row in iterator:\n",
    "            keyword_list.append((row.company_name, _obj.clean(row.company_name)))\n",
    "        return keyword_list\n",
    "    except Exception, e:\n",
    "        return e\n",
    "\n",
    "#原始样本\n",
    "sample_df = spark.read.csv(\n",
    "    (\"/user/antifraud/jinli/rawdata/{company_type}/\"\n",
    "     \"/{company_name}\").format(\n",
    "        company_type=sample_company_type,\n",
    "        company_name=sample_company_name\n",
    "    ))\\\n",
    ".withColumnRenamed('_c0', 'company_name') \\\n",
    ".distinct()\n",
    "\n",
    "\n",
    "#原始关联方\n",
    "relation_df = spark.sql(\n",
    "    '''SELECT \n",
    "    bbd_qyxx_id                          a,\n",
    "    source_bbd_id                       b,\n",
    "    destination_bbd_id                c,\n",
    "    company_name                     a_name,\n",
    "    source_name                         b_name,\n",
    "    destination_name                  c_name,\n",
    "    source_degree                       b_degree,\n",
    "    destination_degree               c_degree,\n",
    "    relation_type                         bc_relation,\n",
    "    source_isperson                    b_isperson,\n",
    "    destination_isperson            c_isperson\n",
    "    FROM \n",
    "    dw.off_line_relations \n",
    "    WHERE \n",
    "    dt='{version}'  \n",
    "    AND\n",
    "    source_degree <= 3\n",
    "    AND\n",
    "    destination_degree <= 3\n",
    "    '''.format(version=relation_version)\n",
    ")\n",
    "\n",
    "#国企列表\n",
    "#url = \"jdbc:mysql://10.10.10.12:3306/bbd_higgs?characterEncoding=UTF-8\"\n",
    "#prop = {\"user\": \"reader\", \"password\":\"\", \"driver\": \"com.mysql.jdbc.Driver\"} \n",
    "#table = \"qyxx_state_owned_enterprise_background\"\n",
    "#so_df = spark.read.jdbc(url=url, table=table, properties=prop)\n",
    "#os.system(\"hadoop fs -rmr /user/antifraud/jinli/rawdata/qyxx_state_owned_enterprise_background/{version}\".format(version=so_version))\n",
    "#so_df.write.parquet(\"/user/antifraud/jinli/rawdata/qyxx_state_owned_enterprise_background/{version}\".format(version=so_version))\n",
    "so_df = spark.sql(\n",
    "    '''\n",
    "    SELECT\n",
    "    bbd_qyxx_id,\n",
    "    company_name,\n",
    "    company_type,\n",
    "    bbd_uptime,\n",
    "    bbd_dotime\n",
    "    FROM\n",
    "    dw.qyxx_state_owned_enterprise_background\n",
    "    WHERE\n",
    "    dt='{version}'  \n",
    "    '''.format(version=state_owned_version)\n",
    ")\n",
    "os.system(\n",
    "    (\"hadoop fs -rmr \"\n",
    "     \"{path}/\"\n",
    "     \"qyxx_state_owned_enterprise_background\"\n",
    "     \"/{version}\").format(version=state_owned_version,\n",
    "                         path='/user/antifraud/jinli/rawdata'))\n",
    "so_df.write.parquet(\n",
    "    (\"{path}/\"\n",
    "     \"qyxx_state_owned_enterprise_background\"\n",
    "     \"/{version}\").format(version=state_owned_version,\n",
    "                          path='/user/antifraud/jinli/rawdata'))\n",
    "\n",
    "\n",
    "\n",
    "#基础工商信息\n",
    "basic_df = spark.sql(\n",
    "    '''\n",
    "    SELECT\n",
    "    bbd_qyxx_id,\n",
    "    company_name,\n",
    "    ipo_company,\n",
    "    regcap_amount,\n",
    "    realcap_amount,\n",
    "    esdate,\n",
    "    operate_scope,\n",
    "    trim(address) address,\n",
    "    enterprise_status,\n",
    "    company_province\n",
    "    FROM\n",
    "    dw.qyxx_basic\n",
    "    WHERE\n",
    "    dt='{version}'  \n",
    "    '''.format(version=basic_version)\n",
    ").select(\n",
    "    'bbd_qyxx_id',\n",
    "    'company_name',\n",
    "    'ipo_company',\n",
    "    'regcap_amount',\n",
    "    'realcap_amount',\n",
    "    'esdate',\n",
    "    has_keyword_udf('operate_scope').alias('operate_scope'),\n",
    "    'address',\n",
    "    is_not_revoked_udf('enterprise_status').alias('enterprise_status'),\n",
    "    'company_province'\n",
    ").cache()\n",
    "os.system(\"hadoop fs -rmr /user/antifraud/jinli/rawdata/basic/{version}\".format(version=basic_version))\n",
    "basic_df.repartition(10).write.parquet(\"/user/antifraud/jinli/rawdata/basic/{version}\".format(version=basic_version))\n",
    "\n",
    "\n",
    "#专利信息\n",
    "zhuanli_count_df = spark.sql(\n",
    "    '''\n",
    "    SELECT \n",
    "    bbd_qyxx_id,\n",
    "    count(*) zhuanli_num\n",
    "    FROM\n",
    "    dw.qyxx_wanfang_zhuanli\n",
    "    WHERE\n",
    "    dt='{version}' \n",
    "    GROUP BY \n",
    "    bbd_qyxx_id\n",
    "    '''.format(version=zhuanli_version)\n",
    ")\n",
    "os.system(\"hadoop fs -rmr /user/antifraud/jinli/rawdata/zhuanli/{version}\".format(version=zhuanli_version))\n",
    "zhuanli_count_df.repartition(10).write.parquet(\"/user/antifraud/jinli/rawdata/zhuanli/{version}\".format(version=zhuanli_version))\n",
    "\n",
    "#商标信息\n",
    "shangbiao_count_df = spark.sql(\n",
    "    '''\n",
    "    SELECT \n",
    "    bbd_qyxx_id,\n",
    "    count(*) shangbiao_num\n",
    "    FROM\n",
    "    dw.xgxx_shangbiao\n",
    "    WHERE\n",
    "    dt='{version}' \n",
    "    GROUP BY \n",
    "    bbd_qyxx_id\n",
    "    '''.format(version=shangbiao_version)\n",
    ")\n",
    "os.system(\"hadoop fs -rmr /user/antifraud/jinli/rawdata/shangbiao/{version}\".format(version=shangbiao_version))\n",
    "shangbiao_count_df.repartition(10).write.parquet(\"/user/antifraud/jinli/rawdata/shangbiao/{version}\".format(version=shangbiao_version))\n",
    "\n",
    "#域名与网址\n",
    "domain_website_df = spark.sql(\n",
    "    '''\n",
    "    SELECT \n",
    "    bbd_qyxx_id,\n",
    "    homepage_url,\n",
    "    domain_name, \n",
    "    record_license\n",
    "    FROM\n",
    "    dw.domain_name_website_info\n",
    "    WHERE\n",
    "    dt='{version}' \n",
    "    '''.format(version=domain_website_version)\n",
    ")\n",
    "os.system(\"hadoop fs -rmr /user/antifraud/jinli/rawdata/domain_website/{version}\".format(version=domain_website_version))\n",
    "domain_website_df.repartition(10).write.parquet(\"/user/antifraud/jinli/rawdata/domain_website/{version}\".format(version=domain_website_version))\n",
    "\n",
    "\n",
    "#变更信息\n",
    "bgxx_df = spark.sql(\n",
    "    '''\n",
    "    SELECT \n",
    "    bbd_qyxx_id,\n",
    "    change_items,\n",
    "    count(*) change_num\n",
    "    FROM\n",
    "    dw.qyxx_bgxx\n",
    "    WHERE\n",
    "    dt='{version}' \n",
    "    AND\n",
    "    (change_items like '%高管%'  OR change_items like '%法定代表人%'  OR \n",
    "    change_items like '%股东%'  OR change_items like '%注册资本%'  OR\n",
    "    change_items like '%经营范围%' )\n",
    "    GROUP BY \n",
    "    bbd_qyxx_id, change_items\n",
    "    '''.format(version=bgxx_version)\n",
    ")\n",
    "bgxx_df = bgxx_df.withColumn('tid_tuple', add_col_udf('change_items', 'change_num')) \\\n",
    "    .groupBy('bbd_qyxx_id') \\\n",
    "    .agg({'tid_tuple': 'collect_list'}) \\\n",
    "    .withColumnRenamed('collect_list(tid_tuple)', 'tid_list') \\\n",
    "    .withColumn('bgxx_dict', to_dict_udf('tid_list')) \\\n",
    "    .select('bbd_qyxx_id', 'bgxx_dict')\n",
    "os.system(\"hadoop fs -rmr /user/antifraud/jinli/rawdata/bgxx/{version}\".format(version=bgxx_version))\n",
    "bgxx_df.repartition(10).write.parquet(\"/user/antifraud/jinli/rawdata/bgxx/{version}\".format(version=bgxx_version))\n",
    "\n",
    "\n",
    "\n",
    "#注册资本变更\n",
    "bgxx_capital_df = spark.sql(\n",
    "    '''\n",
    "    SELECT \n",
    "    bbd_qyxx_id,\n",
    "    change_date,\n",
    "    content_before_change, \n",
    "    content_after_change\n",
    "    FROM\n",
    "    dw.qyxx_bgxx\n",
    "    WHERE\n",
    "    dt='{version}' \n",
    "    AND\n",
    "    change_items like '%注册资本%' \n",
    "    '''.format(version=bgxx_version)\n",
    ")\n",
    "bgxx_capital_df = bgxx_capital_df.withColumn(\n",
    "    'tid_tuple', \n",
    "    add_col_udf(bgxx_capital_df.change_date.astype(tp.StringType()), \n",
    "                         get_change_num_udf('content_before_change', \n",
    "                                                            'content_after_change').alias('change_info'))\n",
    ") \\\n",
    "    .groupBy('bbd_qyxx_id') \\\n",
    "    .agg({'tid_tuple': 'collect_list'}) \\\n",
    "    .withColumnRenamed('collect_list(tid_tuple)', 'tid_list') \\\n",
    "    .select('bbd_qyxx_id', 'tid_list')\n",
    "os.system(\"hadoop fs -rmr /user/antifraud/jinli/rawdata/bgxx_capital/{version}\".format(version=bgxx_version))\n",
    "bgxx_capital_df.repartition(10).write.parquet(\"/user/antifraud/jinli/rawdata/bgxx_capital/{version}\".format(version=bgxx_version))\n",
    "\n",
    "\n",
    "#招聘信息\n",
    "#招聘学历分布\n",
    "recruit_df = spark.sql(\n",
    "    '''\n",
    "    SELECT \n",
    "    bbd_qyxx_id,\n",
    "    sum(bbd_recruit_num) bbd_recruit_num,\n",
    "    education_required\n",
    "    FROM\n",
    "    dw.recruit\n",
    "    WHERE\n",
    "    dt='{version}' \n",
    "    GROUP BY\n",
    "    bbd_qyxx_id, education_required\n",
    "    '''.format(version=recruit_version)\n",
    ")\n",
    "recruit_df = recruit_df.withColumn('tid_tuple', add_col_udf('education_required', 'bbd_recruit_num')) \\\n",
    "    .groupBy('bbd_qyxx_id') \\\n",
    "    .agg({'tid_tuple': 'collect_list'}) \\\n",
    "    .withColumnRenamed('collect_list(tid_tuple)', 'tid_list') \\\n",
    "    .withColumn('recruit_dict', to_dict_udf('tid_list')) \\\n",
    "    .select('bbd_qyxx_id', 'recruit_dict')\n",
    "os.system(\"hadoop fs -rmr /user/antifraud/jinli/rawdata/recruit/{version}\".format(version=recruit_version))\n",
    "recruit_df.repartition(10).write.parquet(\"/user/antifraud/jinli/rawdata/recruit/{version}\".format(version=recruit_version))\n",
    "\n",
    "#招聘行业分布\n",
    "recruit_industry_df = spark.sql(\n",
    "    '''\n",
    "    SELECT \n",
    "    bbd_qyxx_id,\n",
    "    sum(bbd_recruit_num) bbd_recruit_num,\n",
    "    bbd_industry\n",
    "    FROM\n",
    "    dw.recruit\n",
    "    WHERE\n",
    "    dt='{version}' \n",
    "    GROUP BY\n",
    "    bbd_qyxx_id, bbd_industry\n",
    "    '''.format(version=recruit_version)\n",
    ")\n",
    "recruit_industry_df = recruit_industry_df.withColumn('tid_tuple', add_col_udf('bbd_industry', 'bbd_recruit_num')) \\\n",
    "    .groupBy('bbd_qyxx_id') \\\n",
    "    .agg({'tid_tuple': 'collect_list'}) \\\n",
    "    .withColumnRenamed('collect_list(tid_tuple)', 'tid_list') \\\n",
    "    .withColumn('recruit_dict', to_dict_udf('tid_list')) \\\n",
    "    .select('bbd_qyxx_id', 'recruit_dict')\n",
    "os.system(\"hadoop fs -rmr /user/antifraud/jinli/rawdata/recruit_industry/{version}\".format(version=recruit_version))\n",
    "recruit_industry_df.repartition(\n",
    "    10\n",
    ").write.parquet(\n",
    "    \"/user/antifraud/jinli/rawdata/recruit_industry/{version}\".format(version=recruit_version)\n",
    ")\n",
    "\n",
    "\n",
    "#招标信息\n",
    "zhaobiao_df = spark.sql(\n",
    "    '''\n",
    "    SELECT \n",
    "    bbd_qyxx_id,\n",
    "    pubdate\n",
    "    FROM\n",
    "    dw.shgy_zhaobjg\n",
    "    WHERE\n",
    "    dt='{version}' \n",
    "    '''.format(version=zhaobiao_version)\n",
    ")\n",
    "zhaobiao_count_df = zhaobiao_df.where(\n",
    "    fun.date_add('pubdate', 365) > fun.current_date()) \\\n",
    ".where(zhaobiao_df.bbd_qyxx_id.isNotNull()) \\\n",
    ".groupBy('bbd_qyxx_id') \\\n",
    ".count() \\\n",
    ".withColumnRenamed('count', 'zhaobiao_num')\n",
    "os.system(\"hadoop fs -rmr /user/antifraud/jinli/rawdata/zhaobiao/{version}\".format(version=zhaobiao_version))\n",
    "zhaobiao_count_df.repartition(10).write.parquet(\"/user/antifraud/jinli/rawdata/zhaobiao/{version}\".format(version=zhaobiao_version))\n",
    "\n",
    "#中标信息\n",
    "zhongbiao_df = spark.sql(\n",
    "    '''\n",
    "    SELECT \n",
    "    bbd_qyxx_id,\n",
    "    pubdate\n",
    "    FROM\n",
    "    dw.shgy_zhaobjg\n",
    "    WHERE\n",
    "    dt='{version}' \n",
    "    '''.format(version=zhongbiao_version)\n",
    ")\n",
    "zhongbiao_count_df = zhongbiao_df.where(\n",
    "        fun.date_add('pubdate', 365) > fun.current_date()) \\\n",
    "    .where(zhongbiao_df.bbd_qyxx_id.isNotNull()) \\\n",
    "    .groupBy('bbd_qyxx_id') \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed('count', 'zhongbiao_num')\n",
    "os.system(\"hadoop fs -rmr /user/antifraud/jinli/rawdata/zhongbiao/{version}\".format(version=zhongbiao_version))\n",
    "zhongbiao_count_df.repartition(10).write.parquet(\"/user/antifraud/jinli/rawdata/zhongbiao/{version}\".format(version=zhongbiao_version))\n",
    "\n",
    "#开庭公告\n",
    "ktgg_df = spark.sql(\n",
    "    '''\n",
    "    SELECT \n",
    "    bbd_qyxx_id,\n",
    "    action_cause\n",
    "    FROM\n",
    "    dw.ktgg\n",
    "    WHERE\n",
    "    dt='{version}'\n",
    "    AND\n",
    "    bbd_qyxx_id is not null\n",
    "    AND\n",
    "    action_cause != 'NULL'\n",
    "    '''.format(version=ktgg_version)\n",
    ")\n",
    "ktgg_count_df = ktgg_df.groupBy('bbd_qyxx_id')\\\n",
    "    .count() \\\n",
    "    .withColumnRenamed('count', 'ktgg_num')\n",
    "os.system(\"hadoop fs -rmr /user/antifraud/jinli/rawdata/ktgg/{version}\".format(version=ktgg_version))\n",
    "ktgg_count_df.repartition(10).write.parquet(\"/user/antifraud/jinli/rawdata/ktgg/{version}\".format(version=ktgg_version))\n",
    "\n",
    "\n",
    "#裁判文书\n",
    "zgcpwsw_df = spark.sql(\n",
    "    '''\n",
    "    SELECT \n",
    "    bbd_qyxx_id,\n",
    "    action_cause\n",
    "    FROM\n",
    "    dw.zgcpwsw\n",
    "    WHERE\n",
    "    dt='{version}'\n",
    "    AND\n",
    "    bbd_qyxx_id is not null\n",
    "    AND\n",
    "    action_cause != 'NULL'\n",
    "    '''.format(version=zgcpwsw_version)\n",
    ")\n",
    "zgcpwsw_count_df = zgcpwsw_df.groupBy('bbd_qyxx_id') \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed('count', 'zgcpwsw_num')\n",
    "os.system(\"hadoop fs -rmr /user/antifraud/jinli/rawdata/zgcpwsw/{version}\".format(version=zgcpwsw_version))\n",
    "zgcpwsw_count_df.repartition(10).write.parquet(\"/user/antifraud/jinli/rawdata/zgcpwsw/{version}\".format(version=zgcpwsw_version))\n",
    "\n",
    "#非法集资案件的裁判文书\n",
    "zgcpwsw_specific_df = spark.sql(\n",
    "    '''\n",
    "    SELECT\n",
    "    bbd_qyxx_id,\n",
    "    action_cause,\n",
    "    litigant_type,\n",
    "    caseout_come \n",
    "    FROM\n",
    "    dw.zgcpwsw \n",
    "    WHERE\n",
    "    dt='{version}'\n",
    "    AND\n",
    "    (action_cause like '%非法吸收公众存款%' or action_cause like '%集资诈骗%')\n",
    "    '''.format(version=zgcpwsw_version)\n",
    ")\n",
    "zgcpwsw_specific_count_df = zgcpwsw_specific_df.groupBy('bbd_qyxx_id') \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed('count', 'zgcpwsw_specific_num')\n",
    "os.system(\"hadoop fs -rmr /user/antifraud/jinli/rawdata/zgcpwsw_specific/{version}\".format(version=zgcpwsw_version))\n",
    "zgcpwsw_specific_count_df.repartition(10).write.parquet(\n",
    "    \"/user/antifraud/jinli/rawdata/zgcpwsw_specific/{version}\".format(version=zgcpwsw_version))\n",
    "\n",
    "\n",
    "#法院公告\n",
    "rmfygg_df = spark.sql(\n",
    "    '''\n",
    "    SELECT \n",
    "    bbd_qyxx_id,\n",
    "    '' action_cause\n",
    "    FROM\n",
    "    dw.rmfygg\n",
    "    WHERE\n",
    "    dt='{version}'\n",
    "    AND\n",
    "    bbd_qyxx_id is not null\n",
    "    '''.format(version=rmfygg_version)\n",
    ")\n",
    "rmfygg_count_df = rmfygg_df.groupBy('bbd_qyxx_id') \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed('count', 'rmfygg_num')\n",
    "os.system(\"hadoop fs -rmr /user/antifraud/jinli/rawdata/rmfygg/{version}\".format(version=rmfygg_version))\n",
    "rmfygg_count_df.repartition(10).write.parquet(\"/user/antifraud/jinli/rawdata/rmfygg/{version}\".format(version=rmfygg_version))\n",
    "\n",
    "\n",
    "#民间借贷\n",
    "def filter_machine(col):\n",
    "    if u'民间借贷' in col:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "filter_machine_udf = fun.udf(filter_machine, tp.BooleanType())\n",
    "lawsuit_df = ktgg_df.union(zgcpwsw_df).union(rmfygg_df)\n",
    "lawsuit_count_df = lawsuit_df.where(filter_machine_udf('action_cause')) \\\n",
    "    .groupBy('bbd_qyxx_id') \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed('count', 'lawsuit_num')\n",
    "os.system(\"hadoop fs -rmr /user/antifraud/jinli/rawdata/lawsuit/{version}\".format(version=rmfygg_version))\n",
    "lawsuit_count_df.repartition(10).write.parquet(\"/user/antifraud/jinli/rawdata/lawsuit/{version}\".format(version=rmfygg_version))\n",
    "\n",
    "\n",
    "#行政处罚\n",
    "xzcf_df = spark.sql(\n",
    "    '''\n",
    "    SELECT\n",
    "    bbd_qyxx_id\n",
    "    FROM\n",
    "    dw.Xzcf\n",
    "    WHERE\n",
    "    dt='{version}'\n",
    "    '''.format(version=xzcf_version)\n",
    ")\n",
    "xzcf_count_df = xzcf_df.groupBy('bbd_qyxx_id') \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed('count', 'xzcf_num')\n",
    "os.system(\"hadoop fs -rmr /user/antifraud/jinli/rawdata/xzcf/{version}\".format(version=xzcf_version))\n",
    "xzcf_count_df.repartition(10).write.parquet(\"/user/antifraud/jinli/rawdata/xzcf/{version}\".format(version=xzcf_version))\n",
    "\n",
    "\n",
    "#被执行\n",
    "zhixing_df = spark.sql(\n",
    "    '''\n",
    "    SELECT\n",
    "    bbd_qyxx_id\n",
    "    FROM\n",
    "    dw.zhixing\n",
    "    WHERE\n",
    "    dt='{version}'\n",
    "    '''.format(version=zhixing_version)\n",
    ")\n",
    "zhixing_count_df = zhixing_df.groupBy('bbd_qyxx_id') \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed('count', 'zhixing_num')\n",
    "os.system(\"hadoop fs -rmr /user/antifraud/jinli/rawdata/zhixing/{version}\".format(version=zhixing_version))\n",
    "zhixing_count_df.repartition(10).write.parquet(\"/user/antifraud/jinli/rawdata/zhixing/{version}\".format(version=zhixing_version))\n",
    "\n",
    "\n",
    "#失信被执行\n",
    "dishonesty_df = spark.sql(\n",
    "    '''\n",
    "    SELECT\n",
    "    bbd_qyxx_id\n",
    "    FROM\n",
    "    dw.dishonesty\n",
    "    WHERE\n",
    "    dt='{version}'\n",
    "    '''.format(version=dishonesty_version)\n",
    ")\n",
    "dishonesty_count_df = dishonesty_df.groupBy('bbd_qyxx_id') \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed('count', 'dishonesty_num')\n",
    "os.system(\"hadoop fs -rmr /user/antifraud/jinli/rawdata/dishonesty/{version}\".format(version=dishonesty_version))\n",
    "dishonesty_count_df.repartition(10).write.parquet(\"/user/antifraud/jinli/rawdata/dishonesty/{version}\".format(version=dishonesty_version))\n",
    "\n",
    "\n",
    "#经营异常\n",
    "jyyc_df = spark.sql(\n",
    "    '''\n",
    "    SELECT\n",
    "    bbd_qyxx_id\n",
    "    FROM\n",
    "    dw.qyxg_jyyc\n",
    "    WHERE\n",
    "    dt='{version}'\n",
    "    '''.format(version=jyyc_version)\n",
    ")\n",
    "jyyc_count_df = jyyc_df.groupBy('bbd_qyxx_id') \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed('count', 'jyyc_num')\n",
    "os.system(\"hadoop fs -rmr /user/antifraud/jinli/rawdata/jyyc/{version}\".format(version=jyyc_version))\n",
    "jyyc_count_df.repartition(10).write.parquet(\"/user/antifraud/jinli/rawdata/jyyc/{version}\".format(version=jyyc_version))\n",
    "\n",
    "\n",
    "#银监会行政处罚\n",
    "circxzcf_df = spark.sql(\n",
    "    '''\n",
    "    SELECT\n",
    "    bbd_qyxx_id\n",
    "    FROM\n",
    "    dw.qyxg_circxzcf\n",
    "    WHERE\n",
    "    dt='{version}'\n",
    "    '''.format(version=circxzcf_version)\n",
    ")\n",
    "circxzcf_count_df = circxzcf_df.groupBy('bbd_qyxx_id') \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed('count', 'circxzcf_num')\n",
    "os.system(\"hadoop fs -rmr /user/antifraud/jinli/rawdata/circxzcf/{version}\".format(version=circxzcf_version))\n",
    "circxzcf_count_df.repartition(10).write.parquet(\"/user/antifraud/jinli/rawdata/circxzcf/{version}\".format(version=circxzcf_version))\n",
    "\n",
    "\n",
    "#分支机构\n",
    "fzjg_df = spark.sql(\n",
    "    '''\n",
    "    SELECT\n",
    "    bbd_qyxx_id,\n",
    "    name fzjg_name,\n",
    "    1 is_fzjg\n",
    "    FROM\n",
    "    dw.qyxx_fzjg_merge\n",
    "    WHERE\n",
    "    dt='{version}'\n",
    "    '''.format(version=fzjg_version)\n",
    ")\n",
    "fzjg_count_df = fzjg_df.groupBy('bbd_qyxx_id') \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed('count', 'fzjg_num')\n",
    "os.system(\"hadoop fs -rmr /user/antifraud/jinli/rawdata/fzjg/{version}\".format(version=fzjg_version))\n",
    "fzjg_count_df.repartition(10).write.parquet(\"/user/antifraud/jinli/rawdata/fzjg/{version}\".format(version=fzjg_version))\n",
    "\n",
    "os.system(\"hadoop fs -rmr /user/antifraud/jinli/rawdata/fzjg_name/{version}\".format(version=fzjg_version))\n",
    "fzjg_name_df = fzjg_df.select(\n",
    "    'fzjg_name',\n",
    "    'is_fzjg'\n",
    ").repartition(\n",
    "    10\n",
    ").write.parquet(\n",
    "    \"/user/antifraud/jinli/rawdata/fzjg_name/{version}\".format(version=fzjg_version)\n",
    ")\n",
    "\n",
    "\n",
    "#公司字号\n",
    "namefrag_df = sample_df.rdd.repartition(100).mapPartitions(get_company_namefrag).map(\n",
    "            lambda r: Row(company_name=r[0], namefrag=r[1])).toDF()\n",
    "os.system(\"hadoop fs -rmr /user/antifraud/jinli/rawdata/namefrag/{version}\".format(version=relation_version))\n",
    "namefrag_df.repartition(10).write.parquet(\"/user/antifraud/jinli/rawdata/namefrag/{version}\".format(version=relation_version))\n",
    "\n",
    "\n",
    "#黑企业名单\n",
    "black_df = spark.sql(\n",
    "    \"select company_name, 'black' company_type  from dw.qyxg_leijinrong_blacklist where dt={version}\".format(version=black_version)\n",
    ")\n",
    "#黑企业省份分布\n",
    "black_province_df  = black_df.join(\n",
    "    basic_df,\n",
    "    basic_df.company_name == black_df.company_name\n",
    ").select(\n",
    "    black_df.company_name,\n",
    "    basic_df.company_province\n",
    ").where(\n",
    "    basic_df.company_province != 'NULL'\n",
    ").groupBy(\n",
    "    'company_province'\n",
    ").agg(\n",
    "    {\"company_name\": \"count\"}\n",
    ").withColumnRenamed(\n",
    "    'count(company_name)', 'province_black_num'\n",
    ")\n",
    "os.system(\"hadoop fs -rmr /user/antifraud/jinli/rawdata/black_province/{version}\".format(version=black_version))\n",
    "black_province_df.repartition(10).write.parquet(\"/user/antifraud/jinli/rawdata/black_province/{version}\".format(version=black_version))\n",
    "\n",
    "\n",
    "#类金融名单\n",
    "leijinrong_df = spark.read.parquet(\n",
    "        \"/user/antifraud/hongjing2/dataflow/step_one/raw/ljr_sample/{version}\".format(version=leijinrong_version)\n",
    ")\n",
    "#类金融企业省份分布\n",
    "leijinrong_province_df  = leijinrong_df.join(\n",
    "    basic_df,\n",
    "    basic_df.company_name == leijinrong_df.company_name,\n",
    ").select(\n",
    "    basic_df.company_name,\n",
    "    basic_df.company_province\n",
    ").where(\n",
    "    basic_df.company_province != 'NULL'\n",
    ").groupBy(\n",
    "    'company_province'\n",
    ").agg(\n",
    "    {\"company_name\": \"count\"}\n",
    ").withColumnRenamed(\n",
    "    'count(company_name)', 'province_leijinrong_num'\n",
    ")\n",
    "os.system(\"hadoop fs -rmr /user/antifraud/jinli/rawdata/leijinrong_province/{version}\".format(version=leijinrong_version))\n",
    "leijinrong_province_df.repartition(10).write.parquet(\"/user/antifraud/jinli/rawdata/leijinrong_province/{version}\".format(version=leijinrong_version))\n",
    "print \"SUCESS !!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCESS !!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "STEP 2.0：新版关联方处理（投资与被投资关系）\n",
    "\n",
    "2.0：读取所以输入数据\n",
    "2.1：解析关联方，获取全量公司列表 \n",
    "    (sample_df + relation_df) -> tid_df\n",
    "2.2：将所有公司信息计算出来 \n",
    "    tid_df -> (tid_company_list_df + ...) -> tid_company_info_df\n",
    "2.3：根据样本列表构建属性图 \n",
    "    (sample_df + tid_df + tid_company_info_df) -> tid_company_merge_df\n",
    "\n",
    "'''\n",
    "from pyspark.sql import functions as fun\n",
    "from pyspark.sql import  types as tp\n",
    "\n",
    "is_new_finance_udf = fun.udf(lambda r: True, tp.BooleanType())\n",
    "\n",
    "#2.0 输入数据读取\n",
    "so_df = spark.read.parquet(\"/user/antifraud/jinli/rawdata/qyxx_state_owned_enterprise_background/{version}\".format(version=so_version))\n",
    "basic_df = spark.read.parquet(\"/user/antifraud/jinli/rawdata/basic/{version}\".format(version=basic_version)).cache()\n",
    "zhuanli_count_df = spark.read.parquet(\"/user/antifraud/jinli/rawdata/zhuanli/{version}\".format(version=zhuanli_version))\n",
    "shangbiao_count_df = spark.read.parquet(\"/user/antifraud/jinli/rawdata/shangbiao/{version}\".format(version=shangbiao_version))\n",
    "domain_website_df = spark.read.parquet(\"/user/antifraud/jinli/rawdata/domain_website/{version}\".format(version=domain_website_version))\n",
    "bgxx_df = spark.read.parquet(\"/user/antifraud/jinli/rawdata/bgxx/{version}\".format(version=bgxx_version))\n",
    "bgxx_capital_df = spark.read.parquet(\"/user/antifraud/jinli/rawdata/bgxx_capital/{version}\".format(version=bgxx_version))\n",
    "recruit_df = spark.read.parquet(\"/user/antifraud/jinli/rawdata/recruit/{version}\".format(version=recruit_version))\n",
    "recruit_industry_df = spark.read.parquet(\"/user/antifraud/jinli/rawdata/recruit_industry/{version}\".format(version=recruit_version))\n",
    "zhaobiao_count_df = spark.read.parquet(\"/user/antifraud/jinli/rawdata/zhaobiao/{version}\".format(version=zhaobiao_version))\n",
    "zhongbiao_count_df = spark.read.parquet(\"/user/antifraud/jinli/rawdata/zhongbiao/{version}\".format(version=zhongbiao_version))\n",
    "ktgg_count_df = spark.read.parquet(\"/user/antifraud/jinli/rawdata/ktgg/{version}\".format(version=ktgg_version))\n",
    "zgcpwsw_count_df = spark.read.parquet(\"/user/antifraud/jinli/rawdata/zgcpwsw/{version}\".format(version=zgcpwsw_version))\n",
    "zgcpwsw_specific_count_df = spark.read.parquet(\n",
    "    \"/user/antifraud/jinli/rawdata/zgcpwsw_specific/{version}\".format(version=zgcpwsw_version))\n",
    "rmfygg_count_df = spark.read.parquet(\"/user/antifraud/jinli/rawdata/rmfygg/{version}\".format(version=rmfygg_version))\n",
    "lawsuit_count_df = spark.read.parquet(\"/user/antifraud/jinli/rawdata/lawsuit/{version}\".format(version=rmfygg_version))\n",
    "xzcf_count_df = spark.read.parquet(\"/user/antifraud/jinli/rawdata/xzcf/{version}\".format(version=xzcf_version))\n",
    "zhixing_count_df = spark.read.parquet(\"/user/antifraud/jinli/rawdata/zhixing/{version}\".format(version=zhixing_version))\n",
    "dishonesty_count_df = spark.read.parquet(\"/user/antifraud/jinli/rawdata/dishonesty/{version}\".format(version=dishonesty_version))\n",
    "jyyc_count_df = spark.read.parquet(\"/user/antifraud/jinli/rawdata/jyyc/{version}\".format(version=jyyc_version))\n",
    "circxzcf_count_df = spark.read.parquet(\"/user/antifraud/jinli/rawdata/circxzcf/{version}\".format(version=circxzcf_version))\n",
    "fzjg_count_df = spark.read.parquet(\"/user/antifraud/jinli/rawdata/fzjg/{version}\".format(version=fzjg_version))\n",
    "fzjg_name_df = spark.read.parquet(\"/user/antifraud/jinli/rawdata/fzjg_name/{version}\".format(version=fzjg_version))\n",
    "namefrag_df = spark.read.parquet(\"/user/antifraud/jinli/rawdata/namefrag/{version}\".format(version=relation_version))\n",
    "black_province_df = spark.read.parquet(\"/user/antifraud/jinli/rawdata/black_province/{version}\".format(version=black_version))\n",
    "leijinrong_province_df = spark.read.parquet(\"/user/antifraud/jinli/rawdata/leijinrong_province/{version}\".format(version=leijinrong_version))\n",
    "new_finance_df = spark.read.parquet(\n",
    "    \"/user/antifraud/hongjing2/dataflow/step_one/raw/ljr_sample/{version}\".format(version=nf_version)\n",
    ").withColumn(\n",
    "    'is_new_finance', is_new_finance_udf('bbd_qyxx_id')\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#2.1 解析关联方，获取全量公司列表\n",
    "from  pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "def is_invest(edge_iter):\n",
    "    '''\n",
    "    将关联图中的各种关系变成：投资/非投资关系\n",
    "    '''\n",
    "    for each_edge in edge_iter:\n",
    "        if each_edge.bc_relation == 'INVEST':\n",
    "            return Row(\n",
    "                a=each_edge.a,\n",
    "                b=each_edge.b,\n",
    "                c=each_edge.c,\n",
    "                b_degree=each_edge.b_degree,\n",
    "                c_degree=each_edge.c_degree,\n",
    "                bc_relation=each_edge.bc_relation,\n",
    "                b_isperson=each_edge.b_isperson,\n",
    "                c_isperson=each_edge.c_isperson,\n",
    "                a_name=each_edge.a_name,\n",
    "                b_name=each_edge.b_name,\n",
    "                c_name=each_edge.c_name)\n",
    "    else:\n",
    "        return Row(\n",
    "            a=edge_iter.data[0].a,\n",
    "            b=edge_iter.data[0].b,\n",
    "            c=edge_iter.data[0].c,\n",
    "            b_degree=edge_iter.data[0].b_degree,\n",
    "            c_degree=edge_iter.data[0].c_degree,\n",
    "            bc_relation='UNINVEST',\n",
    "            b_isperson=edge_iter.data[0].b_isperson,\n",
    "            c_isperson=edge_iter.data[0].c_isperson,\n",
    "            a_name=edge_iter.data[0].a_name,\n",
    "            b_name=edge_iter.data[0].b_name,\n",
    "            c_name=edge_iter.data[0].c_name)\n",
    "            \n",
    "#关联方\n",
    "tid_df = sample_df.join(\n",
    "    relation_df,\n",
    "    fun.trim(relation_df.a_name) == fun.trim(sample_df.company_name),\n",
    "    'left_outer'\n",
    ").select(\n",
    "    'a','b','c','b_degree','c_degree','bc_relation' ,'b_isperson','c_isperson','a_name','b_name','c_name'\n",
    ")\n",
    "\n",
    "glf_schema = StructType([\n",
    "        StructField('a',StringType(),True),\n",
    "        StructField('b',StringType(),True),\n",
    "        StructField('c',StringType(),True),\n",
    "        StructField('b_degree',IntegerType(),True),\n",
    "        StructField('c_degree',IntegerType(),True),\n",
    "        StructField('bc_relation',StringType(),True),\n",
    "        StructField('b_isperson',IntegerType(),True),\n",
    "        StructField('c_isperson',IntegerType(),True),\n",
    "        StructField('a_name',StringType(),True),\n",
    "        StructField('b_name',StringType(),True),\n",
    "        StructField('c_name',StringType(),True)])\n",
    "\n",
    "tid_df = tid_df.rdd.map(lambda r: ((r.a, r.b, r.c), r)) \\\n",
    "    .groupByKey().mapValues(is_invest) \\\n",
    "    .map(lambda r: r[1]) \\\n",
    "    .toDF(schema=glf_schema) \\\n",
    "    .cache()\n",
    "    \n",
    "tid_company_list_df = tid_df.select(\n",
    "    'a', 'a_name'\n",
    ").union(\n",
    "    tid_df.where(\n",
    "        tid_df.b_isperson == 0\n",
    "    ).select(\n",
    "        'b', 'b_name'\n",
    "    )\n",
    ").union(\n",
    "    tid_df.where(\n",
    "        tid_df.c_isperson == 0\n",
    "    ).select(\n",
    "        'c', 'c_name'\n",
    "    )\n",
    ").withColumnRenamed(\n",
    "    'a', 'bbd_qyxx_id'\n",
    ").withColumnRenamed(\n",
    "    'a_name', 'company_name'\n",
    ").dropDuplicates(\n",
    "    ['bbd_qyxx_id']\n",
    ").cache()\n",
    "\n",
    "#2.2 合并所有公司的相关信息\n",
    "#国企\n",
    "tid_company_info_df = tid_company_list_df.join(\n",
    "    so_df,\n",
    "    tid_company_list_df.bbd_qyxx_id == so_df.bbd_qyxx_id,\n",
    "    'left_outer'\n",
    ").select(\n",
    "    tid_company_list_df.bbd_qyxx_id,\n",
    "    tid_company_list_df.company_name,\n",
    "    fun.when(so_df.company_type.isNotNull(), True).otherwise(False).alias('isSOcompany')\n",
    ")\n",
    "\n",
    "#基本信息\n",
    "tid_company_info_df = tid_company_info_df.join(\n",
    "    basic_df,\n",
    "    basic_df.bbd_qyxx_id == tid_company_info_df.bbd_qyxx_id,\n",
    "    'left_outer'\n",
    ").select(\n",
    "    tid_company_info_df.bbd_qyxx_id,\n",
    "    tid_company_info_df.company_name,\n",
    "    'isSOcompany',\n",
    "    fun.when(basic_df.ipo_company != 'NULL', True).otherwise(False).alias('isIPOcompany'),\n",
    "    basic_df.realcap_amount.alias('realcap'),\n",
    "    basic_df.regcap_amount.alias('regcap'),\n",
    "    basic_df.esdate.alias('regtime'),\n",
    "    basic_df.operate_scope.alias('opescope'),\n",
    "    basic_df.address.alias('address'),\n",
    "    basic_df.enterprise_status.alias('estatus'),\n",
    "    basic_df.company_province.alias('province')    \n",
    ")\n",
    "\n",
    "#专利\n",
    "tid_company_info_df = tid_company_info_df.join(\n",
    "    zhuanli_count_df,\n",
    "    tid_company_info_df.bbd_qyxx_id == zhuanli_count_df.bbd_qyxx_id,\n",
    "    'left_outer'\n",
    ").select(\n",
    "    tid_company_info_df.bbd_qyxx_id,\n",
    "    tid_company_info_df.company_name,\n",
    "    'isSOcompany', 'isIPOcompany', 'realcap',\n",
    "    'regcap', 'regtime', 'opescope',\n",
    "    'address', 'estatus', 'province',\n",
    "    zhuanli_count_df.zhuanli_num.alias('zhuanli')\n",
    ")\n",
    "\n",
    "#商标\n",
    "tid_company_info_df = tid_company_info_df.join(\n",
    "    shangbiao_count_df,\n",
    "    tid_company_info_df.bbd_qyxx_id == shangbiao_count_df.bbd_qyxx_id,\n",
    "    'left_outer'\n",
    ").select(\n",
    "    tid_company_info_df.bbd_qyxx_id,\n",
    "    tid_company_info_df.company_name,\n",
    "    'isSOcompany', 'isIPOcompany', 'realcap',\n",
    "    'regcap', 'regtime', 'opescope',\n",
    "    'address', 'estatus', 'province',\n",
    "    'zhuanli',\n",
    "    shangbiao_count_df.shangbiao_num.alias('shangbiao')\n",
    ")\n",
    "\n",
    "#域名与网址\n",
    "tid_company_info_df = tid_company_info_df.join(\n",
    "    domain_website_df,\n",
    "    tid_company_info_df.bbd_qyxx_id == domain_website_df.bbd_qyxx_id,\n",
    "    'left_outer'\n",
    ").select(\n",
    "    tid_company_info_df.bbd_qyxx_id,\n",
    "    tid_company_info_df.company_name,\n",
    "    'isSOcompany', 'isIPOcompany', 'realcap',\n",
    "    'regcap', 'regtime', 'opescope',\n",
    "    'address', 'estatus', 'province',\n",
    "    'zhuanli', 'shangbiao',\n",
    "    domain_website_df.homepage_url.alias('url'),\n",
    "    domain_website_df.record_license.alias('ICP')\n",
    ")\n",
    "\n",
    "#变更信息\n",
    "tid_company_info_df = tid_company_info_df.join(\n",
    "    bgxx_df,\n",
    "    tid_company_info_df.bbd_qyxx_id == bgxx_df.bbd_qyxx_id,\n",
    "    'left_outer'\n",
    ").select(\n",
    "    tid_company_info_df.bbd_qyxx_id,\n",
    "    tid_company_info_df.company_name,\n",
    "    'isSOcompany', 'isIPOcompany', 'realcap',\n",
    "    'regcap', 'regtime', 'opescope',\n",
    "    'address', 'estatus', 'province',\n",
    "    'zhuanli', 'shangbiao', 'url',\n",
    "    'ICP',\n",
    "    bgxx_df.bgxx_dict.alias('bgxx')\n",
    ")\n",
    "\n",
    "#招聘信息\n",
    "tid_company_info_df = tid_company_info_df.join(\n",
    "    recruit_df,\n",
    "    tid_company_info_df.bbd_qyxx_id == recruit_df.bbd_qyxx_id,\n",
    "    'left_outer'\n",
    ").select(\n",
    "    tid_company_info_df.bbd_qyxx_id,\n",
    "    tid_company_info_df.company_name,\n",
    "    'isSOcompany', 'isIPOcompany', 'realcap',\n",
    "    'regcap', 'regtime', 'opescope',\n",
    "    'address', 'estatus', 'province',\n",
    "    'zhuanli', 'shangbiao', 'url',\n",
    "    'ICP', 'bgxx', \n",
    "    recruit_df.recruit_dict.alias('recruit')\n",
    ")\n",
    "\n",
    "#招标信息\n",
    "tid_company_info_df = tid_company_info_df.join(\n",
    "    zhaobiao_count_df,\n",
    "    tid_company_info_df.bbd_qyxx_id == zhaobiao_count_df.bbd_qyxx_id,\n",
    "    'left_outer'\n",
    ").select(\n",
    "    tid_company_info_df.bbd_qyxx_id,\n",
    "    tid_company_info_df.company_name,\n",
    "    'isSOcompany', 'isIPOcompany', 'realcap',\n",
    "    'regcap', 'regtime', 'opescope',\n",
    "    'address', 'estatus', 'province',\n",
    "    'zhuanli', 'shangbiao', 'url',\n",
    "    'ICP', 'bgxx', 'recruit', \n",
    "    zhaobiao_count_df.zhaobiao_num.alias('zhaobiao') \n",
    ")\n",
    "\n",
    "#中标信息\n",
    "tid_company_info_df = tid_company_info_df.join(\n",
    "    zhongbiao_count_df,\n",
    "    tid_company_info_df.bbd_qyxx_id == zhongbiao_count_df.bbd_qyxx_id,\n",
    "    'left_outer'\n",
    ").select(\n",
    "    tid_company_info_df.bbd_qyxx_id,\n",
    "    tid_company_info_df.company_name,\n",
    "    'isSOcompany', 'isIPOcompany', 'realcap',\n",
    "    'regcap', 'regtime', 'opescope',\n",
    "    'address', 'estatus', 'province',\n",
    "    'zhuanli', 'shangbiao', 'url',\n",
    "    'ICP', 'bgxx', 'recruit', \n",
    "    'zhaobiao',\n",
    "    zhongbiao_count_df.zhongbiao_num.alias('zhongbiao') \n",
    ")\n",
    "\n",
    "#开庭公告\n",
    "tid_company_info_df = tid_company_info_df.join(\n",
    "    ktgg_count_df,\n",
    "    tid_company_info_df.bbd_qyxx_id == ktgg_count_df.bbd_qyxx_id,\n",
    "    'left_outer'\n",
    ").select(\n",
    "    tid_company_info_df.bbd_qyxx_id,\n",
    "    tid_company_info_df.company_name,\n",
    "    'isSOcompany', 'isIPOcompany', 'realcap',\n",
    "    'regcap', 'regtime', 'opescope',\n",
    "    'address', 'estatus', 'province',\n",
    "    'zhuanli', 'shangbiao', 'url',\n",
    "    'ICP', 'bgxx', 'recruit', \n",
    "    'zhaobiao', 'zhongbiao', \n",
    "    ktgg_count_df.ktgg_num.alias('ktgg') \n",
    ")\n",
    "\n",
    "#裁判文书\n",
    "tid_company_info_df = tid_company_info_df.join(\n",
    "    zgcpwsw_count_df,\n",
    "    tid_company_info_df.bbd_qyxx_id == zgcpwsw_count_df.bbd_qyxx_id,\n",
    "    'left_outer'\n",
    ").select(\n",
    "    tid_company_info_df.bbd_qyxx_id,\n",
    "    tid_company_info_df.company_name,\n",
    "    'isSOcompany', 'isIPOcompany', 'realcap',\n",
    "    'regcap', 'regtime', 'opescope',\n",
    "    'address', 'estatus', 'province',\n",
    "    'zhuanli', 'shangbiao', 'url',\n",
    "    'ICP', 'bgxx', 'recruit', \n",
    "    'zhaobiao', 'zhongbiao', 'ktgg',\n",
    "    zgcpwsw_count_df.zgcpwsw_num.alias('zgcpwsw') \n",
    ")\n",
    "\n",
    "#法院公告\n",
    "tid_company_info_df = tid_company_info_df.join(\n",
    "    rmfygg_count_df,\n",
    "    tid_company_info_df.bbd_qyxx_id == rmfygg_count_df.bbd_qyxx_id,\n",
    "    'left_outer'\n",
    ").select(\n",
    "    tid_company_info_df.bbd_qyxx_id,\n",
    "    tid_company_info_df.company_name,\n",
    "    'isSOcompany', 'isIPOcompany', 'realcap',\n",
    "    'regcap', 'regtime', 'opescope',\n",
    "    'address', 'estatus', 'province',\n",
    "    'zhuanli', 'shangbiao', 'url',\n",
    "    'ICP', 'bgxx', 'recruit', \n",
    "    'zhaobiao', 'zhongbiao', 'ktgg',\n",
    "    'zgcpwsw',\n",
    "    rmfygg_count_df.rmfygg_num.alias('rmfygg') \n",
    ")\n",
    "\n",
    "#民间借贷\n",
    "tid_company_info_df = tid_company_info_df.join(\n",
    "    lawsuit_count_df,\n",
    "    tid_company_info_df.bbd_qyxx_id == lawsuit_count_df.bbd_qyxx_id,\n",
    "    'left_outer'\n",
    ").select(\n",
    "    tid_company_info_df.bbd_qyxx_id,\n",
    "    tid_company_info_df.company_name,\n",
    "    'isSOcompany', 'isIPOcompany', 'realcap',\n",
    "    'regcap', 'regtime', 'opescope',\n",
    "    'address', 'estatus', 'province',\n",
    "    'zhuanli', 'shangbiao', 'url',\n",
    "    'ICP', 'bgxx', 'recruit', \n",
    "    'zhaobiao', 'zhongbiao', 'ktgg',\n",
    "    'zgcpwsw', 'rmfygg',\n",
    "    lawsuit_count_df.lawsuit_num.alias('lending') \n",
    ")\n",
    "\n",
    "#行政处罚\n",
    "tid_company_info_df = tid_company_info_df.join(\n",
    "    xzcf_count_df,\n",
    "    tid_company_info_df.bbd_qyxx_id == xzcf_count_df.bbd_qyxx_id,\n",
    "    'left_outer'\n",
    ").select(\n",
    "    tid_company_info_df.bbd_qyxx_id,\n",
    "    tid_company_info_df.company_name,\n",
    "    'isSOcompany', 'isIPOcompany', 'realcap',\n",
    "    'regcap', 'regtime', 'opescope',\n",
    "    'address', 'estatus', 'province',\n",
    "    'zhuanli', 'shangbiao', 'url',\n",
    "    'ICP', 'bgxx', 'recruit', \n",
    "    'zhaobiao', 'zhongbiao', 'ktgg',\n",
    "    'zgcpwsw', 'rmfygg', 'lending',\n",
    "    xzcf_count_df.xzcf_num.alias('xzcf') \n",
    ")\n",
    "\n",
    "#被执行\n",
    "tid_company_info_df = tid_company_info_df.join(\n",
    "    zhixing_count_df,\n",
    "    tid_company_info_df.bbd_qyxx_id == zhixing_count_df.bbd_qyxx_id,\n",
    "    'left_outer'\n",
    ").select(\n",
    "    tid_company_info_df.bbd_qyxx_id,\n",
    "    tid_company_info_df.company_name,\n",
    "    'isSOcompany', 'isIPOcompany', 'realcap',\n",
    "    'regcap', 'regtime', 'opescope',\n",
    "    'address', 'estatus', 'province',\n",
    "    'zhuanli', 'shangbiao', 'url',\n",
    "    'ICP', 'bgxx', 'recruit', \n",
    "    'zhaobiao', 'zhongbiao', 'ktgg', \n",
    "    'zgcpwsw', 'rmfygg', 'lending', 'xzcf', \n",
    "    zhixing_count_df.zhixing_num.alias('zhixing') \n",
    ")\n",
    "\n",
    "#失信被执行\n",
    "tid_company_info_df = tid_company_info_df.join(\n",
    "    dishonesty_count_df,\n",
    "    tid_company_info_df.bbd_qyxx_id == dishonesty_count_df.bbd_qyxx_id,\n",
    "    'left_outer'\n",
    ").select(\n",
    "    tid_company_info_df.bbd_qyxx_id,\n",
    "    tid_company_info_df.company_name,\n",
    "    'isSOcompany', 'isIPOcompany', 'realcap',\n",
    "    'regcap', 'regtime', 'opescope',\n",
    "    'address', 'estatus', 'province',\n",
    "    'zhuanli', 'shangbiao', 'url',\n",
    "    'ICP', 'bgxx', 'recruit', \n",
    "    'zhaobiao', 'zhongbiao', 'ktgg', \n",
    "    'zgcpwsw', 'rmfygg', 'lending', 'xzcf',\n",
    "    'zhixing', \n",
    "    dishonesty_count_df.dishonesty_num.alias('dishonesty') \n",
    ")\n",
    "\n",
    "#经营异常\n",
    "tid_company_info_df = tid_company_info_df.join(\n",
    "    jyyc_count_df,\n",
    "    tid_company_info_df.bbd_qyxx_id == jyyc_count_df.bbd_qyxx_id,\n",
    "    'left_outer'\n",
    ").select(\n",
    "    tid_company_info_df.bbd_qyxx_id,\n",
    "    tid_company_info_df.company_name,\n",
    "    'isSOcompany', 'isIPOcompany', 'realcap',\n",
    "    'regcap', 'regtime', 'opescope',\n",
    "    'address', 'estatus', 'province',\n",
    "    'zhuanli', 'shangbiao', 'url',\n",
    "    'ICP', 'bgxx', 'recruit', \n",
    "    'zhaobiao', 'zhongbiao', 'ktgg', \n",
    "    'zgcpwsw', 'rmfygg', 'lending', 'xzcf',\n",
    "    'zhixing', 'dishonesty', \n",
    "    jyyc_count_df.jyyc_num.alias('jyyc')\n",
    ")\n",
    "\n",
    "#银监会处罚\n",
    "tid_company_info_df = tid_company_info_df.join(\n",
    "    circxzcf_count_df,\n",
    "    tid_company_info_df.bbd_qyxx_id == circxzcf_count_df.bbd_qyxx_id,\n",
    "    'left_outer'\n",
    ").select(\n",
    "    tid_company_info_df.bbd_qyxx_id,\n",
    "    tid_company_info_df.company_name,\n",
    "    'isSOcompany', 'isIPOcompany', 'realcap',\n",
    "    'regcap', 'regtime', 'opescope',\n",
    "    'address', 'estatus', 'province',\n",
    "    'zhuanli', 'shangbiao', 'url',\n",
    "    'ICP', 'bgxx', 'recruit', \n",
    "    'zhaobiao', 'zhongbiao', 'ktgg', \n",
    "    'zgcpwsw', 'rmfygg', 'lending', 'xzcf',\n",
    "    'zhixing', 'dishonesty', 'jyyc',\n",
    "    circxzcf_count_df.circxzcf_num.alias('circxzcf')\n",
    ")\n",
    "\n",
    "#分支机构\n",
    "tid_company_info_df = tid_company_info_df.join(\n",
    "    fzjg_count_df,\n",
    "    tid_company_info_df.bbd_qyxx_id == fzjg_count_df.bbd_qyxx_id,\n",
    "    'left_outer'\n",
    ").select(\n",
    "    tid_company_info_df.bbd_qyxx_id,\n",
    "    tid_company_info_df.company_name,\n",
    "    'isSOcompany', 'isIPOcompany', 'realcap',\n",
    "    'regcap', 'regtime', 'opescope',\n",
    "    'address', 'estatus', 'province',\n",
    "    'zhuanli', 'shangbiao', 'url',\n",
    "    'ICP', 'bgxx', 'recruit', \n",
    "    'zhaobiao', 'zhongbiao', 'ktgg', \n",
    "    'zgcpwsw', 'rmfygg', 'lending', 'xzcf',\n",
    "    'zhixing', 'dishonesty', 'jyyc',\n",
    "    'circxzcf', \n",
    "    fzjg_count_df.fzjg_num.alias('fzjg')\n",
    ")\n",
    "\n",
    "#公司字号\n",
    "tid_company_info_df = tid_company_info_df.join(\n",
    "    namefrag_df,\n",
    "    tid_company_info_df.company_name == namefrag_df.company_name,\n",
    "    'left_outer'\n",
    ").select(\n",
    "    tid_company_info_df.bbd_qyxx_id,\n",
    "    tid_company_info_df.company_name,\n",
    "    'isSOcompany', 'isIPOcompany', 'realcap',\n",
    "    'regcap', 'regtime', 'opescope',\n",
    "    'address', 'estatus', 'province',\n",
    "    'zhuanli', 'shangbiao', 'url',\n",
    "    'ICP', 'bgxx', 'recruit', \n",
    "    'zhaobiao', 'zhongbiao', 'ktgg', \n",
    "    'zgcpwsw', 'rmfygg', 'lending', 'xzcf',\n",
    "    'zhixing', 'dishonesty', 'jyyc',\n",
    "    'circxzcf', 'fzjg',\n",
    "    namefrag_df.namefrag.alias('namefrag')\n",
    ")\n",
    "\n",
    "#某省份黑企业数\n",
    "tid_company_info_df = tid_company_info_df.join(\n",
    "    black_province_df,\n",
    "    tid_company_info_df.province == black_province_df.company_province,\n",
    "    'left_outer'\n",
    ").select(\n",
    "    tid_company_info_df.bbd_qyxx_id,\n",
    "    tid_company_info_df.company_name,\n",
    "    'isSOcompany', 'isIPOcompany', 'realcap',\n",
    "    'regcap', 'regtime', 'opescope',\n",
    "    'address', 'estatus', 'province',\n",
    "    'zhuanli', 'shangbiao', 'url',\n",
    "    'ICP', 'bgxx', 'recruit', \n",
    "    'zhaobiao', 'zhongbiao', 'ktgg', \n",
    "    'zgcpwsw', 'rmfygg', 'lending', 'xzcf',\n",
    "    'zhixing', 'dishonesty', 'jyyc',\n",
    "    'circxzcf', 'fzjg', 'namefrag',\n",
    "    black_province_df.province_black_num\n",
    ")\n",
    "\n",
    "#某省份类金融企业数\n",
    "tid_company_info_df = tid_company_info_df.join(\n",
    "    leijinrong_province_df,\n",
    "    tid_company_info_df.province == leijinrong_province_df.company_province,\n",
    "    'left_outer'\n",
    ").select(\n",
    "    tid_company_info_df.bbd_qyxx_id,\n",
    "    tid_company_info_df.company_name,\n",
    "    'isSOcompany', 'isIPOcompany', 'realcap',\n",
    "    'regcap', 'regtime', 'opescope',\n",
    "    'address', 'estatus', 'province',\n",
    "    'zhuanli', 'shangbiao', 'url',\n",
    "    'ICP', 'bgxx', 'recruit', \n",
    "    'zhaobiao', 'zhongbiao', 'ktgg', \n",
    "    'zgcpwsw', 'rmfygg', 'lending', 'xzcf',\n",
    "    'zhixing', 'dishonesty', 'jyyc',\n",
    "    'circxzcf', 'fzjg', 'namefrag',\n",
    "    'province_black_num',\n",
    "    leijinrong_province_df.province_leijinrong_num\n",
    ")\n",
    "\n",
    "#是否是黑企业\n",
    "tid_company_info_df = tid_company_info_df.join(\n",
    "    black_df,\n",
    "    tid_company_info_df.company_name == black_df.company_name,\n",
    "    'left_outer'\n",
    ").select(\n",
    "    tid_company_info_df.bbd_qyxx_id,\n",
    "    tid_company_info_df.company_name,\n",
    "    'isSOcompany', 'isIPOcompany', 'realcap',\n",
    "    'regcap', 'regtime', 'opescope',\n",
    "    'address', 'estatus', 'province',\n",
    "    'zhuanli', 'shangbiao', 'url',\n",
    "    'ICP', 'bgxx', 'recruit', \n",
    "    'zhaobiao', 'zhongbiao', 'ktgg', \n",
    "    'zgcpwsw', 'rmfygg', 'lending', 'xzcf',\n",
    "    'zhixing', 'dishonesty', 'jyyc',\n",
    "    'circxzcf', 'fzjg', 'namefrag',\n",
    "    'province_black_num', 'province_leijinrong_num',\n",
    "    fun.when(\n",
    "        black_df.company_type == 'black', True\n",
    "    ).otherwise(False).alias('is_black_company')\n",
    ")\n",
    "\n",
    "#是否是分支机构\n",
    "tid_company_info_df = tid_company_info_df.join(\n",
    "    fzjg_name_df,\n",
    "    fzjg_name_df.fzjg_name == tid_company_info_df.company_name,\n",
    "    'left_outer'\n",
    ").select(\n",
    "    tid_company_info_df.bbd_qyxx_id,\n",
    "    tid_company_info_df.company_name,\n",
    "    'isSOcompany', 'isIPOcompany', 'realcap',\n",
    "    'regcap', 'regtime', 'opescope',\n",
    "    'address', 'estatus', 'province',\n",
    "    'zhuanli', 'shangbiao', 'url',\n",
    "    'ICP', 'bgxx', 'recruit', \n",
    "    'zhaobiao', 'zhongbiao', 'ktgg', \n",
    "    'zgcpwsw', 'rmfygg', 'lending', 'xzcf',\n",
    "    'zhixing', 'dishonesty', 'jyyc',\n",
    "    'circxzcf', 'fzjg', 'namefrag',\n",
    "    'province_black_num', 'province_leijinrong_num',\n",
    "    'is_black_company',\n",
    "    fzjg_name_df.is_fzjg\n",
    ")\n",
    "\n",
    "#招聘行业数据\n",
    "tid_company_info_df = tid_company_info_df.join(\n",
    "    recruit_industry_df,\n",
    "    tid_company_info_df.bbd_qyxx_id == recruit_industry_df.bbd_qyxx_id,\n",
    "    'left_outer'\n",
    ").select(\n",
    "    tid_company_info_df.bbd_qyxx_id,\n",
    "    tid_company_info_df.company_name,\n",
    "    'isSOcompany', 'isIPOcompany', 'realcap',\n",
    "    'regcap', 'regtime', 'opescope',\n",
    "    'address', 'estatus', 'province',\n",
    "    'zhuanli', 'shangbiao', 'url',\n",
    "    'ICP', 'bgxx', 'recruit', \n",
    "    'zhaobiao', 'zhongbiao', 'ktgg', \n",
    "    'zgcpwsw', 'rmfygg', 'lending', 'xzcf',\n",
    "    'zhixing', 'dishonesty', 'jyyc',\n",
    "    'circxzcf', 'fzjg', 'namefrag',\n",
    "    'province_black_num', 'province_leijinrong_num',\n",
    "    'is_black_company', 'is_fzjg',\n",
    "    recruit_industry_df.recruit_dict.alias('recruit_industry')\n",
    ")\n",
    "\n",
    "#注册资本变更详情\n",
    "tid_company_info_df = tid_company_info_df.join(\n",
    "    bgxx_capital_df,\n",
    "    bgxx_capital_df.bbd_qyxx_id == tid_company_info_df.bbd_qyxx_id,\n",
    "    'left_outer'\n",
    ").select(\n",
    "    tid_company_info_df.bbd_qyxx_id,\n",
    "    tid_company_info_df.company_name,\n",
    "    'isSOcompany', 'isIPOcompany', 'realcap',\n",
    "    'regcap', 'regtime', 'opescope',\n",
    "    'address', 'estatus', 'province',\n",
    "    'zhuanli', 'shangbiao', 'url',\n",
    "    'ICP', 'bgxx', 'recruit', \n",
    "    'zhaobiao', 'zhongbiao', 'ktgg', \n",
    "    'zgcpwsw', 'rmfygg', 'lending', 'xzcf',\n",
    "    'zhixing', 'dishonesty', 'jyyc',\n",
    "    'circxzcf', 'fzjg', 'namefrag',\n",
    "    'province_black_num', 'province_leijinrong_num',\n",
    "    'is_black_company', 'is_fzjg', 'recruit_industry',\n",
    "    bgxx_capital_df.tid_list.alias('bgxx_capital')\n",
    ")\n",
    "\n",
    "#是否是新金融企业\n",
    "tid_company_info_df = tid_company_info_df.join(\n",
    "    new_finance_df,\n",
    "    new_finance_df.bbd_qyxx_id == tid_company_info_df.bbd_qyxx_id,\n",
    "    'left_outer'\n",
    ").select(\n",
    "    tid_company_info_df.bbd_qyxx_id,\n",
    "    tid_company_info_df.company_name,\n",
    "    'isSOcompany', 'isIPOcompany', 'realcap',\n",
    "    'regcap', 'regtime', 'opescope',\n",
    "    'address', 'estatus', 'province',\n",
    "    'zhuanli', 'shangbiao', 'url',\n",
    "    'ICP', 'bgxx', 'recruit', \n",
    "    'zhaobiao', 'zhongbiao', 'ktgg', \n",
    "    'zgcpwsw', 'rmfygg', 'lending', 'xzcf',\n",
    "    'zhixing', 'dishonesty', 'jyyc',\n",
    "    'circxzcf', 'fzjg', 'namefrag',\n",
    "    'province_black_num', 'province_leijinrong_num',\n",
    "    'is_black_company', 'is_fzjg', 'recruit_industry',\n",
    "    'bgxx_capital',\n",
    "    new_finance_df.is_new_finance.alias('is_new_finance')\n",
    ")\n",
    "\n",
    "#非法集资案件数\n",
    "tid_company_info_df = tid_company_info_df.join(\n",
    "    zgcpwsw_specific_count_df,\n",
    "    zgcpwsw_specific_count_df.bbd_qyxx_id == tid_company_info_df.bbd_qyxx_id,\n",
    "    'left_outer'\n",
    ").select(\n",
    "    tid_company_info_df.bbd_qyxx_id,\n",
    "    tid_company_info_df.company_name,\n",
    "    'isSOcompany', 'isIPOcompany', 'realcap',\n",
    "    'regcap', 'regtime', 'opescope',\n",
    "    'address', 'estatus', 'province',\n",
    "    'zhuanli', 'shangbiao', 'url',\n",
    "    'ICP', 'bgxx', 'recruit', \n",
    "    'zhaobiao', 'zhongbiao', 'ktgg', \n",
    "    'zgcpwsw', 'rmfygg', 'lending', 'xzcf',\n",
    "    'zhixing', 'dishonesty', 'jyyc',\n",
    "    'circxzcf', 'fzjg', 'namefrag',\n",
    "    'province_black_num', 'province_leijinrong_num',\n",
    "    'is_black_company', 'is_fzjg', 'recruit_industry',\n",
    "    'bgxx_capital', 'is_new_finance',\n",
    "    zgcpwsw_specific_count_df.zgcpwsw_specific_num.alias('zgcpwsw_specific')\n",
    ").cache()\n",
    "\n",
    "#某公司是否与黑名单库中任意一家企业存在利益一致行动关系（是1否0）\n",
    "#是否与黑名单库中任意一家企业地址相同（是1否0）\n",
    "def is_common_interests(frag, company_name):\n",
    "    '''\n",
    "    目标工公司是否与企业存在利益一致行动关系\n",
    "    '''\n",
    "    def is_similarity(str_1, str_2):\n",
    "        '''\n",
    "        判断两个字符串是否有连续2个字相同\n",
    "        '''\n",
    "        try:\n",
    "            token_1 = [\n",
    "                str_1[index] + str_1[index+1] \n",
    "                for index,data in enumerate(str_1) \n",
    "                if index < len(str_1) - 1]\n",
    "            is_similarity = sum([\n",
    "                    1 for each_token in token_1 \n",
    "                    if each_token in str_2])\n",
    "            return 1 if is_similarity > 0 else 0        \n",
    "        except:\n",
    "            return 0\n",
    "    return is_similarity(frag, company_name)\n",
    "is_common_interests_udf = fun.udf(is_common_interests, tp.IntegerType())\n",
    "    \n",
    "def is_common_address(address1, address2):\n",
    "    '''\n",
    "    是否有相同的address\n",
    "    '''\n",
    "    return 1 if address1 == address2 else 0\n",
    "is_common_address_udf = fun.udf(is_common_address, tp.IntegerType())\n",
    "    \n",
    "    \n",
    "all_company_namefrag_df = tid_company_info_df.select(\n",
    "    'company_name'\n",
    ").rdd.repartition(\n",
    "    100\n",
    ").mapPartitions(\n",
    "    get_company_namefrag\n",
    ").map(\n",
    "    lambda r: Row(company_name=r[0], namefrag=r[1])\n",
    ").toDF(\n",
    ").cache()\n",
    "    \n",
    "all_company_address_df = all_company_namefrag_df.join(\n",
    "    basic_df,\n",
    "    basic_df.company_name == all_company_namefrag_df.company_name,\n",
    "    'left_outer'\n",
    ").select(\n",
    "    basic_df.bbd_qyxx_id,\n",
    "    all_company_namefrag_df.namefrag,\n",
    "    basic_df.address.alias('all_company_address')\n",
    ").dropDuplicates(\n",
    "    ['bbd_qyxx_id']\n",
    ").cache()\n",
    "\n",
    "black_address_df = black_df.join(\n",
    "    basic_df,\n",
    "    basic_df.company_name == black_df.company_name,\n",
    "    'left_outer'\n",
    ").select(\n",
    "    black_df.company_name,\n",
    "    basic_df.address.alias('black_address')\n",
    ").dropDuplicates(\n",
    "    ['company_name']\n",
    ").cache()\n",
    "\n",
    "\n",
    "\n",
    "os.system(\"hadoop fs -rmr /user/antifraud/jinli/tmpdata/{version}\".format(version=tid_version))\n",
    "all_company_address_df.repartition(10).write.parquet(\"/user/antifraud/jinli/tmpdata/{version}/all_company_address_df\".format(version=tid_version))\n",
    "black_address_df.repartition(10).write.parquet(\"/user/antifraud/jinli/tmpdata/{version}/black_address_df\".format(version=tid_version))\n",
    "tid_company_info_df.repartition(10).write.parquet(\"/user/antifraud/jinli/tmpdata/{version}/tid_company_info_df\".format(version=tid_version))\n",
    "tid_df.repartition(10).write.parquet(\"/user/antifraud/jinli/tmpdata/{version}/tid_df\".format(version=tid_version))\n",
    "print 'SUCESS !!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCESS !!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "STEP 2.1：为了计算与企业相关的指标，必须将中间结果落地\n",
    "'''\n",
    "\n",
    "from pyspark.sql import functions as fun\n",
    "from pyspark.sql import  types as tp\n",
    "from pyspark.sql import  Row\n",
    "\n",
    "def is_common_interests(frag, company_name):\n",
    "    '''\n",
    "    目标工公司是否与企业存在利益一致行动关系\n",
    "    '''\n",
    "    def is_similarity(str_1, str_2):\n",
    "        '''\n",
    "        判断两个字符串是否有连续2个字相同\n",
    "        '''\n",
    "        try:\n",
    "            token_1 = [\n",
    "                str_1[index] + str_1[index+1] \n",
    "                for index,data in enumerate(str_1) \n",
    "                if index < len(str_1) - 1]\n",
    "            is_similarity = sum([\n",
    "                    1 for each_token in token_1 \n",
    "                    if each_token in str_2])\n",
    "            return 1 if is_similarity > 0 else 0        \n",
    "        except:\n",
    "            return 0\n",
    "    return is_similarity(frag, company_name)\n",
    "is_common_interests_udf = fun.udf(is_common_interests, tp.IntegerType())\n",
    "    \n",
    "def is_common_address(address1, address2):\n",
    "    '''\n",
    "    是否有相同的address\n",
    "    '''\n",
    "    return 1 if address1 == address2 else 0\n",
    "is_common_address_udf = fun.udf(is_common_address, tp.IntegerType())\n",
    "\n",
    "\n",
    "all_company_address_df = spark.read.parquet(\n",
    "    \"/user/antifraud/jinli/tmpdata/{version}/all_company_address_df\".format(version=tid_version)\n",
    ").dropDuplicates(\n",
    "    ['bbd_qyxx_id']\n",
    ")\n",
    "black_address_df = spark.read.parquet(\"/user/antifraud/jinli/tmpdata/{version}/black_address_df\".format(version=tid_version))\n",
    "\n",
    "\n",
    "os.system(\n",
    "    \"hadoop fs -rmr /user/antifraud/jinli/{version}/some_black_info_df\".format(version=tid_version)\n",
    ")\n",
    "some_black_info_df = all_company_address_df.join(\n",
    "    black_address_df\n",
    ").select(\n",
    "    'bbd_qyxx_id',\n",
    "    is_common_interests_udf('namefrag', 'company_name').alias('is_common_interests'),\n",
    "    is_common_address_udf('all_company_address', 'black_address').alias('is_common_address')\n",
    ").cache(\n",
    ").where(\n",
    "    'is_common_interests = 1 or  is_common_address = 1'\n",
    ").dropDuplicates(\n",
    "    ['bbd_qyxx_id']\n",
    ").repartition(\n",
    "    10\n",
    ").write.parquet(\n",
    "    \"/user/antifraud/jinli/tmpdata/{version}/some_black_info_df\".format(version=tid_version)\n",
    ")\n",
    "\n",
    "print 'SUCESS !!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCESS ！！\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "STEP 2.2：合并最终结果\n",
    "'''\n",
    "\n",
    "\n",
    "some_black_info_df = spark.read.parquet(\n",
    "    \"/user/antifraud/jinli/tmpdata/{version}/some_black_info_df\".format(version=tid_version)\n",
    ")\n",
    "tid_company_info_df = spark.read.parquet(\"/user/antifraud/jinli/tmpdata/{version}/tid_company_info_df\".format(version=tid_version))\n",
    "tid_df = spark.read.parquet(\"/user/antifraud/jinli/tmpdata/{version}/tid_df\".format(version=tid_version))\n",
    "#原始样本\n",
    "#sample_company_type = 'tar_company'\n",
    "#sample_company_name = 'raw_jl_company_list_20170531_1.data'\n",
    "sample_df = spark.read.csv(\n",
    "    (\"/user/antifraud/jinli/rawdata/{company_type}/\"\n",
    "     \"/{company_name}\").format(\n",
    "        company_type=sample_company_type,\n",
    "        company_name=sample_company_name\n",
    "    ))\\\n",
    ".withColumnRenamed('_c0', 'company_name') \\\n",
    ".distinct()\n",
    "\n",
    "\n",
    "tid_company_info_df = tid_company_info_df.join(\n",
    "    some_black_info_df,\n",
    "    some_black_info_df.bbd_qyxx_id == tid_company_info_df.bbd_qyxx_id,\n",
    "    'left_outer'\n",
    ").select(\n",
    "    tid_company_info_df.bbd_qyxx_id,\n",
    "    tid_company_info_df.company_name,\n",
    "    'isSOcompany', 'isIPOcompany', 'realcap',\n",
    "    'regcap', 'regtime', 'opescope',\n",
    "    'address', 'estatus', 'province',\n",
    "    'zhuanli', 'shangbiao', 'url',\n",
    "    'ICP', 'bgxx', 'recruit', \n",
    "    'zhaobiao', 'zhongbiao', 'ktgg', \n",
    "    'zgcpwsw', 'rmfygg', 'lending', 'xzcf',\n",
    "    'zhixing', 'dishonesty', 'jyyc',\n",
    "    'circxzcf', 'fzjg', 'namefrag',\n",
    "    'province_black_num', 'province_leijinrong_num',\n",
    "    'is_black_company', 'is_fzjg', 'recruit_industry',\n",
    "    'bgxx_capital', 'is_new_finance',\n",
    "    'zgcpwsw_specific', 'is_common_interests', \n",
    "    'is_common_address'\n",
    ")\n",
    "\n",
    "\n",
    "#结果输出\n",
    "tid_company_info_df = tid_company_info_df.dropDuplicates(\n",
    "    ['bbd_qyxx_id']\n",
    ")\n",
    "\n",
    "\n",
    "#2.3 根据样本列表构建属性图\n",
    "##获取关联方\n",
    "tid_company_merge_df = sample_df.join(\n",
    "    tid_df,\n",
    "    fun.trim(tid_df.a_name) == fun.trim(sample_df.company_name),\n",
    "    'left_outer'\n",
    ").select(\n",
    "    'a', 'b', 'c',\n",
    "    'b_degree', 'c_degree', 'bc_relation',\n",
    "    'b_isperson', 'c_isperson',\n",
    "    sample_df.company_name.alias('a_name'),\n",
    "    'b_name', 'c_name'\n",
    ")\n",
    "##目标公司信息\n",
    "tid_company_merge_df = tid_company_merge_df.join(\n",
    "    tid_company_info_df,\n",
    "    tid_company_info_df.company_name == tid_company_merge_df.a_name,\n",
    "    'left_outer'\n",
    ").select(\n",
    "    tid_company_info_df.bbd_qyxx_id.alias('a'), \n",
    "    'b', 'c',\n",
    "    'b_degree', 'c_degree', 'bc_relation',\n",
    "    'b_isperson', 'c_isperson', 'a_name',\n",
    "    'b_name', 'c_name',\n",
    "    tid_company_info_df.isIPOcompany.alias('a_isIPOcompany'), \n",
    "    tid_company_info_df.realcap.alias('a_realcap'),\n",
    "    tid_company_info_df.regcap.alias('a_regcap'), \n",
    "    tid_company_info_df.zhuanli.alias('a_zhuanli'), \n",
    "    tid_company_info_df.shangbiao.alias('a_shangbiao'), \n",
    "    tid_company_info_df.url.alias('a_url'), \n",
    "    tid_company_info_df.ICP.alias('a_ICP'), \n",
    "    tid_company_info_df.bgxx.alias('a_bgxx'), \n",
    "    tid_company_info_df.recruit.alias('a_recruit'), \n",
    "    tid_company_info_df.zhaobiao.alias('a_zhaobiao'), \n",
    "    tid_company_info_df.zhongbiao.alias('a_zhongbiao'), \n",
    "    tid_company_info_df.fzjg.alias('a_fzjg'), \n",
    "    tid_company_info_df.namefrag.alias('a_namefrag'), \n",
    "    tid_company_info_df.province_black_num.alias('a_province_black_num'), \n",
    "    tid_company_info_df.province_leijinrong_num.alias('a_province_leijinrong_num'),\n",
    "    tid_company_info_df.recruit_industry.alias('a_recruit_industry'),\n",
    "    tid_company_info_df.bgxx_capital.alias('a_bgxx_capital'),\n",
    "    tid_company_info_df.is_common_interests.alias('a_is_common_interests'),\n",
    "    tid_company_info_df.is_common_address.alias('a_is_common_address')\n",
    ")\n",
    "#投资方信息\n",
    "tid_company_merge_df = tid_company_merge_df.join(\n",
    "    tid_company_info_df,\n",
    "    tid_company_info_df.bbd_qyxx_id == tid_company_merge_df.b,\n",
    "    'left_outer'\n",
    ").select(\n",
    "    'a', 'b', 'c',\n",
    "    'b_degree', 'c_degree', 'bc_relation',\n",
    "    'b_isperson', 'c_isperson', 'a_name',\n",
    "    'b_name', 'c_name', 'a_isIPOcompany',\n",
    "    'a_realcap', 'a_regcap', 'a_zhuanli',\n",
    "    'a_shangbiao', 'a_url', 'a_ICP',\n",
    "    'a_bgxx', 'a_recruit', 'a_zhaobiao',\n",
    "    'a_zhongbiao', 'a_fzjg', 'a_namefrag',\n",
    "    'a_province_black_num', 'a_province_leijinrong_num',\n",
    "    'a_recruit_industry', 'a_bgxx_capital', \n",
    "    'a_is_common_interests', 'a_is_common_address',\n",
    "    tid_company_info_df.isSOcompany.alias('b_isSOcompany'),\n",
    "    tid_company_info_df.isSOcompany.alias('b_isIPOcompany'),\n",
    "    tid_company_info_df.is_black_company.alias('b_is_black_company'),\n",
    "    tid_company_info_df.regtime.alias('b_regtime'),\n",
    "    tid_company_info_df.ktgg.alias('b_ktgg'),\n",
    "    tid_company_info_df.zgcpwsw.alias('b_zgcpwsw'),\n",
    "    tid_company_info_df.rmfygg.alias('b_rmfygg'),\n",
    "    tid_company_info_df.lending.alias('b_lending'),\n",
    "    tid_company_info_df.xzcf.alias('b_xzcf'),\n",
    "    tid_company_info_df.zhixing.alias('b_zhixing'),\n",
    "    tid_company_info_df.dishonesty.alias('b_dishonesty'),\n",
    "    tid_company_info_df.jyyc.alias('b_jyyc'),\n",
    "    tid_company_info_df.estatus.alias('b_estatus'),\n",
    "    tid_company_info_df.circxzcf.alias('b_circxzcf'),\n",
    "    tid_company_info_df.opescope.alias('b_opescope'),\n",
    "    tid_company_info_df.address.alias('b_address'),\n",
    "    tid_company_info_df.province.alias('b_province'),\n",
    "    tid_company_info_df.is_fzjg.alias('b_is_fzjg'),\n",
    "    tid_company_info_df.is_new_finance.alias('b_is_new_finance'), \n",
    "    tid_company_info_df.zgcpwsw_specific.alias('b_zgcpwsw_specific'),\n",
    "    tid_company_info_df.is_common_interests.alias('b_is_common_interests'),\n",
    "    tid_company_info_df.is_common_address.alias('b_is_common_address')\n",
    ")\n",
    "#被投资方信息\n",
    "tid_company_merge_df = tid_company_merge_df.join(\n",
    "    tid_company_info_df,\n",
    "    tid_company_info_df.bbd_qyxx_id == tid_company_merge_df.c,\n",
    "    'left_outer'\n",
    ").select(\n",
    "    'a', 'b', 'c',\n",
    "    'b_degree', 'c_degree', 'bc_relation',\n",
    "    'b_isperson', 'c_isperson', 'a_name',\n",
    "    'b_name', 'c_name', 'a_isIPOcompany',\n",
    "    'a_realcap', 'a_regcap', 'a_zhuanli',\n",
    "    'a_shangbiao', 'a_url', 'a_ICP',\n",
    "    'a_bgxx', 'a_recruit', 'a_zhaobiao',\n",
    "    'a_zhongbiao', 'a_fzjg', 'a_namefrag',    \n",
    "    'a_province_black_num', 'a_province_leijinrong_num',\n",
    "    'a_recruit_industry', 'a_bgxx_capital',\n",
    "    'a_is_common_interests', 'a_is_common_address',\n",
    "    'b_isSOcompany' , 'b_isIPOcompany', 'b_is_black_company', 'b_regtime',\n",
    "    'b_ktgg', 'b_zgcpwsw', 'b_rmfygg', 'b_lending',   \n",
    "    'b_xzcf' , 'b_zhixing', 'b_dishonesty',   \n",
    "    'b_jyyc' , 'b_estatus', 'b_circxzcf',   \n",
    "    'b_opescope', 'b_address', 'b_province', 'b_is_fzjg',\n",
    "    'b_is_new_finance', 'b_zgcpwsw_specific',\n",
    "    'b_is_common_interests', 'b_is_common_address',\n",
    "    tid_company_info_df.isSOcompany.alias('c_isSOcompany'),\n",
    "    tid_company_info_df.isSOcompany.alias('c_isIPOcompany'),\n",
    "    tid_company_info_df.is_black_company.alias('c_is_black_company'),\n",
    "    tid_company_info_df.regtime.alias('c_regtime'),\n",
    "    tid_company_info_df.ktgg.alias('c_ktgg'),\n",
    "    tid_company_info_df.zgcpwsw.alias('c_zgcpwsw'),\n",
    "    tid_company_info_df.rmfygg.alias('c_rmfygg'),\n",
    "    tid_company_info_df.lending.alias('c_lending'),\n",
    "    tid_company_info_df.xzcf.alias('c_xzcf'),\n",
    "    tid_company_info_df.zhixing.alias('c_zhixing'),\n",
    "    tid_company_info_df.dishonesty.alias('c_dishonesty'),\n",
    "    tid_company_info_df.jyyc.alias('c_jyyc'),\n",
    "    tid_company_info_df.estatus.alias('c_estatus'),\n",
    "    tid_company_info_df.circxzcf.alias('c_circxzcf'),\n",
    "    tid_company_info_df.opescope.alias('c_opescope'),\n",
    "    tid_company_info_df.address.alias('c_address'),\n",
    "    tid_company_info_df.province.alias('c_province'),\n",
    "    tid_company_info_df.is_fzjg.alias('c_is_fzjg'),\n",
    "    tid_company_info_df.is_new_finance.alias('c_is_new_finance'),\n",
    "    tid_company_info_df.zgcpwsw_specific.alias('c_zgcpwsw_specific'),\n",
    "    tid_company_info_df.is_common_interests.alias('c_is_common_interests'),\n",
    "    tid_company_info_df.is_common_address.alias('c_is_common_address')\n",
    ")\n",
    "\n",
    "os.system(\"hadoop fs -rmr /user/antifraud/jinli/tiddata/relation_company_info_merge/{version}\".format(version=tid_version))\n",
    "tid_company_merge_df.repartition(10).write.parquet(\"/user/antifraud/jinli/tiddata/relation_company_info_merge/{version}\".format(version=tid_version))\n",
    "print \"SUCESS ！！\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS !! \n",
      " 耗时：30.3952291012\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "STEP 3：使用DataFrame完成数据计算（静态风险）\n",
    "'''\n",
    "import datetime\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "from operator import itemgetter, methodcaller\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "\n",
    "from pandas.io.json import json_normalize\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class FeatureConstruction(object):\n",
    "    '''\n",
    "    计算特征的函数集\n",
    "    '''      \n",
    "\n",
    "    def __fault_tolerant(func):\n",
    "        '''\n",
    "        一个用于容错的装饰器\n",
    "        '''\n",
    "        @classmethod\n",
    "        def wappen(cls, *args, **kwargs):\n",
    "            try:\n",
    "                return func(cls, *args, **kwargs)\n",
    "            except Exception, e:\n",
    "                return (\n",
    "                    \"{func_name} has a errr : {excp}\"\n",
    "                ).format(func_name=func.__name__, excp=e)\n",
    "        return wappen    \n",
    "    \n",
    "    @__fault_tolerant    \n",
    "    def create_graph_udf(cls, relations, is_directed):\n",
    "        '''\n",
    "        根据关联方的结构创建有向、无向图\n",
    "        '''\n",
    "        def init_graph(edge_list, node_list, is_directed=0):\n",
    "            #网络初始化\n",
    "            G = nx.DiGraph() if is_directed == 1 else nx.Graph()    \n",
    "            #增加带属性的节点\n",
    "            for node in node_list:\n",
    "                G.add_node(node[0], attr_dict=node[1])\n",
    "            #增加带属性的边\n",
    "            G.add_edges_from(edge_list)\n",
    "            return G\n",
    "            \n",
    "        #生成一个图\n",
    "        company_correlative_edges = [\n",
    "            (row.b, row.c, {'is_invest': row.bc_relation}) for row in relations]\n",
    "                                     \n",
    "        company_correlative_nodes = [(\n",
    "                row.b, \n",
    "                dict(\n",
    "                    is_human=row.b_isperson,\n",
    "                    is_black=row.b_is_black_company,\n",
    "                    distance = row.b_degree,\n",
    "                    name = row.b_name,\n",
    "                    isSOcompany = row.b_isSOcompany,\n",
    "                    isIPOcompany = row.b_isIPOcompany,\n",
    "                    esdate = row.b_regtime,\n",
    "                    ktgg = row.b_ktgg,\n",
    "                    zgcpwsw = row.b_zgcpwsw,\n",
    "                    rmfygg = row.b_rmfygg,\n",
    "                    lending = row.b_lending,\n",
    "                    xzcf = row.b_xzcf,\n",
    "                    zhixing = row.b_zhixing,\n",
    "                    dishonesty = row.b_dishonesty,\n",
    "                    jyyc = row.b_jyyc,\n",
    "                    circxzcf = row.b_circxzcf,\n",
    "                    opescope = row.b_opescope,\n",
    "                    address = row.b_address,\n",
    "                    estatus = row.b_estatus,\n",
    "                    province = row.b_province,\n",
    "                    is_fzjg=row.b_is_fzjg,\n",
    "                    is_new_finance=row.b_is_new_finance,\n",
    "                    zgcpwsw_specific=row.b_zgcpwsw_specific,\n",
    "                    common_interests=row.b_is_common_interests,\n",
    "                    common_address=row.b_is_common_address\n",
    "                ))  for row in relations] + [(\n",
    "                row.c, \n",
    "                dict(\n",
    "                    is_human=row.c_isperson,\n",
    "                    is_black=row.c_is_black_company,\n",
    "                    distance = row.c_degree,\n",
    "                    name = row.c_name,\n",
    "                    isSOcompany = row.c_isSOcompany,\n",
    "                    isIPOcompany = row.c_isIPOcompany,\n",
    "                    esdate = row.c_regtime,\n",
    "                    ktgg = row.c_ktgg,\n",
    "                    zgcpwsw = row.c_zgcpwsw,\n",
    "                    rmfygg = row.c_rmfygg,\n",
    "                    lending = row.c_lending,\n",
    "                    xzcf = row.c_xzcf,\n",
    "                    zhixing = row.c_zhixing,\n",
    "                    dishonesty = row.c_dishonesty,\n",
    "                    jyyc = row.c_jyyc,\n",
    "                    circxzcf = row.c_circxzcf,\n",
    "                    opescope = row.c_opescope,\n",
    "                    address = row.c_address,\n",
    "                    estatus = row.c_estatus,\n",
    "                    province = row.c_province,\n",
    "                    is_fzjg=row.c_is_fzjg,\n",
    "                    is_new_finance=row.c_is_new_finance,\n",
    "                    zgcpwsw_specific=row.c_zgcpwsw_specific,\n",
    "                    common_interests=row.c_is_common_interests,\n",
    "                    common_address=row.c_is_common_address\n",
    "                )) for row in relations]\n",
    "        \n",
    "        if is_directed == 1:\n",
    "            g = init_graph(\n",
    "                company_correlative_edges,\n",
    "                company_correlative_nodes, is_directed = 1) \n",
    "        else:\n",
    "            g = init_graph(\n",
    "                company_correlative_edges, \n",
    "                company_correlative_nodes, is_directed = 0)\n",
    "        return g\n",
    "\n",
    "    @__fault_tolerant\n",
    "    def get_feature_1(cls):\n",
    "        '''\n",
    "        企业背景风险\n",
    "        '''\n",
    "        one_relation_set = [\n",
    "            node for node, attr in cls.DIG.nodes_iter(data=True) if attr['distance'] == 1]\n",
    "        shareholder = [\n",
    "            src_node for src_node, des_node, edge_attr \n",
    "            in cls.DIG.edges_iter(one_relation_set, data=True) \n",
    "            if des_node == cls.tarcompany \n",
    "            and edge_attr['is_invest'] == 'INVEST']\n",
    "        os_shareholder = [\n",
    "            node for node in shareholder \n",
    "            if cls.DIG.node[node]['isSOcompany'] is True \n",
    "            and cls.DIG.node[node]['is_human'] == 0]\n",
    "        anti_os_shareholder = [\n",
    "            node for node in shareholder \n",
    "            if cls.DIG.node[node]['isSOcompany'] is False \n",
    "            and cls.DIG.node[node]['is_human'] == 0]\n",
    "        nature_shareholder = [\n",
    "            node for node in shareholder \n",
    "            if cls.DIG.node[node]['is_human'] == 1]\n",
    "        new_finance = [\n",
    "            node for ndoe in shareholder\n",
    "            if cls.DIG.node[node]['is_new_finance']]\n",
    "        ipo_company = [\n",
    "            node for ndoe in shareholder\n",
    "            if cls.DIG.node[node]['isIPOcompany']]\n",
    "        out_degree_obj = cls.DIG.out_degree()\n",
    "        if isinstance(out_degree_obj, dict):\n",
    "            out_degree_list = out_degree_obj.values()\n",
    "            out_degree_list.sort()\n",
    "        else:\n",
    "            out_degree_list = [0]\n",
    "        \n",
    "        is_ipo_tarcompany = 1 if cls.resultiterable.data[0].a_isIPOcompany is True else 0\n",
    "        is_common_interests = 1 if cls.resultiterable.data[0].a_is_common_interests else 0\n",
    "        is_common_address = 1 if cls.resultiterable.data[0].a_is_common_address else 0\n",
    "        \n",
    "        x = len(os_shareholder)\n",
    "        y = len(anti_os_shareholder)\n",
    "        z = len(nature_shareholder)\n",
    "        w = len(new_finance)\n",
    "        n = len(ipo_company)\n",
    "        r_i = is_ipo_tarcompany\n",
    "        r_1 = is_common_interests\n",
    "        r_2 = is_common_address\n",
    "        r_3 = out_degree_list[-1]\n",
    "        r_4 = sum(out_degree_list[-3:])\n",
    "        \n",
    "        r = (0.2*x + 0.5*y + z) * 100. * (2 - r_i) / (2*w + 0.001)\n",
    "        \n",
    "        return dict(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            z=z,\n",
    "            n=n,\n",
    "            w=w,\n",
    "            r_1=r_1,\n",
    "            r_2=r_2,\n",
    "            r_3=r_3,\n",
    "            r_4=r_4,\n",
    "            r_i=r_i,\n",
    "            r=round(r, 2) if r else 100\n",
    "        )\n",
    "\n",
    "\n",
    "    @__fault_tolerant\n",
    "    def get_feature_2(cls):\n",
    "        '''\n",
    "        1、资本风险\n",
    "        2、目标公司任意6个月内注册资本最大增额\n",
    "        '''\n",
    "        def get_date_density(difference_list, timedelta):\n",
    "            '''计算最大连续时间段，并返回他们的index'''\n",
    "            time_density_list = []\n",
    "            result = ''\n",
    "            for index, date in enumerate(difference_list):\n",
    "                if date < timedelta:\n",
    "                    s = 0\n",
    "                    indexes = [index]\n",
    "                    i = 1\n",
    "                    while(s < timedelta and i <= len(difference_list) - index):\n",
    "                        indexes.append(index+i)\n",
    "                        i += 1\n",
    "                        s = sum(difference_list[index:index+i])\n",
    "                    time_density_list.append(indexes)\n",
    "                else:\n",
    "                    continue\n",
    "            if time_density_list:\n",
    "                ma = 0\n",
    "                for each_data in time_density_list:\n",
    "                    if len(each_data) > ma:\n",
    "                        result = each_data\n",
    "                        ma = len(each_data)\n",
    "                    else:\n",
    "                        continue\n",
    "            return time_density_list\n",
    "    \n",
    "        def get_date_list(date_list):\n",
    "            '''获得时间差序列'''\n",
    "            date_strp_list = sorted(date_list)\n",
    "            if len(date_strp_list) > 1:\n",
    "                #构建时间差的序列，例如：[256, 4, 5, 1, 2, 33, 6, 5, 4, 73]\n",
    "                date_difference_list = [\n",
    "                    (date_strp_list[index + 1] - date_strp_list[index]).days \n",
    "                    for index in range(len(date_strp_list) -1)]\n",
    "            else:\n",
    "                date_difference_list = []\n",
    "            return date_difference_list\n",
    "\n",
    "                \n",
    "        captial_list = cls.resultiterable.data[0].a_bgxx_capital\n",
    "        result = []\n",
    "        \n",
    "        if captial_list and len(captial_list) > 1:\n",
    "            captial_list = sorted(captial_list, key=lambda r: r[0])\n",
    "            date_list = map(\n",
    "                lambda r: datetime.datetime.strptime(\n",
    "                    r[0], \"%Y-%m-%d\"), captial_list)\n",
    "            max_date_indexes = get_date_density(get_date_list(date_list), 180)\n",
    "            for each_index in max_date_indexes:  \n",
    "                result.append(\n",
    "                    abs(\n",
    "                        float(captial_list[each_index[-1]][1]) - \n",
    "                        float(captial_list[each_index[0]][1] )))\n",
    "        elif captial_list and len(captial_list) ==1:\n",
    "            result.append(float(captial_list[0][1]))\n",
    "            \n",
    "            \n",
    "        x = cls.resultiterable.data[0].a_regcap \n",
    "        y = cls.resultiterable.data[0].a_realcap\n",
    "        \n",
    "        x = x / 10000 if x != 'NULL' and x is not None else 0\n",
    "        y = y / 10000 if y != 'NULL' and y is not None else 0\n",
    "        \n",
    "        if 0 <= x < 500:\n",
    "            c_i = 100\n",
    "        elif 500 <= x < 1000:\n",
    "            c_i = 60\n",
    "        elif 1000 <= x < 5000:\n",
    "            c_i = 30\n",
    "        elif 5000 <= x:\n",
    "            c_i = 10\n",
    "        \n",
    "        p_i = 1 if y / (x + 0.001) >= 0.5 else 0\n",
    "        \n",
    "        return  dict(\n",
    "            x=x,\n",
    "            x_1=max(result) if result else 0,\n",
    "            y=y,\n",
    "            c=c_i * (1 - p_i/2.)\n",
    "        )\n",
    "    \n",
    "    @__fault_tolerant\n",
    "    def get_feature_3(cls):\n",
    "        '''\n",
    "        资本背景风险\n",
    "        '''\n",
    "        def get_relation_set(distance):\n",
    "            return [\n",
    "                attr['name']\n",
    "                for node, attr in cls.DIG.nodes_iter(data=True)\n",
    "                if attr['distance'] == distance]\n",
    "\n",
    "        from wtyh_company_type_20160816 import my_dict\n",
    "        import numpy as np\n",
    "        \n",
    "        ratio = np.array([1, 1, 0.75, 0.5])\n",
    "        reducing_score = np.array([-20, -40, -30, -15])\n",
    "        \n",
    "        #记录目标企业关联方中的所有企业类别\n",
    "        matrx = np.zeros([4, 4], dtype=int)\n",
    "        \n",
    "        #计算关联方的数据\n",
    "        degree_zero_list = get_relation_set(0)\n",
    "        degree_one_list = get_relation_set(1)\n",
    "        degree_two_list = get_relation_set(2)\n",
    "        degree_three_list = get_relation_set(3)\n",
    "        \n",
    "        for degree_zero_company in degree_zero_list:\n",
    "            if my_dict.has_key(degree_zero_company):\n",
    "                row_index = my_dict[degree_zero_company]\n",
    "                col_index = 0\n",
    "                matrx[row_index][col_index] += 1\n",
    "\n",
    "        \n",
    "        for degree_one_company in degree_one_list:\n",
    "            if my_dict.has_key(degree_one_company):\n",
    "                row_index = my_dict[degree_one_company]\n",
    "                col_index = 1\n",
    "                matrx[row_index][col_index] += 1    \n",
    "\n",
    "        \n",
    "        for degree_two_company in degree_two_list:\n",
    "            if my_dict.has_key(degree_two_company):\n",
    "                row_index = my_dict[degree_two_company]\n",
    "                col_index = 2\n",
    "                matrx[row_index][col_index] += 1    \n",
    "\n",
    "        \n",
    "        for degree_three_company in degree_three_list:\n",
    "            if my_dict.has_key(degree_three_company):\n",
    "                row_index = my_dict[degree_three_company]\n",
    "                col_index = 3\n",
    "                matrx[row_index][col_index] += 1\n",
    "\n",
    "        \n",
    "        #资本背景风险有一个特殊的判断条件：必须小于等于0\n",
    "        B = reducing_score.dot(matrx).dot(ratio) \n",
    "        B = B if B >= -30 else -30\n",
    "        \n",
    "        #各度的条件\n",
    "        data = matrx.sum(axis=1).tolist()\n",
    "        \n",
    "        return dict(\n",
    "            z=B + 30,\n",
    "            x_0=data[0],\n",
    "            x_1=data[1],\n",
    "            x_2=data[2],\n",
    "            x_3=data[3],\n",
    "        )\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    @__fault_tolerant\n",
    "    def get_feature_4(cls):\n",
    "        '''\n",
    "        知识产权风险（专利和商标）\n",
    "        '''\n",
    "        k_1 = cls.resultiterable.data[0].a_zhuanli\n",
    "        k_2 = cls.resultiterable.data[0].a_shangbiao \n",
    "        k_1 = k_1 if k_1 != 'NULL' and k_1 is not None else 0\n",
    "        k_2 = k_2 if k_1 != 'NULL' and k_2 is not None else 0\n",
    "        k_3 = k_1 + k_2\n",
    "        \n",
    "        return dict(\n",
    "            k_1=k_1,\n",
    "            k_2=k_2,\n",
    "            k=k_3\n",
    "        )\n",
    "        \n",
    "    @__fault_tolerant\n",
    "    def get_feature_5(cls):\n",
    "        '''\n",
    "        域名备案风险\n",
    "        '''\n",
    "        icp = cls.resultiterable.data[0].a_ICP\n",
    "        url = cls.resultiterable.data[0].a_url\n",
    "        \n",
    "        c_i = 1 if icp != 'NULL' and icp is not None else 0   \n",
    "        try:\n",
    "            if url is not None and url != 'NULL':\n",
    "                status_code = requests.get(url, allow_redirects=False).status_code \n",
    "                p_i = 1 if status_code == 200 else 0\n",
    "            else:\n",
    "                p_i = 0\n",
    "        except:\n",
    "            p_i = 0\n",
    "        \n",
    "        l = c_i * p_i\n",
    "        \n",
    "        return dict(\n",
    "            c_i=c_i,\n",
    "            p_i=p_i,\n",
    "            l=l\n",
    "        )\n",
    "        \n",
    "    @__fault_tolerant\n",
    "    def get_feature_6(cls):\n",
    "        '''\n",
    "        工商变更风险\n",
    "        '''\n",
    "        def get_bgxx_symbol(bgxx_name):\n",
    "            if u'法定代表人' in bgxx_name:\n",
    "                return 'c_1'\n",
    "            elif u'股东' in bgxx_name:\n",
    "                return 'c_2'\n",
    "            elif u'注册资本' in bgxx_name:\n",
    "                return 'c_3'\n",
    "            elif u'高管' in bgxx_name:\n",
    "                return 'c_4'\n",
    "            elif u'经营范围' in bgxx_name:\n",
    "                return 'c_5'\n",
    "            else:\n",
    "                return 'c_6'\n",
    "        \n",
    "        default_result = OrderedDict([\n",
    "                ('c_1', 0),\n",
    "                ('c_2', 0),\n",
    "                ('c_3', 0),\n",
    "                ('c_4', 0),\n",
    "                ('c_5', 0),\n",
    "                ('c_6', 0),\n",
    "                ('z', 0)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        if cls.resultiterable.data[0].a_bgxx is not None:\n",
    "            for bgxx_name, bgxx_num in cls.resultiterable.data[0].a_bgxx.iteritems():\n",
    "                default_result[get_bgxx_symbol(bgxx_name)] += int(bgxx_num)\n",
    "            default_result['z'] = np.dot(default_result.values(), [2, 1, 1, 1, 2, 0, 0])\n",
    "                \n",
    "        default_result.pop('c_6')\n",
    "        \n",
    "        return dict(default_result)\n",
    "        \n",
    "    @__fault_tolerant\n",
    "    def get_feature_7(cls):\n",
    "        '''\n",
    "        1、人才结构风险\n",
    "        2、目标公司招聘行业分布\n",
    "        '''        \n",
    "        def get_recruit_num(each_recruit):\n",
    "            if u'本科' in each_recruit:\n",
    "                return 'e_2'\n",
    "            elif u'硕士' in each_recruit:\n",
    "                return 'e_3'\n",
    "            elif u'博士' in each_recruit:\n",
    "                return 'e_3'\n",
    "            else:\n",
    "                return 'e_1'\n",
    "\n",
    "        default_result = OrderedDict([\n",
    "                ('e_1', 0),\n",
    "                ('e_2', 0),\n",
    "                ('e_3', 0),\n",
    "                ('e_4', 0),\n",
    "                ('e_5', 0),\n",
    "                ('e_6', 0),\n",
    "                ('e', 0),\n",
    "                ('z', 0)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        recruit_data = cls.resultiterable.data[0].a_recruit_industry\n",
    "        if recruit_data:\n",
    "            for key, value in recruit_data.iteritems():\n",
    "                if u'金融' in key:\n",
    "                    default_result['e_4'] += int(value)\n",
    "                if (u'租赁' or u'商务') in key:\n",
    "                    default_result['e_5'] += int(value)\n",
    "                if (u'信息传输' or u'软件' or u'信息技术') in key:\n",
    "                    default_result['e_6'] += int(value)\n",
    "        \n",
    "        if cls.resultiterable.data[0].a_recruit is not None:\n",
    "            for education_name, education_num in cls.resultiterable.data[0].a_recruit.iteritems():\n",
    "                default_result[get_recruit_num(education_name)] += int(education_num)\n",
    "            default_result['e'] = sum(\n",
    "                map(int ,cls.resultiterable.data[0].a_recruit.values()))\n",
    "            default_result['z'] = default_result['e_1'] / default_result['e'] * 15.  \n",
    "            \n",
    "        return dict(default_result)\n",
    "        \n",
    "    @__fault_tolerant\n",
    "    def get_feature_8(cls):\n",
    "        '''\n",
    "        公司运营持续风险\n",
    "        '''\n",
    "        import math\n",
    "        \n",
    "        esdate_relation_set = [\n",
    "            (datetime.date.today() - attr['esdate']).days \n",
    "            for node, attr in cls.DIG.nodes_iter(data=True) \n",
    "            if 0 < attr['distance'] <= 3 \n",
    "            and attr['is_human'] == 0 \n",
    "            and attr['esdate']]\n",
    "        t_2 = round(np.average(esdate_relation_set), 2)\n",
    "        \n",
    "        if (cls.DIG.node.get(cls.tarcompany, 0) and \n",
    "                    cls.DIG.node[cls.tarcompany]['esdate']):\n",
    "            t_1 = (datetime.date.today() - cls.DIG.node[cls.tarcompany]['esdate']).days \n",
    "        else :\n",
    "            t_1 = 0\n",
    "       \n",
    "        leagal_person_set = [\n",
    "            (datetime.date.today() - attr['esdate']).days \n",
    "            for node, attr in cls.DIG.nodes_iter(data=True) \n",
    "            if attr['distance'] == 1\n",
    "            and attr['is_human'] == 0 \n",
    "            and attr['esdate']\n",
    "        ]\n",
    "        t_3 = round(np.average(leagal_person_set), 2)\n",
    "    \n",
    "        return dict(\n",
    "            t_2=t_2 if not math.isnan(t_2) else 0,\n",
    "            t_1=t_1,\n",
    "            t_3=t_3 if not math.isnan(t_3) else 0,\n",
    "            y=t_1\n",
    "        )\n",
    "\n",
    "        \n",
    "    @__fault_tolerant\n",
    "    def get_feature_9(cls):\n",
    "        '''\n",
    "        招标和中标风险\n",
    "        '''\n",
    "        n_1 = cls.resultiterable.data[0].a_zhaobiao\n",
    "        n_2 = cls.resultiterable.data[0].a_zhongbiao\n",
    "        n_1 = n_1 if n_1 is not None else 0\n",
    "        n_2 = n_2 if n_2 is not None else 0\n",
    "        \n",
    "        n = n_1 + n_2\n",
    "        \n",
    "        return  dict(\n",
    "            n_1=n_1,\n",
    "            n_2=n_2,\n",
    "            n=n\n",
    "        )\n",
    "        \n",
    "    @__fault_tolerant\n",
    "    def get_feature_xgxx(cls):\n",
    "        '''\n",
    "        【计算基础数据】\n",
    "        法律诉讼风险：开庭公告、裁判文书、非法集资裁判文书、法院公告、民间借贷\n",
    "        行政处罚\n",
    "        被执行风险\n",
    "        异常经营风险：经营异常、吊销&注销\n",
    "        银监会行政处罚\n",
    "        ''' \n",
    "        def get_certain_distance_all_info(distance, document_types):\n",
    "            '''\n",
    "            总条数\n",
    "            '''\n",
    "            all_array = []\n",
    "            #处理某一个distance不存在节点的情况\n",
    "            all_array.append([0]*len(document_types))            \n",
    "            for node, attr in cls.DIG.nodes_iter(data=True):\n",
    "                if attr['is_human'] == 0  and attr['distance'] == distance:\n",
    "                    each_array = map(lambda x: x if x else 0, \n",
    "                                                  [attr[each_document] \n",
    "                                                      for each_document in document_types])\n",
    "                    all_array.append(each_array)\n",
    "                else:\n",
    "                    continue\n",
    "            documents_num = np.sum(all_array, axis=0)\n",
    "            return documents_num\n",
    "        \n",
    "        def get_certain_distance_add_info(distance, document_types):\n",
    "            '''\n",
    "            出现次数\n",
    "            '''\n",
    "            all_array = []\n",
    "            #处理某一个distance不存在节点的情况\n",
    "            all_array.append([0]*len(document_types))            \n",
    "            for node, attr in cls.DIG.nodes_iter(data=True):\n",
    "                if attr['is_human'] == 0  and attr['distance'] == distance:\n",
    "                    each_array = map(lambda x: 1 if x else 0, \n",
    "                                                  [attr[each_document] \n",
    "                                                      for each_document in document_types])\n",
    "                    all_array.append(each_array)\n",
    "                else:\n",
    "                    continue\n",
    "            documents_num = np.sum(all_array, axis=0)\n",
    "            return documents_num        \n",
    "        \n",
    "        matrx = dict()\n",
    "        xgxx_type = ['ktgg', 'zgcpwsw', 'rmfygg', \n",
    "                             'lending', 'xzcf', 'zhixing', \n",
    "                             'dishonesty', 'jyyc', 'circxzcf', \n",
    "                             'estatus', 'zgcpwsw_specific']\n",
    "        \n",
    "        xgxx_add_type = ['ktgg_1', 'zgcpwsw_1', 'rmfygg_1', \n",
    "                             'lending_1', 'xzcf_1', 'zhixing_1', \n",
    "                             'dishonesty_1', 'jyyc_1',\n",
    "                             'estatus_1', 'zgcpwsw_specific_1']   \n",
    "\n",
    "        for each_distance in xrange(0, 4):\n",
    "            xgxx_num_list = get_certain_distance_all_info(each_distance, \n",
    "                                                                                      xgxx_type)\n",
    "            xgxx_add_num_list = get_certain_distance_add_info(each_distance, \n",
    "                                                                                                xgxx_type)\n",
    "            matrx[each_distance] = dict(zip(xgxx_type, xgxx_num_list)+\n",
    "                                                         zip(xgxx_add_type, xgxx_add_num_list))\n",
    "            \n",
    "        return matrx\n",
    "    \n",
    "    @__fault_tolerant\n",
    "    def filter_xgxx_type(cls, tar_xgxx):\n",
    "        '''\n",
    "        过滤相关信息\n",
    "        '''\n",
    "        risk = dict()\n",
    "        for each_distance, xgxx_statistics in cls.xgxx_distribution.iteritems():\n",
    "            tar_xgxx_statistics = dict([\n",
    "                    (each_xgxx_type, each_xgxx_num)\n",
    "                    for each_xgxx_type, each_xgxx_num \n",
    "                    in xgxx_statistics.iteritems() \n",
    "                    if each_xgxx_type in tar_xgxx])\n",
    "            risk[each_distance] = tar_xgxx_statistics\n",
    "        return risk\n",
    "    \n",
    "    @__fault_tolerant\n",
    "    def get_feature_10(cls):\n",
    "        '''\n",
    "        1、法律诉讼风险：开庭公告、裁判文书、法院公告、民间借贷\n",
    "        2、某度关联方中非法集资裁判文书总数\n",
    "        '''\n",
    "        tar_xgxx = ['ktgg', 'zgcpwsw', 'rmfygg', 'lending', 'zgcpwsw_specific',\n",
    "                           'ktgg_1', 'zgcpwsw_1', 'rmfygg_1', \n",
    "                           'lending_1', 'zgcpwsw_specific_1']\n",
    "        risk = cls.filter_xgxx_type(tar_xgxx)\n",
    "        \n",
    "        risk['z'] = round(\n",
    "            np.dot(\n",
    "                map(sum, [\n",
    "                        risk[each_distance].values() \n",
    "                        for each_distance in xrange(0, 4)]), \n",
    "                [1., 1/2., 1/3., 1/4.]), 2)\n",
    "        \n",
    "        return risk\n",
    "            \n",
    "    @__fault_tolerant\n",
    "    def get_feature_11(cls):\n",
    "        '''\n",
    "        行政处罚风险\n",
    "        '''\n",
    "        tar_xgxx = ['xzcf', 'xzcf_1']\n",
    "        risk = cls.filter_xgxx_type(tar_xgxx)\n",
    "        \n",
    "        risk['z'] = round(\n",
    "            np.dot(\n",
    "                map(sum, [risk[each_distance].values() \n",
    "                        for each_distance in xrange(0, 4)]), \n",
    "                [1., 1/2., 1/3., 1/4.]), 2)\n",
    "        \n",
    "        return risk\n",
    "    \n",
    "    @__fault_tolerant\n",
    "    def get_feature_12(cls):\n",
    "        '''\n",
    "        被执行风险：被执行、失信被执行\n",
    "        '''\n",
    "        tar_xgxx = ['zhixing', 'dishonesty', 'zhixing_1', \n",
    "                           'dishonesty_1']\n",
    "        risk = cls.filter_xgxx_type(tar_xgxx)        \n",
    "        \n",
    "        risk['z'] = round(\n",
    "            np.dot(\n",
    "                map(\n",
    "                    lambda x: x['zhixing'] + 2. * x['dishonesty'], \n",
    "                    [risk[each_distance] for each_distance in xrange(0, 4)]), \n",
    "                [1., 1/2., 1/3., 1/4.]), 2)     \n",
    "        \n",
    "        return risk\n",
    "        \n",
    "    @__fault_tolerant\n",
    "    def get_feature_13(cls):\n",
    "        '''\n",
    "        异常经营风险\n",
    "        '''\n",
    "        tar_xgxx = ['jyyc', 'estatus', 'jyyc_1', 'estatus_1']\n",
    "        risk = cls.filter_xgxx_type(tar_xgxx)        \n",
    "        \n",
    "        risk['z'] = round(\n",
    "            np.dot(\n",
    "                map(\n",
    "                    lambda x: x['jyyc'] + 2. * x['estatus'], \n",
    "                    [risk[each_distance] for each_distance in xrange(0, 4)]), \n",
    "                [1., 1/2., 1/3., 1/4.]), 2)\n",
    "        \n",
    "        return risk        \n",
    "        \n",
    "    @__fault_tolerant\n",
    "    def get_feature_14(cls):\n",
    "        '''\n",
    "        银监会行政处罚\n",
    "        '''\n",
    "        tar_xgxx = ['circxzcf']\n",
    "        risk = cls.filter_xgxx_type(tar_xgxx)        \n",
    "        \n",
    "        risk['z'] = round(\n",
    "            np.dot(\n",
    "                map(\n",
    "                    lambda x: x['circxzcf'], \n",
    "                    [risk[each_distance] for each_distance in xrange(0, 4)]), \n",
    "                [1., 1/2., 1/3., 1/4.]), 2)\n",
    "     \n",
    "        return risk            \n",
    "        \n",
    "    @__fault_tolerant\n",
    "    def get_feature_15(cls):\n",
    "        '''\n",
    "        实际控制人风险\n",
    "        '''\n",
    "        def get_degree_distribution(is_human):\n",
    "            some_person_sets = [\n",
    "                node\n",
    "                for node, attr in cls.DIG.nodes_iter(data=True) \n",
    "                if attr['is_human'] == is_human\n",
    "                and attr['distance'] <= 3]\n",
    "            person_out_degree = cls.DIG.out_degree(some_person_sets).values()\n",
    "            if not person_out_degree:\n",
    "                person_out_degree.append(0)\n",
    "            return person_out_degree\n",
    "        \n",
    "        nature_person_distribution = get_degree_distribution(1)\n",
    "        legal_person_distribution = get_degree_distribution(0)\n",
    "        \n",
    "        nature_max_control = max(nature_person_distribution)\n",
    "        legal_max_control = max(legal_person_distribution)        \n",
    "        nature_avg_control = round(np.average(nature_person_distribution), 2)\n",
    "        legal_avg_control = round(np.average(legal_person_distribution), 2)\n",
    "        \n",
    "        total_legal_num = len([\n",
    "                node \n",
    "                for node, attr in cls.DIG.nodes_iter(data=True) \n",
    "                if attr['is_human'] == 0])\n",
    "        \n",
    "        risk = round(((\n",
    "                    2*(nature_max_control + legal_max_control) + \n",
    "                    (nature_avg_control + legal_avg_control)) /\n",
    "                (2*total_legal_num + 0.001)), 2)\n",
    "        \n",
    "        return dict(\n",
    "            x_1=nature_max_control,\n",
    "            x_2=legal_max_control,\n",
    "            y_1=nature_avg_control,\n",
    "            y_2=legal_avg_control,\n",
    "            z=total_legal_num,\n",
    "            r=risk\n",
    "        )\n",
    "    \n",
    "    @__fault_tolerant\n",
    "    def get_feature_16(cls):\n",
    "        '''\n",
    "        公司扩张路径风险\n",
    "        '''\n",
    "        def get_node_set(is_human):\n",
    "            return Counter([\n",
    "                    attr['distance']\n",
    "                    for node, attr in cls.DIG.nodes_iter(data=True)\n",
    "                    if attr['is_human'] == is_human])\n",
    "        \n",
    "        def get_node_set_two(company_type):\n",
    "            return Counter([\n",
    "                    attr['distance']\n",
    "                    for node, attr in cls.DIG.nodes_iter(data=True)\n",
    "                    if attr['is_human'] == 0\n",
    "                    and attr[company_type]])\n",
    "        \n",
    "        \n",
    "        nature_person_distribution = get_node_set(1)\n",
    "        legal_person_distribution = get_node_set(0)\n",
    "        so_company_distribution = get_node_set_two('isSOcompany')\n",
    "        ipo_company_distribution = get_node_set_two('isIPOcompany')\n",
    "        \n",
    "        nature_person_num = [\n",
    "            nature_person_distribution.get(each_distance, 0)\n",
    "            for each_distance in range(1, 4)]\n",
    "        legal_person_num = [\n",
    "            legal_person_distribution.get(each_distance, 0)\n",
    "            for each_distance in range(1, 4)]\n",
    "        so_company_num = [\n",
    "            so_company_distribution.get(each_distance, 0)\n",
    "            for each_distance in range(1, 4)]\n",
    "        ipo_company_num = [\n",
    "            ipo_company_distribution.get(each_distance, 0)\n",
    "            for each_distance in range(1, 4)]\n",
    "        \n",
    "        risk = round(np.sum(\n",
    "                np.divide(\n",
    "                    [\n",
    "                        np.divide(\n",
    "                            np.sum(nature_person_num[:each_distance]), \n",
    "                            np.sum(legal_person_num[:each_distance]), \n",
    "                            dtype=float)\n",
    "                        for each_distance in range(1, 4)], \n",
    "                    np.array([1, 2, 3], dtype=float))), 2)\n",
    "        \n",
    "        return dict(\n",
    "            x_1=legal_person_num[0],\n",
    "            x_2=legal_person_num[1],\n",
    "            x_3=legal_person_num[2],\n",
    "            y_1=nature_person_num[0],\n",
    "            y_2=nature_person_num[1],\n",
    "            y_3=nature_person_num[2],\n",
    "            v_1=so_company_num[0],\n",
    "            v_2=so_company_num[1],\n",
    "            v_3=so_company_num[2],\n",
    "            w_1=ipo_company_num[0],\n",
    "            w_2=ipo_company_num[1],\n",
    "            w_3=ipo_company_num[2],\n",
    "            r=risk if risk < 10000 else 0\n",
    "        )\n",
    "\n",
    "    @__fault_tolerant\n",
    "    def get_feature_17(cls):\n",
    "        '''\n",
    "        关联方中心集聚风险\n",
    "        '''\n",
    "        one_relation_set = [\n",
    "            node for node, attr in cls.DIG.nodes_iter(data=True) \n",
    "            if attr['distance'] == 1]\n",
    "        legal_person_shareholder = len([\n",
    "                src_node \n",
    "                for src_node, des_node, edge_attr \n",
    "                in cls.DIG.edges_iter(one_relation_set, data=True) \n",
    "                if des_node == cls.tarcompany \n",
    "                and edge_attr['is_invest'] == 'INVEST'\n",
    "                and cls.DIG.node[src_node]['is_human'] == 0])\n",
    "        legal_person_subsidiary = len([\n",
    "                des_node \n",
    "                for src_node, des_node, edge_attr \n",
    "                in cls.DIG.edges_iter(data=True) \n",
    "                if src_node == cls.tarcompany \n",
    "                and edge_attr['is_invest'] == 'INVEST'\n",
    "                and cls.DIG.node[des_node]['is_human'] == 0])\n",
    "        \n",
    "        risk =(\n",
    "            legal_person_subsidiary -\n",
    "            legal_person_shareholder )    \n",
    "        \n",
    "        return dict(\n",
    "            x=legal_person_subsidiary,\n",
    "            y=legal_person_shareholder,\n",
    "            r=risk\n",
    "        )\n",
    "\n",
    "    @__fault_tolerant\n",
    "    def get_feature_18(cls):\n",
    "        '''\n",
    "        分支机构过度扩张风险\n",
    "        '''\n",
    "        risk = cls.resultiterable.data[0].a_fzjg\n",
    "        risk = risk if risk is not None else 0\n",
    "        \n",
    "        return dict(\n",
    "            d=risk, \n",
    "            z=risk\n",
    "        )\n",
    "    \n",
    "    @__fault_tolerant\n",
    "    def get_feature_19(cls):\n",
    "        '''\n",
    "        关联方结构稳定风险\n",
    "        '''\n",
    "        def get_relation_num():\n",
    "            return Counter([\n",
    "                    attr['distance']\n",
    "                    for node, attr in cls.DIG.nodes(data=True)])\n",
    "        \n",
    "        #目标企业各个度的节点的数量\n",
    "        relations_num = get_relation_num()\n",
    "        relation_three_num = relations_num.get(3, 0)\n",
    "        relation_two_num = relations_num.get(2, 0)\n",
    "        relation_one_num = relations_num.get(1, 0)\n",
    "        relation_zero_num = relations_num.get(0, 1)\n",
    "        \n",
    "        x = np.array([\n",
    "                relation_zero_num, \n",
    "                relation_one_num, \n",
    "                relation_two_num, \n",
    "                relation_three_num]).astype(float)\n",
    "        \n",
    "        y_2 = x[2] / (x[1]+x[2])\n",
    "        y_3 = x[3] / (x[1]+x[2]+x[3])\n",
    "        risk = y_2/2 + y_3/3\n",
    "        \n",
    "        return dict(\n",
    "            x_1=x[1],\n",
    "            x_2=x[2],\n",
    "            x_3=x[3],\n",
    "            y_2=y_2,\n",
    "            y_3=y_3,\n",
    "            z=risk if risk else 0\n",
    "        )        \n",
    "    \n",
    "    @__fault_tolerant\n",
    "    def get_feature_20(cls):\n",
    "        '''\n",
    "        1、潜在违规融资风险\n",
    "        2、某度关联方中新金融企业数量\n",
    "        '''    \n",
    "        def get_relation_info(distance):\n",
    "            return sum([\n",
    "                1\n",
    "                for node, attr in cls.DIG.nodes_iter(data=True)\n",
    "                if attr['distance'] == distance \n",
    "                and attr['is_new_finance']\n",
    "            ])\n",
    "        \n",
    "        def get_relation_risk_num(keyword_type):\n",
    "            return Counter([attr['distance']\n",
    "                    for node, attr in cls.DIG.nodes_iter(data=True) \n",
    "                    if attr['is_human'] == 0 and \n",
    "                    attr['opescope'] == keyword_type])\n",
    "        \n",
    "        k_2_num = get_relation_risk_num('k_2')\n",
    "        k_1_num = get_relation_risk_num('k_1')\n",
    "        \n",
    "        x = np.array([\n",
    "                k_2_num.get(each_distance, 0) \n",
    "                for each_distance in xrange(1, 4)])\n",
    "        y = np.array([\n",
    "                k_1_num.get(each_distance, 0) \n",
    "                for each_distance in xrange(1, 4)])\n",
    "    \n",
    "        risk = round(\n",
    "            np.sum(\n",
    "                np.divide(np.add(x, y), [1, 2, 3], dtype=float)), 2)\n",
    "    \n",
    "        return dict(\n",
    "            x_1=x[0],\n",
    "            x_2=x[1],\n",
    "            x_3=x[2],\n",
    "            y_1=y[0],\n",
    "            y_2=y[1],\n",
    "            y_3=y[2],\n",
    "            z_1=get_relation_info(1),\n",
    "            z_2=get_relation_info(2),\n",
    "            z_3=get_relation_info(3),\n",
    "            g=risk\n",
    "        )\n",
    "    \n",
    "    @__fault_tolerant\n",
    "    def get_feature_21(cls):\n",
    "        '''\n",
    "        关联方地址集中度风险\n",
    "        '''\n",
    "        legal_person_address = [\n",
    "            attr['address'] \n",
    "            for node, attr in cls.DIG.nodes_iter(data=True) \n",
    "            if attr['distance'] <= 3 \n",
    "            and attr['is_human'] == 0]\n",
    "        c = Counter(\n",
    "            filter(\n",
    "                lambda x: x is not None and len(x) >= 21,  \n",
    "                legal_person_address))\n",
    "        n = c.most_common(1)\n",
    "        n= n[0][1] if len(n) > 0 else 1\n",
    "        risk = n -1\n",
    "                 \n",
    "        return dict(\n",
    "            n=risk,\n",
    "            y=risk\n",
    "        )\n",
    "    \n",
    "    @__fault_tolerant\n",
    "    def get_feature_22(cls):\n",
    "        '''\n",
    "        利益一致行动法人风险\n",
    "        '''\n",
    "        def is_similarity(str_1, str_2):\n",
    "            '''\n",
    "            判断两个字符串是否有连续2个字相同\n",
    "            '''\n",
    "            try:\n",
    "                token_1 = [\n",
    "                    str_1[index] + str_1[index+1] \n",
    "                    for index,data in enumerate(str_1) \n",
    "                    if index < len(str_1) - 1]\n",
    "                is_similarity = sum([\n",
    "                        1 for each_token in token_1 \n",
    "                        if each_token in str_2])\n",
    "                return True if is_similarity > 0 else False        \n",
    "            except:\n",
    "                return False\n",
    "            \n",
    "        #目标公司的名称frag\n",
    "        tar_company = cls.resultiterable.data[0].a_name\n",
    "        tar_company_frag = cls.resultiterable.data[0].a_namefrag\n",
    "        #三度以内，所有关联方的节点集合(不包含自身)\n",
    "        relation_set = [\n",
    "                attr['name'] \n",
    "                for node, attr in cls.DIG.nodes_iter(data=True) \n",
    "                if attr['distance'] <= 3]\n",
    "        #relation_set.remove(tar_company)\n",
    "        common_interests_list = [\n",
    "            1 \n",
    "            for node_name in relation_set \n",
    "            if is_similarity(tar_company_frag, node_name)]\n",
    "        \n",
    "        risk = sum(common_interests_list)\n",
    "        \n",
    "        return dict(\n",
    "            d=risk, \n",
    "            z=risk\n",
    "        )        \n",
    "        \n",
    "    @__fault_tolerant\n",
    "    def get_feature_23(cls):\n",
    "        '''\n",
    "        关联方信誉风险\n",
    "        '''\n",
    "        def get_black_num(distance):\n",
    "            '''\n",
    "            某一度黑企业的总数\n",
    "            '''\n",
    "            return sum([\n",
    "                    1\n",
    "                    for node, attr in cls.DIG.nodes_iter(data=True)\n",
    "                    if (attr['is_black'] and attr['distance'] == distance \n",
    "                            and attr['is_human'] == 0)\n",
    "            ])\n",
    "        \n",
    "        def get_common_interests_num(distance):\n",
    "            '''\n",
    "            与黑名单库中任意一家企业存在利益一致行动关系\n",
    "            '''\n",
    "            return sum([\n",
    "                    1\n",
    "                    for node, attr in cls.DIG.nodes_iter(data=True)\n",
    "                    if attr['distance'] == distance \n",
    "                    and attr['common_interests']\n",
    "            ])\n",
    "        \n",
    "        def get_common_address_num(distance):\n",
    "            '''\n",
    "            与黑名单库中任意一家企业地址相同\n",
    "            '''\n",
    "            return sum([\n",
    "                    1\n",
    "                    for node, attr in cls.DIG.nodes_iter(data=True)\n",
    "                    if attr['distance'] == distance \n",
    "                    and attr['common_address']\n",
    "            ])\n",
    "        \n",
    "        black_relation_set =[\n",
    "            get_black_num(each_distance) \n",
    "            for each_distance in range(1, 4)]\n",
    "        common_interests_set=[\n",
    "            get_common_interests_num(each_distance) \n",
    "            for each_distance in range(1, 4)]\n",
    "        common_address_set=[\n",
    "            get_common_address_num(each_distance) \n",
    "            for each_distance in range(1, 4)]\n",
    "        \n",
    "        risk = np.dot(black_relation_set, [1, 1/2., 1/3.])\n",
    "        \n",
    "        return dict(\n",
    "            b_1=black_relation_set[0],\n",
    "            b_2=black_relation_set[1],\n",
    "            b_3=black_relation_set[2],\n",
    "            c_1=common_interests_set[0],\n",
    "            c_2=common_interests_set[1],\n",
    "            c_3=common_interests_set[2],\n",
    "            d_1=common_address_set[0],\n",
    "            d_2=common_address_set[1],\n",
    "            d_3=common_address_set[2],\n",
    "            y=risk\n",
    "        )\n",
    "    \n",
    "    @__fault_tolerant\n",
    "    def get_feature_24(cls):\n",
    "        '''\n",
    "        短期逐利风险\n",
    "        '''\n",
    "        \n",
    "        def get_max_established(distance, timedelta):\n",
    "            '''\n",
    "            获取在某distance关联方企业中，某企业在任意timedelta内投资成立公司数量的最大值\n",
    "            '''\n",
    "            #用于获取最大连续时间数，这里i的取值要仔细琢磨一下，这里输入一个时间差序列与时间差\n",
    "            def get_date_density(difference_list, timedelta):\n",
    "                time_density_list = []\n",
    "                for index, date in enumerate(difference_list):\n",
    "                    if date < timedelta:\n",
    "                        s = 0\n",
    "                        i = 1\n",
    "                        while(s < timedelta and i <= len(difference_list) - index):\n",
    "                            i += 1\n",
    "                            s = sum(difference_list[index:index+i])\n",
    "                        time_density_list.append(i)\n",
    "                    else:\n",
    "                        continue\n",
    "                return max(time_density_list) if len(time_density_list) >0 else 0            \n",
    "\n",
    "            #distance所有节点集合\n",
    "            relation_set = [\n",
    "                node \n",
    "                for node, attr in cls.DIG.nodes(data=True) \n",
    "                if attr['distance'] == distance]\n",
    "            investment_dict = defaultdict(lambda : [])     \n",
    "            \n",
    "            if len(cls.DIG.edge) > 1:\n",
    "                for src_node, des_node, edge_attr in (\n",
    "                        cls.DIG.edges_iter(relation_set, data=True)):\n",
    "                    if (edge_attr.get('is_invest', 0) == 'INVEST' \n",
    "                            and cls.DIG.node[des_node]['distance'] == distance\n",
    "                            and cls.DIG.node[des_node]['esdate'] is not None \n",
    "                            and cls.DIG.node[src_node]['is_human'] == 0):\n",
    "                        #将所有节点投资的企业的成立时间加进列表中\n",
    "                        investment_dict[src_node].append(cls.DIG.node[des_node]['esdate'])\n",
    "             \n",
    "            #目标企业所有节点所投资的企业时间密度字典\n",
    "            all_date_density_dict = {}\n",
    "            \n",
    "            for node, date_list in investment_dict.iteritems():\n",
    "                #构建按照时间先后排序的序列\n",
    "                date_strp_list = sorted(date_list)\n",
    "                if len(date_strp_list) > 1:\n",
    "                    #构建时间差的序列，例如：[256, 4, 5, 1, 2, 33, 6, 5, 4, 73]\n",
    "                    date_difference_list = [\n",
    "                        (date_strp_list[index + 1] - date_strp_list[index]).days \n",
    "                        for index in range(len(date_strp_list) -1)]\n",
    "                    #计算某法人节点在timedelta天之内有多少家公司成立\n",
    "                    es_num = get_date_density(date_difference_list, timedelta)\n",
    "                    if all_date_density_dict.has_key(es_num):\n",
    "                        all_date_density_dict[es_num].append(node)\n",
    "                    else:\n",
    "                        all_date_density_dict[es_num] = [node]              \n",
    "                else:\n",
    "                    continue\n",
    "            keys = all_date_density_dict.keys()        \n",
    "            max_num = max(keys) if len(keys) > 0 else 0\n",
    "            \n",
    "            return max_num\n",
    "        \n",
    "        x = [\n",
    "            get_max_established(each_distance, 180) \n",
    "            for each_distance in xrange(0, 4)]\n",
    "        \n",
    "        legal_person_num = sum([\n",
    "            1 \n",
    "            for node, attr in cls.DIG.nodes(data=True) \n",
    "            if not attr['is_human']])\n",
    "        \n",
    "        risk = round(\n",
    "            np.sum(\n",
    "                np.divide(x, [1, 2, 3, 4], dtype=float)), 2) / legal_person_num * 15\n",
    "        \n",
    "        if  risk == 0:\n",
    "            r = 10\n",
    "        elif 0 < risk <= 0.7:\n",
    "            r = 30\n",
    "        elif 0.7 < risk <= 1:\n",
    "            r = 60  \n",
    "        elif 1 < risk:\n",
    "            r = 100       \n",
    "        else:\n",
    "            r = 10\n",
    "        \n",
    "        return dict(\n",
    "            x_0=x[0],\n",
    "            x_1=x[1],\n",
    "            x_2=x[2],\n",
    "            x_3=x[3],\n",
    "            w=legal_person_num,\n",
    "            z=risk\n",
    "        )\n",
    "    \n",
    "    @__fault_tolerant\n",
    "    def get_feature_25(cls):\n",
    "        '''\n",
    "        目标公司在任意6个月内分支机构数量的最大值\n",
    "        '''\n",
    "        \n",
    "        def get_max_established(distance, timedelta):\n",
    "            '''\n",
    "            获取在某distance关联方企业中，某企业在任意timedelta内投资成立公司（分支机构）数量的最大值\n",
    "            '''\n",
    "            #用于获取最大连续时间数，这里i的取值要仔细琢磨一下，这里输入一个时间差序列与时间差\n",
    "            def get_date_density(difference_list, timedelta):\n",
    "                time_density_list = []\n",
    "                for index, date in enumerate(difference_list):\n",
    "                    if date < timedelta:\n",
    "                        s = 0\n",
    "                        i = 1\n",
    "                        while(s < timedelta and i <= len(difference_list) - index):\n",
    "                            i += 1\n",
    "                            s = sum(difference_list[index:index+i])\n",
    "                        time_density_list.append(i)\n",
    "                    else:\n",
    "                        continue\n",
    "                return max(time_density_list) if len(time_density_list) >0 else 0            \n",
    "\n",
    "            #distance所有节点集合\n",
    "            relation_set = [\n",
    "                node \n",
    "                for node, attr in cls.DIG.nodes(data=True) \n",
    "                if attr['distance'] == distance]\n",
    "            investment_dict = defaultdict(lambda : [])     \n",
    "            \n",
    "            if len(cls.DIG.edge) > 1:\n",
    "                for src_node, des_node, edge_attr in (\n",
    "                        cls.DIG.edges_iter(relation_set, data=True)):\n",
    "                    if (edge_attr.get('is_invest', 0) == 'INVEST' \n",
    "                            and cls.DIG.node[des_node]['distance'] == distance\n",
    "                            and cls.DIG.node[des_node]['esdate'] is not None \n",
    "                            and cls.DIG.node[src_node]['is_human'] == 0\n",
    "                            and cls.DIG.node[des_node]['is_fzjg']):\n",
    "                        #将所有节点投资的企业（分支机构）的成立时间加进列表中\n",
    "                        investment_dict[src_node].append(cls.DIG.node[des_node]['esdate'])\n",
    "             \n",
    "            #目标企业所有节点所投资的企业时间密度字典\n",
    "            all_date_density_dict = {}\n",
    "            \n",
    "            for node, date_list in investment_dict.iteritems():\n",
    "                #构建按照时间先后排序的序列\n",
    "                date_strp_list = sorted(date_list)\n",
    "                if len(date_strp_list) > 1:\n",
    "                    #构建时间差的序列，例如：[256, 4, 5, 1, 2, 33, 6, 5, 4, 73]\n",
    "                    date_difference_list = [\n",
    "                        (date_strp_list[index + 1] - date_strp_list[index]).days \n",
    "                        for index in range(len(date_strp_list) -1)]\n",
    "                    #计算某法人节点在timedelta天之内有多少家公司成立\n",
    "                    es_num = get_date_density(date_difference_list, timedelta)\n",
    "                    if all_date_density_dict.has_key(es_num):\n",
    "                        all_date_density_dict[es_num].append(node)\n",
    "                    else:\n",
    "                        all_date_density_dict[es_num] = [node]              \n",
    "                else:\n",
    "                    continue\n",
    "            keys = all_date_density_dict.keys()        \n",
    "            max_num = max(keys) if len(keys) > 0 else 0\n",
    "            \n",
    "            return max_num\n",
    "        \n",
    "        x = [\n",
    "            get_max_established(each_distance, 180) \n",
    "            for each_distance in xrange(0, 4)]\n",
    "        \n",
    "        return dict(\n",
    "            x_0=x[0],\n",
    "            x_1=x[1],\n",
    "            x_2=x[2],\n",
    "            x_3=x[3]\n",
    "        )\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def get_some_feature(cls, resultiterable, feature_nums):\n",
    "        #创建类变量\n",
    "        cls.resultiterable = resultiterable\n",
    "        cls.tarcompany = cls.resultiterable.data[0].a\n",
    "        cls.DIG = cls.create_graph_udf(cls.resultiterable, 1)\n",
    "        cls.xgxx_distribution = cls.get_feature_xgxx()\n",
    "        \n",
    "        feature_list = [\n",
    "            ('feature_{0}'.format(feature_index), \n",
    "                     methodcaller('get_feature_{0}'.format(feature_index))(cls))\n",
    "            for feature_index in feature_nums]\n",
    "        feature_list.append(('bbd_qyxx_id', cls.tarcompany))\n",
    "        feature_list.append(('company_name', cls.resultiterable.data[0].a_name))\n",
    "        \n",
    "        return dict(feature_list) #json.dumps(dict(feature_list), ensure_ascii=False)      \n",
    "    \n",
    "    @classmethod\n",
    "    def get_some_feature_test(cls, resultiterable, dig, feature_nums):\n",
    "        #创建类变量\n",
    "        cls.resultiterable = resultiterable\n",
    "        cls.tarcompany = cls.resultiterable.data[0].a\n",
    "        cls.DIG =  cls.create_graph_udf(cls.resultiterable, 1)\n",
    "        cls.xgxx_distribution = cls.get_feature_xgxx()\n",
    "        \n",
    "       \n",
    "        #eval('cls.get_feature_{0}()'.format(feature_index))\n",
    "        feature_list = [\n",
    "            ('feature_{0}'.format(feature_index), \n",
    "                     operator.methodcaller('get_feature_{0}'.format(feature_index))(cls))\n",
    "            for feature_index in feature_nums]\n",
    "        feature_list.append(('bbd_qyxx_id', cls.tarcompany))\n",
    "        feature_list.append(('company_name', cls.resultiterable.data[0].a_name))\n",
    "        \n",
    "        return dict(feature_list)\n",
    "    \n",
    "def spark_data_flow(tidversion):\n",
    "    '''\n",
    "    dataflow\n",
    "    '''\n",
    "    #输入\n",
    "    #算法设计直接用rdd\n",
    "    tid_df = spark.read.parquet(\"/user/antifraud/jinli/tiddata/relation_company_info_merge/{version}\".format(version=tidversion))\n",
    "    tid_rdd = tid_df.rdd\n",
    "        \n",
    "    #最终计算流程\n",
    "    tid_rdd_2 = tid_rdd.map(lambda row: (row.a_name, row)) \\\n",
    "        .groupByKey() \\\n",
    "        .repartition(600) \\\n",
    "        .filter(lambda r: len(r[1].data) <= 100000) \\\n",
    "        .cache()\n",
    "\n",
    "\n",
    "    import signal\n",
    "    def handler(signum, frame):\n",
    "        raise AssertionError\n",
    "\n",
    "    def fault_tolerant(func):\n",
    "        def wappen(*args):\n",
    "            try:\n",
    "                return func(*args)\n",
    "            except Exception, e:\n",
    "                return e\n",
    "        return wappen\n",
    "\n",
    "    @fault_tolerant\n",
    "    def time_out(data):\n",
    "        signal.signal(signal.SIGALRM, handler)\n",
    "        signal.alarm(60)\n",
    "        result = FeatureConstruction.get_some_feature(data, \n",
    "                                                      [_ for _ in range(1, 26)])\n",
    "        signal.alarm(0)\n",
    "        return result        \n",
    "        \n",
    "    feature_list = tid_rdd_2.mapValues(\n",
    "        time_out\n",
    "    ).map(\n",
    "        itemgetter(1)\n",
    "    )\n",
    "    \n",
    "    return feature_list\n",
    "\n",
    "def collect_to_driver(version, out_path, tidversion):\n",
    "    '''\n",
    "    格式化输出\n",
    "    '''\n",
    "    src_time = time.time()\n",
    "    pd_df = json_normalize(spark_data_flow(tidversion).collect())\n",
    "    os.system(\"rm /data5/antifraud/Jinli/data/prddata/prd_jinli_feature_score_distribution_{version}.xlsx\".format(version=version))\n",
    "    pd_df.to_excel(\"{path}/prd_jinli_feature_score_distribution_{version}.xlsx\".format(path=OUTPATH, version=version))\n",
    "    des_time = time.time()\n",
    "    \n",
    "    print 'SUCCESS !! \\n 耗时：{0}'.format(des_time - src_time)\n",
    "\n",
    "def to_json(version, tidversion):\n",
    "    '''\n",
    "    导成json\n",
    "    '''\n",
    "    src_time = time.time()\n",
    "    pd_rdd = spark_data_flow(tidversion)\n",
    "    os.system(\"hadoop fs -rmr /user/antifraud/jinli/prddata/prd_jinli_feature_score_distribution_{version}\".format(version=version))\n",
    "    pd_rdd.repartition(\n",
    "        10\n",
    "    ).map(\n",
    "        lambda r: json.dumps(r, ensure_ascii=False)    \n",
    "    ).saveAsTextFile(\n",
    "        \"/user/antifraud/jinli/prddata/prd_jinli_feature_score_distribution_{version}\".format(version=version)\n",
    "    )\n",
    "    des_time = time.time()\n",
    "    \n",
    "    print 'SUCCESS !! \\n 耗时：{0}'.format(des_time - src_time)    \n",
    "    \n",
    "if __name__ == '__main__':  \n",
    "    #输入参数\n",
    "    VERSION = \"20180724_01\"\n",
    "    OUTPATH = \"/data5/antifraud/Jinli/data/prddata\"\n",
    "    \n",
    "    #collect_to_driver(\n",
    "    #    version=VERSION,\n",
    "    #    out_path=OUTPATH,\n",
    "    #     tidversion='20170117'\n",
    "    # )\n",
    "\n",
    "    to_json(version=VERSION,\n",
    "                 tidversion=relation_version)\n",
    "    #print spark_data_flow('20170117').take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+--------------------+\n",
      "|            features|         bbd_qyxx_id|       company_name|      scaledFeatures|\n",
      "+--------------------+--------------------+-------------------+--------------------+\n",
      "|[0.0,100.0,30.0,0...|673c99d996ca44d18...|       上海上豪投资管理有限公司|[0.0,0.8490061918...|\n",
      "|[0.0,60.0,30.0,0....|9dd8f06edac743e1a...|      上海会钜富投资管理有限公司|[0.0,-0.264613252...|\n",
      "|[0.0,100.0,22.5,0...|15eae5aa74da451ab...|       上海勤合投资咨询有限公司|[0.0,0.8490061918...|\n",
      "|[0.0,100.0,30.0,0...|44f9c3403cf64206a...|   上海奔瑞投资合伙企业（有限合伙）|[0.0,0.8490061918...|\n",
      "|[0.0,100.0,30.0,0...|a809a0a3cc6d48668...|       上海密彗投资咨询有限公司|[0.0,0.8490061918...|\n",
      "|[0.0,100.0,30.0,0...|10deda38649b4c92a...|   上海心云投资管理中心（有限合伙）|[0.0,0.8490061918...|\n",
      "|[0.0,30.0,30.0,0....|ab2944d05f1840e98...|       上海旗绩投资管理有限公司|[0.0,-1.099827836...|\n",
      "|[0.0,100.0,30.0,0...|84ceb3198b84477ab...|     上海梓珖投资中心（有限合伙）|[0.0,0.8490061918...|\n",
      "|[0.0,10.0,30.0,0....|9a9bd7bd216b482fb...|       上海满加投资咨询有限公司|[0.0,-1.656637558...|\n",
      "|[0.0,60.0,30.0,0....|f205d49d436e4169b...|       上海玖健投资管理有限公司|[0.0,-0.264613252...|\n",
      "|[0.0,30.0,30.0,0....|0adeaa5dd8404c0d9...|         上海盈太投资有限公司|[0.0,-1.099827836...|\n",
      "|[0.0,100.0,30.0,0...|04c0515d2cbb43508...|   上海福沿投资合伙企业（有限合伙）|[0.0,0.8490061918...|\n",
      "|[0.0,60.0,30.0,0....|339ebf91f8ea4938b...|     上海缘传园林景观工程有限公司|[0.0,-0.264613252...|\n",
      "|[0.0,100.0,30.0,0...|eac4d0eb270a47d79...|     上海茂衫商务信息咨询有限公司|[0.0,0.8490061918...|\n",
      "|[0.0,100.0,30.0,0...|6a54ed469148407e9...| 上海赐浩创业投资合伙企业（有限合伙）|[0.0,0.8490061918...|\n",
      "|[0.0,100.0,30.0,0...|82de7393dc224e489...|       上海逸奇健康咨询有限公司|[0.0,0.8490061918...|\n",
      "|[0.0,60.0,30.0,0....|fed9870322044501b...|       上海雁衡电子商务有限公司|[0.0,-0.264613252...|\n",
      "|[0.0,10.0,30.0,0....|cc16fa1bdf0246eeb...|     世邦融资租赁（上海）有限公司|[0.0,-1.656637558...|\n",
      "|[0.0,100.0,30.0,0...|29012f4a7f0e40449...|      东莞市格子网络科技有限公司|[0.0,0.8490061918...|\n",
      "|[0.0,100.0,30.0,0...|0ce9ee7fc83a48d18...|中国邮政集团公司河北省迁西县潘家口支局|[0.0,0.8490061918...|\n",
      "+--------------------+--------------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "使用mllib来做标准化\n",
    "'''\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from  pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "input_df = spark.read.parquet(\n",
    "    \"/user/antifraud/hongjing2/dataflow/step_two/raw/nf_feature_merge/20170221\"\n",
    ")\n",
    "\n",
    "def get_vectors(row):\n",
    "    if row['feature_26']:\n",
    "        return (Vectors.dense([\n",
    "                0,\n",
    "                row['feature_2']['c'],\n",
    "                row['feature_3']['z'],\n",
    "                0,\n",
    "                row['feature_5']['l'],\n",
    "                row['feature_6']['z'],\n",
    "                row['feature_7']['z'],\n",
    "                row['feature_8']['y'],\n",
    "                row['feature_9']['n'],\n",
    "                row['feature_10']['z'],\n",
    "                row['feature_11']['z'],\n",
    "                0,\n",
    "                row['feature_13']['z'],\n",
    "                0,\n",
    "                row['feature_15']['r'],\n",
    "                row['feature_16']['r'],\n",
    "                row['feature_17']['r'],\n",
    "                row['feature_18']['z'],\n",
    "                0,\n",
    "                row['feature_20']['g'],\n",
    "                row['feature_21']['y'],\n",
    "                row['feature_22']['z'],\n",
    "                row['feature_23']['y'],\n",
    "                row['feature_24']['z'],\n",
    "                row['feature_26']['a_1'],\n",
    "                0,\n",
    "                0,\n",
    "                row['feature_26']['a_4'],\n",
    "                row['feature_26']['a_5'],\n",
    "                row['feature_26']['a_6'],\n",
    "                0,\n",
    "                row['feature_26']['b_1'],\n",
    "                row['feature_26']['b_2'],\n",
    "                row['feature_26']['b_3'],\n",
    "                0,\n",
    "                0,\n",
    "                row['feature_26']['c_1'],\n",
    "                0,\n",
    "                row['feature_26']['d_2'],\n",
    "            ]),        \n",
    "            row['bbd_qyxx_id'],\n",
    "            row['company_name'])\n",
    "    else:\n",
    "        return (Vectors.dense([\n",
    "                0,\n",
    "                row['feature_2']['c'],\n",
    "                row['feature_3']['z'],\n",
    "                0,\n",
    "                row['feature_5']['l'],\n",
    "                row['feature_6']['z'],\n",
    "                row['feature_7']['z'],\n",
    "                row['feature_8']['y'],\n",
    "                row['feature_9']['n'],\n",
    "                row['feature_10']['z'],\n",
    "                row['feature_11']['z'],\n",
    "                0,\n",
    "                row['feature_13']['z'],\n",
    "                0,\n",
    "                row['feature_15']['r'],\n",
    "                row['feature_16']['r'],\n",
    "                row['feature_17']['r'],\n",
    "                row['feature_18']['z'],\n",
    "                0,\n",
    "                row['feature_20']['g'],\n",
    "                row['feature_21']['y'],\n",
    "                row['feature_22']['z'],\n",
    "                row['feature_23']['y'],\n",
    "                row['feature_24']['z'],\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "            ]), \n",
    "            row['bbd_qyxx_id'],\n",
    "            row['company_name'])\n",
    "        \n",
    "\n",
    "tid_vector_df = input_df.rdd.map(\n",
    "    get_vectors\n",
    ").toDF(\n",
    ").withColumnRenamed(\n",
    "    '_1', 'features'\n",
    ").withColumnRenamed(\n",
    "    '_2', 'bbd_qyxx_id'\n",
    ").withColumnRenamed(\n",
    "    '_3', 'company_name'\n",
    ")\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=True)\n",
    "\n",
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel = scaler.fit(tid_vector_df)\n",
    "\n",
    "# Normalize each feature to have unit standard deviation.\n",
    "scaledData = scalerModel.transform(tid_vector_df)\n",
    "scaledData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_df = spark.read.parquet(\"/user/antifraud/hongjing2/dataflow/step_two/tid/nf_feature_preprocessing/20170221\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_label_probability(iter_data):\n",
    "    '''计算判黑概率'''\n",
    "\n",
    "    from sklearn.externals import joblib\n",
    "    \n",
    "    company_names = []\n",
    "    bbd_qyxx_ids = []\n",
    "    data_set = []\n",
    "    \n",
    "    for each_row in iter_data:\n",
    "        data = each_row['scaledFeatures'].values\n",
    "        company_names.append(each_row['company_name'])\n",
    "        bbd_qyxx_ids.append(each_row['bbd_qyxx_id'])\n",
    "        data_set.append(data)\n",
    "        \n",
    "    data_set = filter(lambda x: 1 if len(x) == 39  else 0, data_set)\n",
    "    lr = joblib.load(\"GM_release_LR.model\")\n",
    "    raw_prob = lr.predict_proba(data_set)\n",
    "    \n",
    "    return zip(bbd_qyxx_ids, company_names, raw_prob[:, 1], data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#step_one 计算判黑概率\n",
    "raw_rdd = input_df.rdd.repartition(\n",
    "    100\n",
    ").mapPartitions(\n",
    "    get_label_probability\n",
    ").cache()\n",
    "\n",
    "data = raw_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "特征计算,打分\n",
    "'''\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "input_df = spark.read.parquet(\"/user/antifraud/hongjing2/dataflow/step_two/tid/nf_feature_preprocessing/20170221\")\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1. / (1 + math.exp(-x))\n",
    "\n",
    "def get_first_grade_indexes(row):\n",
    "    '''计算一级指标'''\n",
    "    vector = row[4]\n",
    "    F1_weight = [78.995978788237665, 19.643289163698, 1.3607320480643235]\n",
    "    F2_weight = [26.060202553046778, 32.455344310155851, \n",
    "                         35.422074212291704, 6.062378924505663]\n",
    "    F3_weight = [8.5671628439652956, 67.099469406473432, 24.33336774956128]\n",
    "    F4_weight = [5.4992316645363379, 0.73619670729601905, \n",
    "                         16.465178242190262, 6.8112990078436821, \n",
    "                         27.336531281175152, 5.9500466959961198, \n",
    "                         0.94871599993642608, 35.054955599386361, 1.1978448016396601]\n",
    "    F5_weight = [2.1642169432775109, 7.2205710674551362, \n",
    "                         2.7554815418792837, 1.8386921453073, \n",
    "                         4.9583378078768945, 0.24537414679201996,\n",
    "                         0.14077225761444445, 75.358040614215426, 5.3185134755819945]\n",
    "    \n",
    "    #综合实力风险\n",
    "    GM_company_strength_risk = np.dot(F1_weight, map(sigmoid,\n",
    "                                                     [vector[1], vector[2], vector[4]]))\n",
    "    #经营行为风险\n",
    "    GM_behavior_risk = np.dot(F2_weight, map(sigmoid,\n",
    "                                             [vector[5], vector[6], vector[7], vector[8]]))\n",
    "    #企业诚信风险\n",
    "    GM_credit_risk = np.dot(F3_weight, map(sigmoid,\n",
    "                                           [vector[9], vector[10], vector[12]]))\n",
    "    #静态关联方风险\n",
    "    GM_static_relationship_risk = np.dot(F4_weight, map(sigmoid,\n",
    "                                                        [vector[14], vector[15], vector[16],\n",
    "                                                         vector[17], vector[19], vector[20],\n",
    "                                                         vector[21], row[4][22], vector[23]]))\n",
    "    #动态关联方风险\n",
    "    GM_dynamic_relationship_risk = np.dot(F5_weight, map(sigmoid,\n",
    "                                                         [vector[24], vector[27], vector[28],\n",
    "                                                          vector[29], vector[31], vector[32],\n",
    "                                                          vector[33], vector[36], vector[38]]))\n",
    "    return dict(\n",
    "            bbd_qyxx_id=row[0],\n",
    "            company_name=row[1],\n",
    "            total_score=row[3],\n",
    "            GM_company_strength_risk=round(GM_company_strength_risk, 1),\n",
    "            GM_behavior_risk=round(GM_behavior_risk, 1),\n",
    "            GM_credit_risk=round(GM_credit_risk, 1),\n",
    "            GM_static_relationship_risk=round(GM_static_relationship_risk, 1),\n",
    "            GM_dynamic_relationship_risk=round(GM_dynamic_relationship_risk, 1)\n",
    "        )\n",
    "\n",
    "\n",
    "def change_prob_score(row, quantile_one, quantile_two):\n",
    "    '''根据分位点，将判黑概率值转化成分'''\n",
    "    raw_prob = sigmoid(row[2])\n",
    "    quantile_one = sigmoid(quantile_one)\n",
    "    quantile_two = sigmoid(quantile_two)\n",
    "    \n",
    "    if raw_prob >= quantile_one:\n",
    "        score = (raw_prob - quantile_one) * 40. / (0.75 - quantile_one) + 50\n",
    "    elif quantile_two <= raw_prob <= quantile_one:\n",
    "        score = ((raw_prob - quantile_two) * 50. / (quantile_one - quantile_two) + \n",
    "                    (quantile_one - raw_prob) * 30. / (quantile_one - quantile_two))\n",
    "    elif raw_prob < quantile_two:\n",
    "        score = (raw_prob - quantile_two) * 30. / quantile_two + 30\n",
    "    \n",
    "    return (row[0], row[1], row[2], round(score, 1), row[3])\n",
    "    \n",
    "def get_label_probability(iter_data):\n",
    "    '''计算判黑概率'''\n",
    "\n",
    "    from sklearn.externals import joblib\n",
    "    \n",
    "    company_names = []\n",
    "    bbd_qyxx_ids = []\n",
    "    data_set = []\n",
    "    \n",
    "    for each_row in iter_data:\n",
    "        data = each_row['scaledFeatures'].values\n",
    "        company_names.append(each_row['company_name'])\n",
    "        bbd_qyxx_ids.append(each_row['bbd_qyxx_id'])\n",
    "        data_set.append(data)\n",
    "        \n",
    "    data_set = filter(lambda x: 1 if len(x) == 39 else 0, data_set)\n",
    "    lr = joblib.load(\"GM_release_LR.model\")\n",
    "    raw_prob = lr.predict_proba(data_set)\n",
    "    \n",
    "    return zip(bbd_qyxx_ids, company_names, raw_prob[:, 1], data_set)\n",
    "\n",
    "\n",
    "def get_label_probability_2(each_row):\n",
    "    '''计算判黑概率'''\n",
    "    from sklearn.externals import joblib\n",
    "    data = each_row['scaledFeatures'].values\n",
    "    lr = joblib.load(\"GM_release_LR.model\")\n",
    "    raw_prob = lr.predict_proba(data)\n",
    "    return (each_row['bbd_qyxx_id'], each_row['company_name'], raw_prob[0, 1])\n",
    "\n",
    "\n",
    "#step_one 计算判黑概率\n",
    "raw_rdd = input_df.rdd.repartition(\n",
    "    100\n",
    ").mapPartitions(\n",
    "    get_label_probability\n",
    ").cache()\n",
    "\n",
    "\n",
    "#step_two 将判黑概率转换成分数\n",
    "#计算分位点\n",
    "score_distribution = raw_rdd.map(\n",
    "    lambda r: r[2]\n",
    ").collect()\n",
    "score_distribution.sort(reverse=True)\n",
    "#取5%与50%的分位点\n",
    "top_five_index = int(len(score_distribution) * 0.05) - 1\n",
    "top_fifty_index = int(len(score_distribution) * 0.5) - 1\n",
    "#得到分位点\n",
    "Y_1 = score_distribution[top_five_index]\n",
    "Y_2 = score_distribution[top_fifty_index]\n",
    "#得到结果\n",
    "tid_rdd = raw_rdd.map(\n",
    "    lambda r: change_prob_score(r, Y_1, Y_2)\n",
    ").cache()\n",
    "\n",
    "#step_three 计算一级指标的得分\n",
    "prd_rdd = tid_rdd.map(\n",
    "    get_first_grade_indexes\n",
    ")\n",
    "\n",
    "#输出\n",
    "codec = \"org.apache.hadoop.io.compress.GzipCodec\"\n",
    "\n",
    "prd_rdd.map(\n",
    "    lambda r: json.dumps(r, ensure_ascii=False)\n",
    ").coalesce(\n",
    "    10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------+\n",
      "|         bbd_qyxx_id|       company_name|           risk_tags|\n",
      "+--------------------+-------------------+--------------------+\n",
      "|673c99d996ca44d18...|       上海上豪投资管理有限公司|{\"--\": {\"静态关联方风险\"...|\n",
      "|9dd8f06edac743e1a...|      上海会钜富投资管理有限公司|{\"--\": {\"静态关联方风险\"...|\n",
      "|15eae5aa74da451ab...|       上海勤合投资咨询有限公司|{\"--\": {\"静态关联方风险\"...|\n",
      "|44f9c3403cf64206a...|   上海奔瑞投资合伙企业（有限合伙）|{\"--\": {\"静态关联方风险\"...|\n",
      "|a809a0a3cc6d48668...|       上海密彗投资咨询有限公司|{\"--\": {\"静态关联方风险\"...|\n",
      "|10deda38649b4c92a...|   上海心云投资管理中心（有限合伙）|{\"--\": {\"静态关联方风险\"...|\n",
      "|ab2944d05f1840e98...|       上海旗绩投资管理有限公司|{\"--\": {\"静态关联方风险\"...|\n",
      "|84ceb3198b84477ab...|     上海梓珖投资中心（有限合伙）|{\"--\": {\"静态关联方风险\"...|\n",
      "|9a9bd7bd216b482fb...|       上海满加投资咨询有限公司|{\"--\": {\"静态关联方风险\"...|\n",
      "|f205d49d436e4169b...|       上海玖健投资管理有限公司|{\"--\": {\"静态关联方风险\"...|\n",
      "|0adeaa5dd8404c0d9...|         上海盈太投资有限公司|{\"--\": {\"静态关联方风险\"...|\n",
      "|04c0515d2cbb43508...|   上海福沿投资合伙企业（有限合伙）|{\"--\": {\"静态关联方风险\"...|\n",
      "|339ebf91f8ea4938b...|     上海缘传园林景观工程有限公司|{\"--\": {\"静态关联方风险\"...|\n",
      "|eac4d0eb270a47d79...|     上海茂衫商务信息咨询有限公司|{\"--\": {\"静态关联方风险\"...|\n",
      "|6a54ed469148407e9...| 上海赐浩创业投资合伙企业（有限合伙）|{\"--\": {\"静态关联方风险\"...|\n",
      "|82de7393dc224e489...|       上海逸奇健康咨询有限公司|{\"--\": {\"静态关联方风险\"...|\n",
      "|fed9870322044501b...|       上海雁衡电子商务有限公司|{\"--\": {\"静态关联方风险\"...|\n",
      "|cc16fa1bdf0246eeb...|     世邦融资租赁（上海）有限公司|{\"--\": {\"静态关联方风险\"...|\n",
      "|29012f4a7f0e40449...|      东莞市格子网络科技有限公司|{\"--\": {\"静态关联方风险\"...|\n",
      "|0ce9ee7fc83a48d18...|中国邮政集团公司河北省迁西县潘家口支局|{\"--\": {\"静态关联方风险\"...|\n",
      "+--------------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "打标签\n",
    "'''\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def get_tags(row):\n",
    "    \n",
    "    def get_behavior_risk(risk_name=u\"经营行为风险\"):\n",
    "        tags = []\n",
    "        if row['feature_6']['c_1'] >= 2:\n",
    "            tags.append(u'法定代表人变更频繁')\n",
    "        if row['feature_6']['c_2'] >= 2:\n",
    "            tags.append(u'股东变更频繁')\n",
    "        if row['feature_6']['c_3'] >= 2:\n",
    "            tags.append(u'注册资本变更频繁')\n",
    "        if row['feature_6']['c_4'] >= 2:\n",
    "            tags.append(u'高管变更频繁')\n",
    "        if row['feature_6']['c_5'] >= 2:\n",
    "            tags.append(u'经营范围变更频繁')\n",
    "        if row['feature_18']['d'] >= 2:\n",
    "            tags.append(u'分支机构数量较多')\n",
    "        if row['feature_22']['z'] >= 2:\n",
    "            tags.append(u'企业利益一致行动法人较多')\n",
    "        return {\n",
    "            risk_name: tags\n",
    "        }\n",
    "    \n",
    "    def get_company_strength_risk(risk_name=u'综合实力风险'):\n",
    "        tags = []\n",
    "        if row['feature_8']['t_1'] and row['feature_8']['t_1'] <= 700:\n",
    "            tags.append(u'公司成立时间较短')\n",
    "        if (row['feature_7']['e'] and \n",
    "                row['feature_7']['e_1'] / row['feature_7']['e'] >= 0.3):\n",
    "            tags.append(u'大专及大专以下或不限专业招聘比例较高')\n",
    "        if row['feature_17']['x'] >= 3:\n",
    "            tags.append(u'对外投资公司数量较多')\n",
    "        return {\n",
    "            risk_name: tags\n",
    "        }\n",
    "    \n",
    "    def get_credit_risk(risk_name=u'企业诚信风险'):\n",
    "        tags = []\n",
    "        if (row['feature_10']['0']['ktgg'] + \n",
    "                row['feature_10']['0']['rmfygg'] +\n",
    "                row['feature_10']['0']['zgcpwsw']) >= 10:\n",
    "            tags.append(u'企业诉讼文书数量较多')\n",
    "        if row['feature_10']['0']['lending']:\n",
    "            tags.append(u'企业存在“民间借贷”类诉讼文书')\n",
    "        if row['feature_11']['0']['xzcf']:\n",
    "            tags.append(u'企业受到行政处罚')\n",
    "        if row['feature_12']['0']['zhixing']:\n",
    "            tags.append(u'企业存在被执行人信息')\n",
    "        if row['feature_12']['0']['dishonesty']:\n",
    "            tags.append(u'企业存在失信被执行人信息')\n",
    "        if row['feature_13']['0']['jyyc']:\n",
    "            tags.append(u'企业存在经营异常信息')\n",
    "        return {\n",
    "            risk_name: tags\n",
    "        }\n",
    "    \n",
    "    def get_all_nums(feature_name, distances=['0', '1', '2', '3']):\n",
    "        return sum(\n",
    "            map(\n",
    "                sum, \n",
    "                [v.asDict().values() \n",
    "                     for k ,v in row[feature_name].asDict().iteritems() \n",
    "                     if k in distances]))\n",
    "    def get_some_nums(feature_name, value_name, distances=['0', '1', '2', '3']):\n",
    "        return sum([\n",
    "                v.asDict().get(value_name, 0)\n",
    "                for k, v in row[feature_name].asDict().iteritems()\n",
    "                if k in distances])         \n",
    "    def get_static_relationship_risk(risk_name=u'静态关联方风险'):\n",
    "        tags = []\n",
    "        if get_all_nums('feature_10', distances=['0', '1', '2']) >= 15:\n",
    "            tags.append(u'二度内关联方诉讼文书数量较多')\n",
    "        if get_some_nums('feature_10', 'lending') >= 3:\n",
    "            tags.append(u'三度内关联方存在较多“民间借贷”类诉讼文书')\n",
    "        if get_some_nums('feature_11', 'xzcf', distances=['0', '1', '2']):\n",
    "            tags.append(u'二度内关联方存在受行政处罚企业')\n",
    "        if get_some_nums('feature_12', 'zhixing'):\n",
    "            tags.append(u'三度内关联方存在被执行人企业数量较多')\n",
    "        if get_some_nums('feature_12', 'dishonesty'):\n",
    "            tags.append(u'三度内关联方存在失信被执行人企业')\n",
    "        if get_some_nums('feature_13', 'estatus', distances=['0', '1', '2']):\n",
    "            tags.append(u'二度内关联方存在吊销企业')\n",
    "        if get_some_nums('feature_13', 'jyyc', distances=['0', '1', '2']):\n",
    "            tags.append(u'二度内关联方存在经营异常企业')\n",
    "        if row['feature_8']['t_2'] and row['feature_8']['t_2'] <= 365:\n",
    "            tags.append(u'二度内法人关联方平均成立时间较短')\n",
    "        if row['feature_15']['x_1'] >= 10:\n",
    "            tags.append(u'二度内单个关联自然人中最大投资企业数量较多')\n",
    "        if row['feature_15']['x_2'] >= 25:\n",
    "            tags.append(u'二度内单个关联法人最大投资企业数量较多')\n",
    "        if ((row['feature_16']['y_1'] + row['feature_16']['y_2']) /\n",
    "               (row['feature_16']['y_1'] + row['feature_16']['y_2'] +\n",
    "                row['feature_16']['x_1'] + row['feature_16']['x_2'] + 0.01)) >= 0.9:\n",
    "            tags.append(u'二度内关联方中自然人比例较高')\n",
    "        if row['feature_21']['n'] >= 2:\n",
    "            tags.append(u'关联企业中地址相同公司较多')\n",
    "        if (row['feature_23']['b_1'] + row['feature_23']['b_2']) :\n",
    "            tags.append(u'二度内关联方中存在黑名单企业')\n",
    "        return {\n",
    "            risk_name: tags\n",
    "        }\n",
    "            \n",
    "    def get_dynamic_relationship_risk(risk_name=u'动态关联方风险'):\n",
    "        tags = []\n",
    "        if row['feature_26']['a_1'] > 2 or row['feature_26']['a_1'] < -0.7:\n",
    "            tags.append(u'近三个月内自然人关联节点变动较大')\n",
    "        if row['feature_26']['a_4'] > 0.1 or row['feature_26']['a_4'] < -0.1:\n",
    "            tags.append(u'近三个月内对外投资公司数量变动较大')\n",
    "        if row['feature_26']['a_5'] > 0.5:\n",
    "            tags.append(u'近三个月内公司分支机构数量变动较大')\n",
    "        if row['feature_26']['a_6'] > 0.5:\n",
    "            tags.append(u'近三个月内利益一致行动法人的数量变动较大')\n",
    "        if row['feature_26']['c_1'] > 1 or row['feature_26']['c_1'] < -0.5:\n",
    "            tags.append(u'近三个月内低学历招聘人数变动较大')\n",
    "        if row['feature_26']['d_2'] > 2 or row['feature_26']['d_2'] < -0.7:\n",
    "            tags.append(u'近三个月内3度及3度以下核心自然人（前3）控制节点总数变动较大')\n",
    "        return {\n",
    "            risk_name: tags\n",
    "        }\n",
    "    \n",
    "    result = {}\n",
    "    result.update(get_behavior_risk())\n",
    "    result.update(get_company_strength_risk())\n",
    "    result.update(get_credit_risk())\n",
    "    result.update(get_static_relationship_risk())    \n",
    "    if row['feature_26']:\n",
    "        result.update(get_dynamic_relationship_risk())\n",
    "    else:\n",
    "        result.update({u'动态关联方风险': []})\n",
    "        \n",
    "    final_out_dict = {\n",
    "        '--': result\n",
    "    }\n",
    "    \n",
    "    return json.dumps(final_out_dict, ensure_ascii=False)\n",
    "\n",
    "RELATION_VERSION = '20170221'\n",
    "\n",
    "raw_nf_feature_df = spark.read.parquet(\n",
    "    (\"/user/antifraud/hongjing2/dataflow/step_two/raw\"\n",
    "     \"/nf_feature_merge/{version}\").format(version=RELATION_VERSION))\n",
    "nf_info_df = spark.read.parquet(\n",
    "    (\"/user/antifraud/hongjing2/dataflow/step_three/raw\"\n",
    "     \"/nf_info_merge/{version}\").format(version=RELATION_VERSION))\n",
    "\n",
    "tid_nf_feature_df = raw_nf_feature_df.rdd.map(\n",
    "    lambda r: Row(\n",
    "        company_name=r['company_name'],\n",
    "        bbd_qyxx_id=r['bbd_qyxx_id'],\n",
    "        risk_tags=get_tags(r)\n",
    "    )\n",
    ").toDF().cache()\n",
    "\n",
    "tid_nf_feature_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(company_name=u'\\u4e0a\\u6d77\\u4e0a\\u8c6a\\u6295\\u8d44\\u7ba1\\u7406\\u6709\\u9650\\u516c\\u53f8', bbd_qyxx_id=u'673c99d996ca44d18e171f01246d12ad', risk_tags=u'{\"--\": {\"\\u9759\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": [], \"\\u7ecf\\u8425\\u884c\\u4e3a\\u98ce\\u9669\": [\"\\u4f01\\u4e1a\\u5229\\u76ca\\u4e00\\u81f4\\u884c\\u52a8\\u6cd5\\u4eba\\u8f83\\u591a\"], \"\\u7efc\\u5408\\u5b9e\\u529b\\u98ce\\u9669\": [], \"\\u52a8\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": [], \"\\u4f01\\u4e1a\\u8bda\\u4fe1\\u98ce\\u9669\": []}}', risk_index=30.0, risk_composition=u'{\"--\": {\"\\u9759\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": 49.6, \"\\u4f01\\u4e1a\\u8bda\\u4fe1\\u98ce\\u9669\": 47.0, \"total_score\": 30.0, \"\\u7efc\\u5408\\u5b9e\\u529b\\u98ce\\u9669\": 67.4, \"\\u52a8\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": 48.2, \"\\u7ecf\\u8425\\u884c\\u4e3a\\u98ce\\u9669\": 52.6}}', province=u'\\u4e0a\\u6d77\\u5e02', city=u'\\u4e0a\\u6d77\\u5e02', county=u'\\u95f5\\u884c\\u533a', is_black=None),\n",
       " Row(company_name=u'\\u4e0a\\u6d77\\u4f1a\\u949c\\u5bcc\\u6295\\u8d44\\u7ba1\\u7406\\u6709\\u9650\\u516c\\u53f8', bbd_qyxx_id=u'9dd8f06edac743e1a161531cb40640c5', risk_tags=u'{\"--\": {\"\\u9759\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": [\"\\u4e8c\\u5ea6\\u5185\\u5355\\u4e2a\\u5173\\u8054\\u81ea\\u7136\\u4eba\\u4e2d\\u6700\\u5927\\u6295\\u8d44\\u4f01\\u4e1a\\u6570\\u91cf\\u8f83\\u591a\"], \"\\u7ecf\\u8425\\u884c\\u4e3a\\u98ce\\u9669\": [], \"\\u7efc\\u5408\\u5b9e\\u529b\\u98ce\\u9669\": [\"\\u516c\\u53f8\\u6210\\u7acb\\u65f6\\u95f4\\u8f83\\u77ed\"], \"\\u52a8\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": [], \"\\u4f01\\u4e1a\\u8bda\\u4fe1\\u98ce\\u9669\": []}}', risk_index=32.4, risk_composition=u'{\"--\": {\"\\u9759\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": 49.9, \"\\u4f01\\u4e1a\\u8bda\\u4fe1\\u98ce\\u9669\": 47.0, \"total_score\": 32.4, \"\\u7efc\\u5408\\u5b9e\\u529b\\u98ce\\u9669\": 46.4, \"\\u52a8\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": 48.2, \"\\u7ecf\\u8425\\u884c\\u4e3a\\u98ce\\u9669\": 46.4}}', province=u'\\u4e0a\\u6d77\\u5e02', city=u'\\u4e0a\\u6d77\\u5e02', county=u'\\u957f\\u5b81\\u533a', is_black=None),\n",
       " Row(company_name=u'\\u4e0a\\u6d77\\u52e4\\u5408\\u6295\\u8d44\\u54a8\\u8be2\\u6709\\u9650\\u516c\\u53f8', bbd_qyxx_id=u'15eae5aa74da451ab211c8fd7eb62745', risk_tags=u'{\"--\": {\"\\u9759\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": [\"\\u4e8c\\u5ea6\\u5185\\u5173\\u8054\\u65b9\\u5b58\\u5728\\u7ecf\\u8425\\u5f02\\u5e38\\u4f01\\u4e1a\"], \"\\u7ecf\\u8425\\u884c\\u4e3a\\u98ce\\u9669\": [\"\\u4f01\\u4e1a\\u5229\\u76ca\\u4e00\\u81f4\\u884c\\u52a8\\u6cd5\\u4eba\\u8f83\\u591a\"], \"\\u7efc\\u5408\\u5b9e\\u529b\\u98ce\\u9669\": [], \"\\u52a8\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": [], \"\\u4f01\\u4e1a\\u8bda\\u4fe1\\u98ce\\u9669\": []}}', risk_index=30.0, risk_composition=u'{\"--\": {\"\\u9759\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": 50.8, \"\\u4f01\\u4e1a\\u8bda\\u4fe1\\u98ce\\u9669\": 48.3, \"total_score\": 30.0, \"\\u7efc\\u5408\\u5b9e\\u529b\\u98ce\\u9669\": 62.7, \"\\u52a8\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": 48.2, \"\\u7ecf\\u8425\\u884c\\u4e3a\\u98ce\\u9669\": 55.5}}', province=u'\\u4e0a\\u6d77\\u5e02', city=u'\\u4e0a\\u6d77\\u5e02', county=u'\\u5d07\\u660e\\u533a', is_black=None),\n",
       " Row(company_name=u'\\u4e0a\\u6d77\\u5954\\u745e\\u6295\\u8d44\\u5408\\u4f19\\u4f01\\u4e1a\\uff08\\u6709\\u9650\\u5408\\u4f19\\uff09', bbd_qyxx_id=u'44f9c3403cf64206a75bf14bd831cddc', risk_tags=u'{\"--\": {\"\\u9759\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": [\"\\u4e09\\u5ea6\\u5185\\u5173\\u8054\\u65b9\\u5b58\\u5728\\u88ab\\u6267\\u884c\\u4eba\\u4f01\\u4e1a\\u6570\\u91cf\\u8f83\\u591a\", \"\\u4e8c\\u5ea6\\u5185\\u5173\\u8054\\u65b9\\u5b58\\u5728\\u540a\\u9500\\u4f01\\u4e1a\"], \"\\u7ecf\\u8425\\u884c\\u4e3a\\u98ce\\u9669\": [], \"\\u7efc\\u5408\\u5b9e\\u529b\\u98ce\\u9669\": [], \"\\u52a8\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": [], \"\\u4f01\\u4e1a\\u8bda\\u4fe1\\u98ce\\u9669\": []}}', risk_index=30.0, risk_composition=u'{\"--\": {\"\\u9759\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": 48.9, \"\\u4f01\\u4e1a\\u8bda\\u4fe1\\u98ce\\u9669\": 48.0, \"total_score\": 30.0, \"\\u7efc\\u5408\\u5b9e\\u529b\\u98ce\\u9669\": 67.4, \"\\u52a8\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": 48.2, \"\\u7ecf\\u8425\\u884c\\u4e3a\\u98ce\\u9669\": 44.1}}', province=u'\\u4e0a\\u6d77\\u5e02', city=u'\\u4e0a\\u6d77\\u5e02', county=u'\\u666e\\u9640\\u533a', is_black=None),\n",
       " Row(company_name=u'\\u4e0a\\u6d77\\u5bc6\\u5f57\\u6295\\u8d44\\u54a8\\u8be2\\u6709\\u9650\\u516c\\u53f8', bbd_qyxx_id=u'a809a0a3cc6d48668af4be996480ef32', risk_tags=u'{\"--\": {\"\\u9759\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": [\"\\u4e8c\\u5ea6\\u5185\\u5173\\u8054\\u65b9\\u4e2d\\u81ea\\u7136\\u4eba\\u6bd4\\u4f8b\\u8f83\\u9ad8\"], \"\\u7ecf\\u8425\\u884c\\u4e3a\\u98ce\\u9669\": [], \"\\u7efc\\u5408\\u5b9e\\u529b\\u98ce\\u9669\": [], \"\\u52a8\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": [], \"\\u4f01\\u4e1a\\u8bda\\u4fe1\\u98ce\\u9669\": []}}', risk_index=30.0, risk_composition=u'{\"--\": {\"\\u9759\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": 49.1, \"\\u4f01\\u4e1a\\u8bda\\u4fe1\\u98ce\\u9669\": 47.0, \"total_score\": 30.0, \"\\u7efc\\u5408\\u5b9e\\u529b\\u98ce\\u9669\": 67.4, \"\\u52a8\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": 48.2, \"\\u7ecf\\u8425\\u884c\\u4e3a\\u98ce\\u9669\": 54.3}}', province=u'\\u4e0a\\u6d77\\u5e02', city=u'\\u4e0a\\u6d77\\u5e02', county=u'\\u5b9d\\u5c71\\u533a', is_black=None),\n",
       " Row(company_name=u'\\u4e0a\\u6d77\\u5fc3\\u4e91\\u6295\\u8d44\\u7ba1\\u7406\\u4e2d\\u5fc3\\uff08\\u6709\\u9650\\u5408\\u4f19\\uff09', bbd_qyxx_id=u'10deda38649b4c92a7c54ef4cfdfb2a3', risk_tags=u'{\"--\": {\"\\u9759\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": [], \"\\u7ecf\\u8425\\u884c\\u4e3a\\u98ce\\u9669\": [\"\\u4f01\\u4e1a\\u5229\\u76ca\\u4e00\\u81f4\\u884c\\u52a8\\u6cd5\\u4eba\\u8f83\\u591a\"], \"\\u7efc\\u5408\\u5b9e\\u529b\\u98ce\\u9669\": [], \"\\u52a8\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": [], \"\\u4f01\\u4e1a\\u8bda\\u4fe1\\u98ce\\u9669\": []}}', risk_index=30.0, risk_composition=u'{\"--\": {\"\\u9759\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": 47.9, \"\\u4f01\\u4e1a\\u8bda\\u4fe1\\u98ce\\u9669\": 47.0, \"total_score\": 30.0, \"\\u7efc\\u5408\\u5b9e\\u529b\\u98ce\\u9669\": 67.4, \"\\u52a8\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": 48.2, \"\\u7ecf\\u8425\\u884c\\u4e3a\\u98ce\\u9669\": 43.0}}', province=u'\\u4e0a\\u6d77\\u5e02', city=u'\\u4e0a\\u6d77\\u5e02', county=u'\\u6768\\u6d66\\u533a', is_black=None),\n",
       " Row(company_name=u'\\u4e0a\\u6d77\\u65d7\\u7ee9\\u6295\\u8d44\\u7ba1\\u7406\\u6709\\u9650\\u516c\\u53f8', bbd_qyxx_id=u'ab2944d05f1840e98b0d99a20445d8ea', risk_tags=u'{\"--\": {\"\\u9759\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": [\"\\u4e8c\\u5ea6\\u5185\\u5173\\u8054\\u65b9\\u4e2d\\u81ea\\u7136\\u4eba\\u6bd4\\u4f8b\\u8f83\\u9ad8\"], \"\\u7ecf\\u8425\\u884c\\u4e3a\\u98ce\\u9669\": [], \"\\u7efc\\u5408\\u5b9e\\u529b\\u98ce\\u9669\": [], \"\\u52a8\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": [], \"\\u4f01\\u4e1a\\u8bda\\u4fe1\\u98ce\\u9669\": []}}', risk_index=36.3, risk_composition=u'{\"--\": {\"\\u9759\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": 49.8, \"\\u4f01\\u4e1a\\u8bda\\u4fe1\\u98ce\\u9669\": 47.0, \"total_score\": 36.3, \"\\u7efc\\u5408\\u5b9e\\u529b\\u98ce\\u9669\": 31.8, \"\\u52a8\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": 48.2, \"\\u7ecf\\u8425\\u884c\\u4e3a\\u98ce\\u9669\": 54.0}}', province=u'\\u4e0a\\u6d77\\u5e02', city=u'\\u4e0a\\u6d77\\u5e02', county=u'\\u6768\\u6d66\\u533a', is_black=None),\n",
       " Row(company_name=u'\\u4e0a\\u6d77\\u6893\\u73d6\\u6295\\u8d44\\u4e2d\\u5fc3\\uff08\\u6709\\u9650\\u5408\\u4f19\\uff09', bbd_qyxx_id=u'84ceb3198b84477ab4cef5b4f66baa52', risk_tags=u'{\"--\": {\"\\u9759\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": [], \"\\u7ecf\\u8425\\u884c\\u4e3a\\u98ce\\u9669\": [], \"\\u7efc\\u5408\\u5b9e\\u529b\\u98ce\\u9669\": [\"\\u516c\\u53f8\\u6210\\u7acb\\u65f6\\u95f4\\u8f83\\u77ed\"], \"\\u52a8\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": [], \"\\u4f01\\u4e1a\\u8bda\\u4fe1\\u98ce\\u9669\": []}}', risk_index=30.0, risk_composition=u'{\"--\": {\"\\u9759\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": 47.0, \"\\u4f01\\u4e1a\\u8bda\\u4fe1\\u98ce\\u9669\": 47.0, \"total_score\": 30.0, \"\\u7efc\\u5408\\u5b9e\\u529b\\u98ce\\u9669\": 67.4, \"\\u52a8\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": 48.2, \"\\u7ecf\\u8425\\u884c\\u4e3a\\u98ce\\u9669\": 39.9}}', province=u'\\u4e0a\\u6d77\\u5e02', city=u'\\u4e0a\\u6d77\\u5e02', county=u'\\u5d07\\u660e\\u533a', is_black=None),\n",
       " Row(company_name=u'\\u4e0a\\u6d77\\u6ee1\\u52a0\\u6295\\u8d44\\u54a8\\u8be2\\u6709\\u9650\\u516c\\u53f8', bbd_qyxx_id=u'9a9bd7bd216b482fbc8818f6363567a0', risk_tags=u'{\"--\": {\"\\u9759\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": [\"\\u4e8c\\u5ea6\\u5185\\u5173\\u8054\\u65b9\\u4e2d\\u81ea\\u7136\\u4eba\\u6bd4\\u4f8b\\u8f83\\u9ad8\"], \"\\u7ecf\\u8425\\u884c\\u4e3a\\u98ce\\u9669\": [], \"\\u7efc\\u5408\\u5b9e\\u529b\\u98ce\\u9669\": [\"\\u516c\\u53f8\\u6210\\u7acb\\u65f6\\u95f4\\u8f83\\u77ed\"], \"\\u52a8\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": [], \"\\u4f01\\u4e1a\\u8bda\\u4fe1\\u98ce\\u9669\": []}}', risk_index=49.7, risk_composition=u'{\"--\": {\"\\u9759\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": 49.1, \"\\u4f01\\u4e1a\\u8bda\\u4fe1\\u98ce\\u9669\": 47.0, \"total_score\": 49.7, \"\\u7efc\\u5408\\u5b9e\\u529b\\u98ce\\u9669\": 24.7, \"\\u52a8\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": 48.2, \"\\u7ecf\\u8425\\u884c\\u4e3a\\u98ce\\u9669\": 41.2}}', province=u'\\u4e0a\\u6d77\\u5e02', city=u'\\u4e0a\\u6d77\\u5e02', county=u'\\u95f5\\u884c\\u533a', is_black=None),\n",
       " Row(company_name=u'\\u4e0a\\u6d77\\u7396\\u5065\\u6295\\u8d44\\u7ba1\\u7406\\u6709\\u9650\\u516c\\u53f8', bbd_qyxx_id=u'f205d49d436e4169b88263130bd3734c', risk_tags=u'{\"--\": {\"\\u9759\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": [\"\\u4e8c\\u5ea6\\u5185\\u5173\\u8054\\u65b9\\u4e2d\\u81ea\\u7136\\u4eba\\u6bd4\\u4f8b\\u8f83\\u9ad8\"], \"\\u7ecf\\u8425\\u884c\\u4e3a\\u98ce\\u9669\": [], \"\\u7efc\\u5408\\u5b9e\\u529b\\u98ce\\u9669\": [], \"\\u52a8\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": [], \"\\u4f01\\u4e1a\\u8bda\\u4fe1\\u98ce\\u9669\": []}}', risk_index=32.5, risk_composition=u'{\"--\": {\"\\u9759\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": 49.1, \"\\u4f01\\u4e1a\\u8bda\\u4fe1\\u98ce\\u9669\": 47.0, \"total_score\": 32.5, \"\\u7efc\\u5408\\u5b9e\\u529b\\u98ce\\u9669\": 46.4, \"\\u52a8\\u6001\\u5173\\u8054\\u65b9\\u98ce\\u9669\": 48.2, \"\\u7ecf\\u8425\\u884c\\u4e3a\\u98ce\\u9669\": 41.9}}', province=u'\\u4e0a\\u6d77\\u5e02', city=u'\\u4e0a\\u6d77\\u5e02', county=u'\\u6d66\\u4e1c\\u65b0\\u533a', is_black=None)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RELATION_VERSION = '20170221'\n",
    "\n",
    "df = spark.read.parquet(\n",
    "    '/user/antifraud/hongjing2/dataflow/step_three/tid/nf_feature_tags/20170221'\n",
    ")\n",
    "df.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0168715545885\n",
      "0.000775719110953\n"
     ]
    }
   ],
   "source": [
    "print Y_1\n",
    "print Y_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6138134506932581e-16"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(score_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5000000000000001"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(min(score_distribution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.000000000098018"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1. / (1 + math.exp(-x))\n",
    "\n",
    "#130W家公司最小的概率\n",
    "raw_prob = sigmoid(min(score_distribution)*50000)\n",
    "#50%分位点\n",
    "quantile_two = sigmoid(0.000775719110953*50000)\n",
    "\n",
    "#原始值\n",
    "raw_prob * 30. / quantile_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.json(\"/user/antifraud/hongjing2/dataflow/step_two/prd/nf_feature_risk_score/20170221\")\n",
    "data = df.rdd.map(lambda r: r.total_score).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.0"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sort(reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.43938350763932"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sigmoid(50.2 / 100.) * 100 -62) * 15 / 10. + 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5000000000032673"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0015930328110282772"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "import random\n",
    "\n",
    "lr = joblib.load(\"/data5/antifraud/Hongjing2/data/inputdata/model/GM_release_LR.model\")\n",
    "\n",
    "result = lr.predict_proba([[0 for _ in range(39)]])\n",
    "result[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.99263620e-01,   7.36380271e-04],\n",
       "       [  9.99314607e-01,   6.85393289e-04],\n",
       "       [  9.97008961e-01,   2.99103903e-03]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = input_df.rdd.map(lambda r: r.scaledFeatures.values).take(3)\n",
    "lr.predict_proba(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as fun\n",
    "from pyspark.sql import types as tp\n",
    "\n",
    "TR = True\n",
    "\n",
    "def is_has(col, s=TR):\n",
    "    if 'has' in col and s:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "is_has_udf = fun.udf(is_has, tp.BooleanType())\n",
    "\n",
    "\n",
    "input_df.where(is_has_udf('feature_1')).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bbd_qyxx_id': u'111146dbf706468899add1ac18174eed',\n",
       "  'company_name': u'\\u5317\\u4eac\\u535a\\u745e\\u5b89\\u5eb7\\u79d1\\u6280\\u53d1\\u5c55\\u6709\\u9650\\u516c\\u53f8',\n",
       "  'feature_3': {'x_0': 0, 'x_1': 0, 'x_2': 0, 'x_3': 0, 'z': 30.0}},\n",
       " {'bbd_qyxx_id': u'23cbb08e51fc4df0aa3a46acd5e3320e',\n",
       "  'company_name': u'\\u5317\\u4eac\\u6b23\\u8f89\\u7535\\u529b\\u6295\\u8d44\\u6709\\u9650\\u516c\\u53f8',\n",
       "  'feature_3': {'x_0': 0, 'x_1': 0, 'x_2': 0, 'x_3': 0, 'z': 30.0}},\n",
       " {'bbd_qyxx_id': u'1f33b89ebe614ef1bd9f64c1b213d9a7',\n",
       "  'company_name': u'\\u5317\\u4eac\\u878d\\u5c1a\\u667a\\u5178\\u6295\\u8d44\\u7ba1\\u7406\\u6709\\u9650\\u516c\\u53f8',\n",
       "  'feature_3': {'x_0': 0, 'x_1': 0, 'x_2': 0, 'x_3': 0, 'z': 30.0}},\n",
       " {'bbd_qyxx_id': u'9ff2229092e749d0944a68b0c9545039',\n",
       "  'company_name': u'\\u541b\\u6d77\\u65f6\\u4ee3\\u6295\\u8d44\\u6709\\u9650\\u516c\\u53f8',\n",
       "  'feature_3': {'x_0': 0, 'x_1': 0, 'x_2': 0, 'x_3': 0, 'z': 30.0}},\n",
       " {'bbd_qyxx_id': u'ed3f6b62e6ae4b9e82ffc593c312a79c',\n",
       "  'company_name': u'\\u5317\\u4eac\\u94f6\\u6f6e\\u6295\\u8d44\\u54a8\\u8be2\\u6709\\u9650\\u516c\\u53f8',\n",
       "  'feature_3': {'x_0': 0, 'x_1': 0, 'x_2': 0, 'x_3': 0, 'z': 30.0}},\n",
       " {'bbd_qyxx_id': u'fa2cc0b5f5434c2da8e5691967fe7c00',\n",
       "  'company_name': u'\\u5317\\u4eac\\u8d5b\\u8fea\\u7ecf\\u667a\\u6295\\u8d44\\u987e\\u95ee\\u6709\\u9650\\u516c\\u53f8',\n",
       "  'feature_3': {'x_0': 0, 'x_1': 0, 'x_2': 0, 'x_3': 1, 'z': 22.5}},\n",
       " {'bbd_qyxx_id': u'eb112c788b0c4d42adc76916a71ffc46',\n",
       "  'company_name': u'\\u4e0a\\u6d77\\u7fce\\u6295\\u6295\\u8d44\\u7ba1\\u7406\\u6709\\u9650\\u516c\\u53f8',\n",
       "  'feature_3': {'x_0': 0, 'x_1': 0, 'x_2': 0, 'x_3': 0, 'z': 30.0}},\n",
       " {'bbd_qyxx_id': u'eec338c05c3d45a298dd9fa04f69c988',\n",
       "  'company_name': u'\\u4e2d\\u878d\\u6613\\u5174\\u6295\\u8d44\\u63a7\\u80a1\\u6709\\u9650\\u516c\\u53f8',\n",
       "  'feature_3': {'x_0': 0, 'x_1': 0, 'x_2': 0, 'x_3': 0, 'z': 30.0}},\n",
       " {'bbd_qyxx_id': u'3003713f783f41d2a20378f46f83ffa8',\n",
       "  'company_name': u'\\u4e2d\\u5bfc\\u91d1\\u8f6e\\u8d44\\u4ea7\\u7ba1\\u7406\\u6709\\u9650\\u516c\\u53f8',\n",
       "  'feature_3': {'x_0': 0, 'x_1': 0, 'x_2': 0, 'x_3': 0, 'z': 30.0}},\n",
       " {'bbd_qyxx_id': u'191cc829e02842018f280dcfb2e90e22',\n",
       "  'company_name': u'\\u5317\\u4eac\\u4e54\\u535a\\u534e\\u590f\\u6295\\u8d44\\u7ba1\\u7406\\u6709\\u9650\\u516c\\u53f8',\n",
       "  'feature_3': {'x_0': 0, 'x_1': 0, 'x_2': 0, 'x_3': 1, 'z': 22.5}}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tid_df = spark.read.parquet(\"/user/antifraud/jinli/tiddata/relation_company_info_merge/{version}\".format(version='20170403')).limit(100000)\n",
    "tid_rdd = tid_df.rdd\n",
    "    \n",
    "#最终计算流程\n",
    "tid_rdd_2 = tid_rdd.map(lambda row: (row.a_name, row)) \\\n",
    "    .groupByKey() \\\n",
    "    .repartition(600) \\\n",
    "    .filter(lambda r: len(r[1].data) <= 100000) \\\n",
    "    .cache()\n",
    "\n",
    "feature_list = tid_rdd_2.mapValues(\n",
    "    lambda v: FeatureConstruction.get_some_feature(v, [_ for _ in range(3, 4)])\n",
    ").map(\n",
    "    itemgetter(1)\n",
    ").take(10)\n",
    "\n",
    "feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FeatureConstruction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1663bdd40a5e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mDynamicFeatureConstruction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFeatureConstruction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     '''\n\u001b[0;32m     21\u001b[0m     \u001b[0;31m计\u001b[0m\u001b[0;31m算\u001b[0m\u001b[0;31m特\u001b[0m\u001b[0;31m征\u001b[0m\u001b[0;31m的\u001b[0m\u001b[0;31m函\u001b[0m\u001b[0;31m数\u001b[0m\u001b[0;31m集\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'FeatureConstruction' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "动态风险：需要2个时间节点的中间数据\n",
    "'''\n",
    "import datetime\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "from operator import itemgetter\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "from pandas.io.json import json_normalize\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DynamicFeatureConstruction(FeatureConstruction):\n",
    "    '''\n",
    "    计算特征的函数集\n",
    "    '''      \n",
    "\n",
    "    def __fault_tolerant(func):\n",
    "        '''\n",
    "        一个用于容错的装饰器\n",
    "        '''\n",
    "        @classmethod\n",
    "        def wappen(cls, *args, **kwargs):\n",
    "            try:\n",
    "                return func(cls, *args, **kwargs)\n",
    "            except Exception, e:\n",
    "                return (\n",
    "                    \"{func_name} has a errr : {excp}\"\n",
    "                ).format(func_name=func.__name__, excp=e)\n",
    "        return wappen    \n",
    "    \n",
    "    @classmethod\n",
    "    def get_feature_25(cls):\n",
    "        '''\n",
    "        平台稳态运营风险\n",
    "        '''\n",
    "        nature_person_num = sum([\n",
    "                1\n",
    "                for node, attr in  cls.DIG.nodes_iter(data=True)\n",
    "                if (attr['is_human'] == 1\n",
    "                        and attr['distance'] <= 3)\n",
    "        ])\n",
    "        legal_person_num = sum([\n",
    "                1\n",
    "                for node, attr in  cls.DIG.nodes_iter(data=True)\n",
    "                if (attr['is_human'] == 0\n",
    "                        and attr['distance'] <= 3)\n",
    "        ])\n",
    "        return dict(\n",
    "            nature_person_num=nature_person_num,\n",
    "            legal_person_num=legal_person_num\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def get_feature_26(cls):\n",
    "        '''\n",
    "        平台核心资本运作风险\n",
    "        '''\n",
    "        some_person_sets = [\n",
    "            node\n",
    "            for node, attr in cls.DIG.nodes_iter(data=True) \n",
    "            if (attr['is_human'] == 1\n",
    "                    and attr['distance'] <= 3)]\n",
    "        person_out_degree = cls.DIG.out_degree(\n",
    "            some_person_sets).values()\n",
    "        person_out_degree.sort(reverse=True)\n",
    "        if not person_out_degree:\n",
    "            person_out_degree.append(0)\n",
    "\n",
    "        return dict(\n",
    "            kernel_control_num=sum(person_out_degree[:3])\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def get_feature_27(cls):\n",
    "        '''\n",
    "        可持续性风险\n",
    "        '''\n",
    "        def is_similarity(str_1, str_2):\n",
    "            '''\n",
    "            判断两个字符串是否有连续2个字相同\n",
    "            '''\n",
    "            try:\n",
    "                token_1 = [\n",
    "                    str_1[index] + str_1[index+1] \n",
    "                    for index,data in enumerate(str_1) \n",
    "                    if index < len(str_1) - 1]\n",
    "                is_similarity = sum([\n",
    "                        1 for each_token in token_1 \n",
    "                        if each_token in str_2])\n",
    "                return True if is_similarity > 0 else False        \n",
    "            except:\n",
    "                return False\n",
    "            \n",
    "        #目标公司的名称frag\n",
    "        tar_company = cls.resultiterable.data[0].a_name\n",
    "        tar_company_frag = cls.resultiterable.data[0].a_namefrag\n",
    "        #三度以内，所有关联方的节点集合(不包含自身)\n",
    "        relation_set = [\n",
    "                attr['name']\n",
    "                for node, attr in cls.DIG.nodes_iter(data=True) \n",
    "                if attr['distance'] <= 3]\n",
    "        #relation_set.remove(tar_company)\n",
    "        common_interests_num = sum([\n",
    "                1 \n",
    "                for node_name in relation_set \n",
    "                if is_similarity(tar_company_frag, node_name)\n",
    "        ])\n",
    "        total_legal_person_num = sum([\n",
    "                1\n",
    "                for node, attr in cls.DIG.nodes_iter(data=True) \n",
    "                if (attr['distance'] <= 3 and \n",
    "                       attr['is_human'] == 0)\n",
    "        ])\n",
    "        if total_legal_person_num:\n",
    "            ratio = round(\n",
    "                common_interests_num / total_legal_person_num, 2\n",
    "            )\n",
    "        else:\n",
    "            ratio = 0\n",
    "            \n",
    "        return dict(\n",
    "            ratio=ratio\n",
    "        )\n",
    "        \n",
    "    @classmethod\n",
    "    def get_feature_28(cls):\n",
    "        '''\n",
    "        平台跨区域舞弊风险\n",
    "        '''\n",
    "        province_list = filter(\n",
    "            None, [\n",
    "                attr['province']\n",
    "                for node, attr in cls.DIG.nodes_iter(data=True)\n",
    "                if (attr['distance'] <= 3 and \n",
    "                       attr['is_human'] == 0)]\n",
    "        )\n",
    "        province_distribution = Counter(province_list)\n",
    "        province_top4_num = sum(\n",
    "            map(lambda (k, v): v, province_distribution.most_common(4)))\n",
    "        \n",
    "        return dict(\n",
    "            province_top4_num=province_top4_num\n",
    "        )\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def get_some_feature(cls, resultiterable, feature_nums):\n",
    "        #创建类变量\n",
    "        cls.resultiterable = resultiterable\n",
    "        cls.tarcompany = cls.resultiterable.data[0].a\n",
    "        cls.DIG = cls.create_graph_udf(cls.resultiterable, 1)\n",
    "        cls.G = cls.create_graph_udf(cls.resultiterable, 0)\n",
    "        \n",
    "        feature_list = [\n",
    "            ('feature_{0}'.format(feature_index), \n",
    "                     eval('cls.get_feature_{0}()'.format(feature_index)))\n",
    "            for feature_index in feature_nums]\n",
    "        feature_list.append(('bbd_qyxx_id', cls.tarcompany))\n",
    "        feature_list.append(('company_name', cls.resultiterable.data[0].a_name))\n",
    "        \n",
    "        return dict(feature_list) #json.dumps(dict(feature_list), ensure_ascii=False)      \n",
    "\n",
    "    \n",
    "def get_dynamic_risk(data):\n",
    "    '''\n",
    "    计算动态风险\n",
    "    '''\n",
    "    old_data, new_data = data\n",
    "    #平台稳态运营风险\n",
    "    if old_data['feature_25']['nature_person_num']:\n",
    "        nature_person_risk = ((new_data['feature_25']['nature_person_num'] - \n",
    "                                            old_data['feature_25']['nature_person_num']) / \n",
    "                                            old_data['feature_25']['nature_person_num'])\n",
    "    else:\n",
    "        nature_person_risk = 0.\n",
    "    if old_data['feature_25']['legal_person_num']:\n",
    "        legal_person_risk = ((new_data['feature_25']['legal_person_num'] - \n",
    "                                          old_data['feature_25']['legal_person_num']) / \n",
    "                                          old_data['feature_25']['legal_person_num'])\n",
    "    else:\n",
    "        legal_person_risk = 0.\n",
    "    feature_25 = 5. * (nature_person_risk + legal_person_risk)\n",
    "    \n",
    "    #平台核心资本运作风险\n",
    "    if old_data['feature_26']['kernel_control_num']:\n",
    "        feature_26 = (15. * \n",
    "                              new_data['feature_26']['kernel_control_num'] / \n",
    "                              old_data['feature_26']['kernel_control_num'])\n",
    "    else:\n",
    "        feature_26 = 0.\n",
    "        \n",
    "    #可持续性风险\n",
    "    if old_data['feature_27']['ratio']:\n",
    "        feature_27 = (4. * \n",
    "                              new_data['feature_27']['ratio'] / \n",
    "                              old_data['feature_27']['ratio'])\n",
    "    else:\n",
    "        feature_27 = 0.\n",
    "        \n",
    "    #平台跨区域舞弊风险\n",
    "    if old_data['feature_28']['province_top4_num']:\n",
    "        feature_28 = (15. * \n",
    "                              new_data['feature_28']['province_top4_num'] / \n",
    "                              old_data['feature_28']['province_top4_num'])\n",
    "    else:\n",
    "        feature_28 = 0.\n",
    "        \n",
    "    return dict(\n",
    "        feature_25=round(feature_25, 2),\n",
    "        feature_26=round(feature_26, 2),\n",
    "        feature_27=round(feature_27, 2),\n",
    "        feature_28=round(feature_28, 2)\n",
    "    )\n",
    "    \n",
    "def spark_data_flow(tid_old_version, tid_new_version):\n",
    "    '''\n",
    "    dataflow\n",
    "    '''\n",
    "    #输入\n",
    "    #算法设计直接用rdd\n",
    "    tid_old_df = spark.read.parquet(\n",
    "        \"/user/antifraud/jinli/tiddata/relation_company_info_merge/{version}\".format(version=tid_old_version))\n",
    "    tid_old_rdd = tid_old_df.rdd\n",
    "    tid_new_df = spark.read.parquet(\n",
    "        \"/user/antifraud/jinli/tiddata/relation_company_info_merge/{version}\".format(version=tid_new_version))\n",
    "    tid_new_rdd = tid_new_df.rdd    \n",
    "        \n",
    "    #最终计算流程\n",
    "    tid_new_rdd_2 = tid_new_rdd.map(lambda row: (row.a_name, row)) \\\n",
    "        .groupByKey() \\\n",
    "        .repartition(600) \\\n",
    "        .filter(lambda r: len(r[1].data) <= 150000) \\\n",
    "        .cache()\n",
    "    tid_old_rdd_2 = tid_old_rdd.map(lambda row: (row.a_name, row)) \\\n",
    "        .groupByKey() \\\n",
    "        .repartition(600) \\\n",
    "        .filter(lambda r: len(r[1].data) <= 150000) \\\n",
    "        .cache()\n",
    "\n",
    "    feature_new_list = tid_new_rdd_2.mapValues(\n",
    "        lambda v: DynamicFeatureConstruction.get_some_feature(\n",
    "            v, [_ for _ in range(25, 29)]))\n",
    "    feature_old_list = tid_old_rdd_2.mapValues(\n",
    "        lambda v: DynamicFeatureConstruction.get_some_feature(\n",
    "            v, [_ for _ in range(25, 29)]))\n",
    "    \n",
    "    dynamic_risk_data = feature_old_list.join(\n",
    "        feature_new_list\n",
    "    ).mapValues(\n",
    "        get_dynamic_risk\n",
    "    ).map(\n",
    "        lambda (k, v): dict({'company_name': k}, **v)\n",
    "    ).collect()\n",
    "    \n",
    "    \n",
    "    return dynamic_risk_data\n",
    "\n",
    "def collect_to_driver(version, out_path, old_version, new_version):\n",
    "    '''\n",
    "    格式化输出\n",
    "    '''\n",
    "    src_time = time.time()\n",
    "    dynamic_risk_data = json_normalize(spark_data_flow(old_version, new_version))\n",
    "    os.system(\"rm /data5/antifraud/Jinli/data/prddata/prd_jinli_dynamic_feature_score_distribution_{version}.xlsx\".format(version=version))\n",
    "    dynamic_risk_data.to_excel(\"{path}/prd_jinli_dynamic_feature_score_distribution_{version}.xlsx\".format(path=OUTPATH, version=version))\n",
    "    des_time = time.time()    \n",
    "    \n",
    "    print 'SUCCESS !! \\n 耗时：{0}'.format(des_time - src_time)\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':  \n",
    "    #输出参数\n",
    "    VERSION = \"20170519_1\"\n",
    "    OUTPATH = \"/data5/antifraud/Jinli/data/prddata\"\n",
    "    \n",
    "    collect_to_driver(\n",
    "        version=VERSION,\n",
    "        out_path=OUTPATH,\n",
    "        old_version='20170117',\n",
    "        new_version='20170403'\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "新版动态风险：需要2个时间节点的中间数据\n",
    "'''\n",
    "import datetime\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "from operator import itemgetter\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "from pandas.io.json import json_normalize\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_dynamic_risk(data):\n",
    "    '''\n",
    "    计算动态风险\n",
    "    '''\n",
    "    old_data, new_data = data\n",
    "    \n",
    "    def get_result(my_denominator, my_numerator):\n",
    "        '''特殊计算规则，嘻嘻'''\n",
    "        if not denominator and  numerator:\n",
    "            result = 65535.\n",
    "        elif not denominator and  not numerator:\n",
    "            result = 0.\n",
    "        else:\n",
    "            result = numerator * 1. / denominator \n",
    "        return result\n",
    "\n",
    "    #3度及3度以下自然人节点变动率\n",
    "    denominator = (old_data['feature_16']['y_1'] + \n",
    "                               old_data['feature_16']['y_2'] +\n",
    "                               old_data['feature_16']['y_3'])\n",
    "    numerator = (new_data['feature_16']['y_1'] + \n",
    "                               new_data['feature_16']['y_2'] +\n",
    "                               new_data['feature_16']['y_3']) - denominator\n",
    "    a_1 = get_result(denominator, numerator)\n",
    "        \n",
    "    #对外投资公司数量变动率\n",
    "    denominator = old_data['feature_17']['x']\n",
    "    numerator = new_data['feature_17']['x'] - denominator\n",
    "    a_4 = get_result(denominator, numerator)\n",
    "    \n",
    "    #公司分支机构数量变动率\n",
    "    denominator = old_data['feature_18']['d']\n",
    "    numerator = new_data['feature_18']['d'] - denominator\n",
    "    a_5 = get_result(denominator, numerator)\n",
    "    \n",
    "    #利益一致行动法人的数量变动率\n",
    "    denominator = old_data['feature_22']['d']\n",
    "    numerator = new_data['feature_22']['d'] - denominator\n",
    "    a_6 = get_result(denominator, numerator)\n",
    "    \n",
    "    #法定代表人变更次数变动率\n",
    "    denominator = old_data['feature_6']['c_1']\n",
    "    numerator = new_data['feature_6']['c_1'] - denominator\n",
    "    b_1 = get_result(denominator, numerator)\n",
    "    \n",
    "    #股东变更次数变动率\n",
    "    denominator = old_data['feature_6']['c_2']\n",
    "    numerator = new_data['feature_6']['c_2'] - denominator\n",
    "    b_2 = get_result(denominator, numerator)\n",
    "    \n",
    "    #注册资本变更次数变动率\n",
    "    denominator = old_data['feature_6']['c_3']\n",
    "    numerator = new_data['feature_6']['c_3'] - denominator\n",
    "    b_3 = get_result(denominator, numerator)\n",
    "    \n",
    "    #大专及大专以下或不限专业招聘人数变动率\n",
    "    denominator = old_data['feature_7']['e_1']\n",
    "    numerator = new_data['feature_7']['e_1'] - denominator\n",
    "    c_1 = get_result(denominator, numerator)\n",
    "    \n",
    "    #3度及3度以下核心自然人（前3）控制节点总数变动率\n",
    "    denominator = old_data['feature_1']['r_4']\n",
    "    numerator = new_data['feature_1']['r_4'] - denominator\n",
    "    d_2 = get_result(denominator, numerator)\n",
    "    \n",
    "    \n",
    "    return dict(\n",
    "        feature_26={\n",
    "            'a_1': a_1,\n",
    "            'a_4': a_4,\n",
    "            'a_5': a_5,\n",
    "            'a_6': a_6,\n",
    "            'b_1': b_1,\n",
    "            'b_2': b_2,\n",
    "            'b_3': b_3,\n",
    "            'c_1': c_1,\n",
    "            'd_2': d_2\n",
    "        }\n",
    "    )\n",
    "    \n",
    "def spark_data_flow(tid_old_version, tid_new_version):\n",
    "    '''\n",
    "    dataflow\n",
    "    '''\n",
    "    #输入\n",
    "    #算法设计直接用rdd\n",
    "    tid_old_df = spark.read.json(\n",
    "        \"/user/antifraud/hongjing2/dataflow/step_one/prd/common_static_feature_distribution_v2/{version}\".format(version=tid_old_version))\n",
    "    feature_old_rdd = tid_old_df.rdd.map(lambda r: (r.bbd_qyxx_id, r))\n",
    "    tid_new_df = spark.read.json(\n",
    "        \"/user/antifraud/hongjing2/dataflow/step_one/prd/common_static_feature_distribution_v2/{version}\".format(version=tid_new_version))\n",
    "    feature_new_rdd = tid_new_df.rdd.map(lambda r: (r.bbd_qyxx_id, r))\n",
    "        \n",
    "    #最终计算流程\n",
    "    dynamic_risk_data = feature_old_rdd.join(\n",
    "        feature_new_rdd\n",
    "    ).mapValues(\n",
    "        get_dynamic_risk\n",
    "    ).map(\n",
    "        lambda (k, v): dict({'company_name': k}, **v)\n",
    "    ).map(\n",
    "        lambda data: json.dumps(data, ensure_ascii=False)\n",
    "    )\n",
    "    \n",
    "    return dynamic_risk_data\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':  \n",
    "    #输出参数\n",
    "    VERSION = \"20170519_1\"\n",
    "    OUTPATH = \"/data5/antifraud/Jinli/data/prddata\"\n",
    "    \n",
    "    prd_rdd = spark_data_flow(\n",
    "        tid_old_version='20170117',\n",
    "        tid_new_version='20170117'\n",
    "    ).cache()\n",
    "\n",
    "    print prd_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "测试mysql存储有向属性图的性能\n",
    "1、将某家公司的数据提取出来\n",
    "2、生成属性图，序列化，写入mysql\n",
    "3、读取属性图，反序列化，计算特征\n",
    "'''\n",
    "\n",
    "# 1\n",
    "from operator import itemgetter\n",
    "tidversion='20170410'\n",
    "    \n",
    "    \n",
    "tid_df = spark.read.parquet(\"/user/antifraud/jinli/tiddata/relation_company_info_merge/{version}\".format(version=tidversion))\n",
    "tid_rdd = tid_df.rdd\n",
    "    \n",
    "#最终计算流程\n",
    "tid_rdd_2 = tid_rdd.filter(lambda row: row.a == u'be29fc2b652d4eb7b1fdcfecfa6a9edf').map(lambda row: (row.a_name, row)).groupByKey() \n",
    "data = tid_rdd_2.collect()\n",
    "\n",
    "\n",
    "\n",
    "# 2\n",
    "\n",
    "import pickle\n",
    "\n",
    "IP = '10.10.20.180'\n",
    "USER =  'airflow'\n",
    "PASSWORD = 'airflow'\n",
    "DB_NAME = 'airflow'\n",
    "\n",
    "def insert_table(table):\n",
    "    '''\n",
    "    连接mysql，执行一个SQL\n",
    "    '''\n",
    "    db = MySQLdb.connect(host=IP, user=USER, passwd=PASSWORD, db=DB_NAME)\n",
    "    # 使用cursor()方法获取操作游标 \n",
    "    cursor = db.cursor()\n",
    "    # 使用execute方法执行SQL语句\n",
    "    sql = \"\"\"INSERT INTO {table}(bbd_qyxx_id,\n",
    "         graph_obj)\n",
    "         VALUES ('{id}', \"{obj}\" )\"\"\".format(table=table, id='be29fc2b652d4eb7b1fdcfecfa6a9edf', obj=pickle.dumps(dig).encode('string-escape'))\n",
    "    try:\n",
    "        # 执行SQL语句\n",
    "        cursor.execute(sql)\n",
    "        # 提交到数据库执行\n",
    "        db.commit()\n",
    "        print \"插入{0}成功\".format(table)\n",
    "    except Exception as e:\n",
    "        # 发生错误时回滚\n",
    "        db.rollback()\n",
    "        print e\n",
    "    # 关闭数据库连接\n",
    "    db.close()\n",
    "    \n",
    "    \n",
    "insert_table('my_test_graph_object')\n",
    "\n",
    "\n",
    "# 3\n",
    "\n",
    "import pickle\n",
    "import MySQLdb\n",
    "import time \n",
    "import cProfile\n",
    "\n",
    "IP = '10.10.20.180'\n",
    "USER =  'airflow'\n",
    "PASSWORD = 'airflow'\n",
    "DB_NAME = 'airflow'\n",
    "\n",
    "def select_table(table):\n",
    "    '''\n",
    "    连接mysql，执行一个SQL\n",
    "    '''\n",
    "    db = MySQLdb.connect(host=IP, user=USER, passwd=PASSWORD, db=DB_NAME)\n",
    "    # 使用cursor()方法获取操作游标 \n",
    "    cursor = db.cursor()\n",
    "    # 使用execute方法执行SQL语句\n",
    "    sql = \"\"\"SELECT * FROM {table} LIMIT 1\"\"\".format(table=table)\n",
    "    try:\n",
    "        # 执行SQL语句\n",
    "        cursor.execute(sql)\n",
    "        # 查询数据\n",
    "        pickled_obj = cursor.fetchall()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print e\n",
    "    # 关闭数据库连接\n",
    "    db.close()\n",
    "    \n",
    "    return pickled_obj\n",
    "\n",
    "\n",
    "\n",
    "d = select_table('my_test_graph_object')\n",
    "dig = pickle.loads(d[0][1])\n",
    "src_time = time.time()\n",
    "#feature_distribution = FeatureConstruction.get_some_feature(data[0][1], [_ for _ in range(1, 25)])\n",
    "#cProfile.run(\"FeatureConstruction.get_some_feature(data[0][1], [_ for _ in range(1, 25)])\")\n",
    "des_time = time.time()\n",
    "\n",
    "print des_time - src_time\n",
    "\n",
    "feature_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "单个公司DEBUG\n",
    "'''\n",
    "tid_df = spark.read.parquet(\"/user/antifraud/jinli/tiddata/relation_company_info_merge\")\n",
    "tid_rdd = tid_df.where(tid_df.a_name == u'上海诺戎电子科技有限公司').rdd\n",
    "\n",
    "#最终计算流程\n",
    "tid_rdd_2 = tid_rdd.map(lambda row: (row.a, row)) \\\n",
    "    .groupByKey(400).filter(lambda r: len(r[1].data) <= 150000) \\\n",
    "    .cache()\n",
    "\n",
    "#输出\n",
    "VERSION = \"20170301\"\n",
    "OUTPATH = \"/data5/antifraud/Jinli/data/prddata\"\n",
    "\n",
    "tid_rdd_2.mapValues(\n",
    "        FeatureConstruction.get_all_feature).map(itemgetter(1)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|sum(estatus)|\n",
      "+------------+\n",
      "|           0|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "BUG修复测试\n",
    "'''\n",
    "from pyspark.sql import functions as fun\n",
    "\n",
    "tmp_df = spark.read.parquet(\"/user/antifraud/jinli/tiddata/relation_company_info_merge\")\n",
    "tid_1 = tmp_df.where(tmp_df.a == '5ef468297555411aba041a16c4fad4c5') \\\n",
    ".where(tmp_df.b_degree == 1).select(\n",
    "    'b',\n",
    "    tmp_df.b_estatus.alias('estatus')\n",
    ")\n",
    "tid_2 = tmp_df.where(tmp_df.a == '5ef468297555411aba041a16c4fad4c5') \\\n",
    ".where(tmp_df.c_degree == 1).select(\n",
    "    'c',\n",
    "    tmp_df.c_estatus.alias('estatus')\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tid_2.union(tid_1).distinct().select(fun.sum('estatus')).show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'ce5277ab116a4ab89bdfab7bdfb7a63a', 32512),\n",
       " (u'be29fc2b652d4eb7b1fdcfecfa6a9edf', 25229),\n",
       " (u'79941af7fec84db1a06169da06bba181', 25229),\n",
       " (u'47c3144831574dceb0268b86974e371e', 24271),\n",
       " (u'964c8879b5bd48aca460530b5ef3b664', 23514),\n",
       " (u'68129f3e9c4f4d319160b8502d90348d', 23242),\n",
       " (u'acaebbfefd364f258b84c8e0457b8b8f', 22848),\n",
       " (u'3744963e8ca04715ac30a604799e6c68', 22242),\n",
       " (u'97d29bb53fa64a8c8e159a487d9c887b', 21333),\n",
       " (u'd5c13f432308484098ce1022b0bcfeed', 20937)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "小工具：用来看数据倾斜的，输出: [(企业ID，关联方数)]\n",
    "'''\n",
    "\n",
    "tid_df = spark.read.parquet(\"/user/antifraud/jinli/tiddata/relation_company_info_merge/20170403\")\n",
    "tid_rdd = tid_df.rdd\n",
    "tid_rdd_2 = tid_rdd.map(lambda row: (row.a, row)).groupByKey(800).cache()\n",
    "realation_strip = tid_rdd_2.mapValues(lambda x: len(x.data)).collect()\n",
    "\n",
    "import operator\n",
    "sorted(realation_strip, key=operator.itemgetter(1), reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1683"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.csv(\"/user/antifraud/jinli/rawdata/tar_company/raw_jl_company_list_20170514_1.data\")\n",
    "df.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /data5/antifraud/source/tmp_20170413/hongjing_jinli_static_kpi.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /data5/antifraud/source/tmp_20170413/hongjing_jinli_static_kpi.py\n",
    "# -*- coding: utf-8 -*-\n",
    "import datetime\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "from operator import itemgetter\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as fun\n",
    "from pyspark.sql import types as tp\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pandas.io.json import json_normalize\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class FeatureConstruction(object):\n",
    "    '''\n",
    "    计算特征的函数集\n",
    "    '''      \n",
    "\n",
    "    def __fault_tolerant(func):\n",
    "        '''\n",
    "        一个用于容错的装饰器\n",
    "        '''\n",
    "        @classmethod\n",
    "        def wappen(cls, *args, **kwargs):\n",
    "            try:\n",
    "                return func(cls, *args, **kwargs)\n",
    "            except Exception, e:\n",
    "                return (\n",
    "                    \"{func_name} has a errr : {excp}\"\n",
    "                ).format(func_name=func.__name__, excp=e)\n",
    "        return wappen    \n",
    "    \n",
    "    @__fault_tolerant    \n",
    "    def create_graph_udf(cls, relations, is_directed):\n",
    "        '''\n",
    "        根据关联方的结构创建有向、无向图\n",
    "        '''\n",
    "        def init_graph(edge_list, node_list, is_directed=0):\n",
    "            #网络初始化\n",
    "            G = nx.DiGraph() if is_directed == 1 else nx.Graph()    \n",
    "            #增加带属性的节点\n",
    "            for node in node_list:\n",
    "                G.add_node(node[0], attr_dict=node[1])\n",
    "            #增加带属性的边\n",
    "            G.add_edges_from(edge_list)\n",
    "            return G\n",
    "            \n",
    "        #生成一个图\n",
    "        company_correlative_edges = [\n",
    "            (row.b, row.c, {'is_invest': row.bc_relation}) for row in relations]\n",
    "                                     \n",
    "        company_correlative_nodes = [(\n",
    "                row.b, \n",
    "                dict(\n",
    "                    is_human=row.b_isperson,\n",
    "                    is_black=row.b_is_black_company,\n",
    "                    distance = row.b_degree,\n",
    "                    name = row.b_name,\n",
    "                    isSOcompany = row.b_isSOcompany,\n",
    "                    esdate = row.b_regtime,\n",
    "                    ktgg = row.b_ktgg,\n",
    "                    zgcpwsw = row.b_zgcpwsw,\n",
    "                    rmfygg = row.b_rmfygg,\n",
    "                    lending = row.b_lending,\n",
    "                    xzcf = row.b_xzcf,\n",
    "                    zhixing = row.b_zhixing,\n",
    "                    dishonesty = row.b_dishonesty,\n",
    "                    jyyc = row.b_jyyc,\n",
    "                    circxzcf = row.b_circxzcf,\n",
    "                    opescope = row.b_opescope,\n",
    "                    address = row.b_address,\n",
    "                    estatus = row.b_estatus,\n",
    "                    province = row.b_province\n",
    "                ))  for row in relations] + [(\n",
    "                row.c, \n",
    "                dict(\n",
    "                    is_human=row.c_isperson,\n",
    "                    is_black=row.c_is_black_company,\n",
    "                    distance = row.c_degree,\n",
    "                    name = row.c_name,\n",
    "                    isSOcompany = row.c_isSOcompany,\n",
    "                    esdate = row.c_regtime,\n",
    "                    ktgg = row.c_ktgg,\n",
    "                    zgcpwsw = row.c_zgcpwsw,\n",
    "                    rmfygg = row.c_rmfygg,\n",
    "                    lending = row.c_lending,\n",
    "                    xzcf = row.c_xzcf,\n",
    "                    zhixing = row.c_zhixing,\n",
    "                    dishonesty = row.c_dishonesty,\n",
    "                    jyyc = row.c_jyyc,\n",
    "                    circxzcf = row.c_circxzcf,\n",
    "                    opescope = row.c_opescope,\n",
    "                    address = row.c_address,\n",
    "                    estatus = row.c_estatus,\n",
    "                    province = row.c_province\n",
    "                )) for row in relations]\n",
    "        \n",
    "        if is_directed == 1:\n",
    "            g = init_graph(\n",
    "                company_correlative_edges,\n",
    "                company_correlative_nodes, is_directed = 1) \n",
    "        else:\n",
    "            g = init_graph(\n",
    "                company_correlative_edges, \n",
    "                company_correlative_nodes, is_directed = 0)\n",
    "        return g\n",
    "\n",
    "    @__fault_tolerant\n",
    "    def get_feature_1(cls):\n",
    "        '''\n",
    "        企业背景风险\n",
    "        '''\n",
    "        one_relation_set = [\n",
    "            node for node, attr in cls.DIG.nodes_iter(data=True) if attr['distance'] == 1]\n",
    "        shareholder = [\n",
    "            src_node for src_node, des_node, edge_attr \n",
    "            in cls.DIG.edges_iter(one_relation_set, data=True) \n",
    "            if des_node == cls.tarcompany \n",
    "            and edge_attr['is_invest'] == 'INVEST']\n",
    "        os_shareholder = [\n",
    "            node for node in shareholder \n",
    "            if cls.DIG.node[node]['isSOcompany'] is True \n",
    "            and cls.DIG.node[node]['is_human'] == 0]\n",
    "        anti_os_shareholder = [\n",
    "            node for node in shareholder \n",
    "            if cls.DIG.node[node]['isSOcompany'] is False \n",
    "            and cls.DIG.node[node]['is_human'] == 0]\n",
    "        nature_shareholder = [\n",
    "            node for node in shareholder \n",
    "            if cls.DIG.node[node]['is_human'] == 1]\n",
    "        is_ipo_tarcompany = 1 if cls.resultiterable.data[0].a_isIPOcompany is True else 0\n",
    "    \n",
    "        x = len(os_shareholder)\n",
    "        y = len(anti_os_shareholder)\n",
    "        z = len(nature_shareholder)\n",
    "        w = len(shareholder)\n",
    "        r_i = is_ipo_tarcompany\n",
    "        r = (0.2*x + 0.5*y + z) * 100. * (2 - r_i) / (2*w + 0.001) \n",
    "        \n",
    "        return dict(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            z=z,\n",
    "            w=w,\n",
    "            r_i=r_i,\n",
    "            r=round(r, 2) if r else 100\n",
    "        )\n",
    "\n",
    "\n",
    "    @__fault_tolerant\n",
    "    def get_feature_2(cls):\n",
    "        '''\n",
    "        资本风险\n",
    "        '''\n",
    "        x = cls.resultiterable.data[0].a_regcap \n",
    "        y = cls.resultiterable.data[0].a_realcap\n",
    "        \n",
    "        x = x / 10000 if x != 'NULL' and x is not None else 0\n",
    "        y = y / 10000 if y != 'NULL' and y is not None else 0\n",
    "        \n",
    "        if 0 <= x < 500:\n",
    "            c_i = 100\n",
    "        elif 500 <= x < 1000:\n",
    "            c_i = 60\n",
    "        elif 1000 <= x < 5000:\n",
    "            c_i = 30\n",
    "        elif 5000 <= x:\n",
    "            c_i = 10\n",
    "        \n",
    "        p_i = 1 if y / (x + 0.001) >= 0.5 else 0\n",
    "        \n",
    "        return  dict(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            c=c_i * (1 - p_i/2.)\n",
    "        )\n",
    "    \n",
    "    @__fault_tolerant\n",
    "    def get_feature_3(cls):\n",
    "        '''\n",
    "        公司地域风险\n",
    "        '''\n",
    "        province_black_num = cls.resultiterable.data[0].a_province_black_num\n",
    "        province_leijinrong_num = cls.resultiterable.data[0].a_province_leijinrong_num\n",
    "        province_black_num = province_black_num if province_black_num else 0\n",
    "        province_leijinrong_num = province_leijinrong_num if province_leijinrong_num else 0\n",
    "        \n",
    "        return dict(\n",
    "            j_1=province_black_num,\n",
    "            j_2=province_leijinrong_num,\n",
    "            j=round(province_black_num / (province_leijinrong_num + 0.0001), 4)\n",
    "        )\n",
    "    \n",
    "    @__fault_tolerant\n",
    "    def get_feature_4(cls):\n",
    "        '''\n",
    "        知识产权风险（专利和商标）\n",
    "        '''\n",
    "        k_1 = cls.resultiterable.data[0].a_zhuanli\n",
    "        k_2 = cls.resultiterable.data[0].a_shangbiao \n",
    "        k_1 = k_1 if k_1 != 'NULL' and k_1 is not None else 0\n",
    "        k_2 = k_2 if k_1 != 'NULL' and k_2 is not None else 0\n",
    "        k_3 = k_1 + k_2\n",
    "        \n",
    "        if k_3 == 0:\n",
    "            k = 100\n",
    "        elif 1 <= k_3 < 5:\n",
    "            k = 60\n",
    "        elif 5 <= k_3 < 10:\n",
    "            k = 30\n",
    "        elif 10 <= k_3:\n",
    "            k = 10\n",
    "        \n",
    "        return dict(\n",
    "            k_1=k_1,\n",
    "            k_2=k_2,\n",
    "            k=k\n",
    "        )\n",
    "        \n",
    "    @__fault_tolerant\n",
    "    def get_feature_5(cls):\n",
    "        '''\n",
    "        域名备案风险\n",
    "        '''\n",
    "        icp = cls.resultiterable.data[0].a_ICP\n",
    "        url = cls.resultiterable.data[0].a_url\n",
    "        \n",
    "        c_i = 1 if icp != 'NULL' and icp is not None else 0   \n",
    "        try:\n",
    "            if url is not None and url != 'NULL':\n",
    "                status_code = requests.get(url, allow_redirects=False).status_code \n",
    "                p_i = 1 if status_code == 200 else 0\n",
    "            else:\n",
    "                p_i = 0\n",
    "        except:\n",
    "            p_i = 0\n",
    "        \n",
    "        l = (1 - c_i/2.) * (1 - p_i/2.) * 100\n",
    "        \n",
    "        return dict(\n",
    "            c_i=c_i,\n",
    "            p_i=p_i,\n",
    "            l=l\n",
    "        )\n",
    "        \n",
    "    @__fault_tolerant\n",
    "    def get_feature_6(cls):\n",
    "        '''\n",
    "        工商变更风险\n",
    "        '''\n",
    "        def get_bgxx_symbol(bgxx_name):\n",
    "            if u'法定代表人' in bgxx_name:\n",
    "                return 'c_1'\n",
    "            elif u'股东' in bgxx_name:\n",
    "                return 'c_2'\n",
    "            elif u'注册资本' in bgxx_name:\n",
    "                return 'c_3'\n",
    "            elif u'高管' in bgxx_name:\n",
    "                return 'c_4'\n",
    "            elif u'经营范围' in bgxx_name:\n",
    "                return 'c_5'\n",
    "            else:\n",
    "                return 'c_6'\n",
    "        \n",
    "        default_result = OrderedDict([\n",
    "                ('c_1', 0),\n",
    "                ('c_2', 0),\n",
    "                ('c_3', 0),\n",
    "                ('c_4', 0),\n",
    "                ('c_5', 0),\n",
    "                ('c_6', 0),\n",
    "                ('r', 0),\n",
    "                ('z', 10)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        if cls.resultiterable.data[0].a_bgxx is not None:\n",
    "            for bgxx_name, bgxx_num in cls.resultiterable.data[0].a_bgxx.iteritems():\n",
    "                default_result[get_bgxx_symbol(bgxx_name)] += int(bgxx_num)\n",
    "            default_result['r'] = np.dot(default_result.values(), [2, 1, 1, 1, 2, 0, 0, 0])\n",
    "        \n",
    "            if default_result['r'] == 0:\n",
    "                default_result['z'] = 10\n",
    "            elif 1 <= default_result['r'] < 5:\n",
    "                default_result['z'] = 30\n",
    "            elif 5 <= default_result['r'] < 10:\n",
    "                default_result['z'] = 60\n",
    "            elif 10 <= default_result['r']:\n",
    "                default_result['z'] = 100\n",
    "                \n",
    "        default_result.pop('c_6')\n",
    "        \n",
    "        return dict(default_result)\n",
    "        \n",
    "    @__fault_tolerant\n",
    "    def get_feature_7(cls):\n",
    "        '''\n",
    "        人才结构风险\n",
    "        '''        \n",
    "        def get_recruit_num(each_recruit):\n",
    "            if u'本科' in each_recruit:\n",
    "                return 'e_2'\n",
    "            elif u'硕士' in each_recruit:\n",
    "                return 'e_3'\n",
    "            elif u'博士' in each_recruit:\n",
    "                return 'e_3'\n",
    "            else:\n",
    "                return 'e_1'\n",
    "            \n",
    "        default_result = OrderedDict([\n",
    "                ('e_1', 0),\n",
    "                ('e_2', 0),\n",
    "                ('e_3', 0),\n",
    "                ('e', 0),\n",
    "                ('r', 0),\n",
    "                ('z', 10)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        if cls.resultiterable.data[0].a_recruit is not None:\n",
    "            for education_name, education_num in cls.resultiterable.data[0].a_recruit.iteritems():\n",
    "                default_result[get_recruit_num(education_name)] += int(education_num)\n",
    "            default_result['e'] = sum(\n",
    "                map(int ,cls.resultiterable.data[0].a_recruit.values()))\n",
    "            default_result['r'] = round(\n",
    "                np.dot(default_result.values(), [100., 60., 30., 0, 0, 0]) / default_result['e'], 2)\n",
    "            \n",
    "            if default_result['r'] == 0:\n",
    "                default_result['z'] = 10\n",
    "            elif 0 < default_result['r'] < 60:\n",
    "                default_result['z'] = 30\n",
    "            elif 60 <= default_result['r'] < 80:\n",
    "                default_result['z'] = 60\n",
    "            elif 80 <= default_result['r']:\n",
    "                default_result['z'] = 100         \n",
    "            \n",
    "        return dict(default_result)\n",
    "        \n",
    "    @__fault_tolerant\n",
    "    def get_feature_8(cls):\n",
    "        '''\n",
    "        公司运营持续风险\n",
    "        '''\n",
    "        esdate_relation_set = [\n",
    "            (datetime.date.today() - attr['esdate']).days \n",
    "            for node, attr in cls.DIG.nodes_iter(data=True) \n",
    "            if attr['distance'] <= 3 \n",
    "            and attr['is_human'] == 0 \n",
    "            and attr['esdate'] is not None]\n",
    "        t = round(np.average(esdate_relation_set), 2)\n",
    "        \n",
    "        if 0 <= t < 365:\n",
    "            y = 100\n",
    "        elif 365 <= t < 1825:\n",
    "            y = 60\n",
    "        elif 1825 <= t < 3650:\n",
    "            y = 30\n",
    "        elif 3650 <= t:\n",
    "            y = 10\n",
    "        else:\n",
    "            y = 100\n",
    "        return dict(\n",
    "            t=t if t else 0,\n",
    "            y=y\n",
    "        )\n",
    "\n",
    "        \n",
    "    @__fault_tolerant\n",
    "    def get_feature_9(cls):\n",
    "        '''\n",
    "        招标和中标风险\n",
    "        '''\n",
    "        n_1 = cls.resultiterable.data[0].a_zhaobiao\n",
    "        n_2 = cls.resultiterable.data[0].a_zhongbiao\n",
    "        n_1 = n_1 if n_1 is not None else 0\n",
    "        n_2 = n_2 if n_2 is not None else 0\n",
    "        \n",
    "        n = n_1 + n_2\n",
    "        \n",
    "        if n == 0:\n",
    "            n = 100\n",
    "        elif 1 <= n < 5:\n",
    "            n = 60\n",
    "        elif 5<= n < 10:\n",
    "            n = 30\n",
    "        else:\n",
    "            n = 10\n",
    "        \n",
    "        return  dict(\n",
    "            n_1=n_1,\n",
    "            n_2=n_2,\n",
    "            n=n\n",
    "        )\n",
    "        \n",
    "    @__fault_tolerant\n",
    "    def get_feature_xgxx(cls):\n",
    "        '''\n",
    "        【计算基础数据】\n",
    "        法律诉讼风险：开庭公告、裁判文书、法院公告、民间借贷\n",
    "        行政处罚\n",
    "        被执行风险\n",
    "        异常经营风险：经营异常、吊销&注销\n",
    "        银监会行政处罚\n",
    "        ''' \n",
    "        def get_certain_distance_all_info(distance, document_types):\n",
    "            all_array = []\n",
    "            #处理某一个distance不存在节点的情况\n",
    "            all_array.append([0]*len(document_types))\n",
    "            for node, attr in cls.DIG.nodes_iter(data=True):\n",
    "                if attr['is_human'] == 0  and attr['distance'] == distance:\n",
    "                    each_array = map(lambda x: x if x else 0, \n",
    "                                                  [attr[each_document] \n",
    "                                                      for each_document in document_types])\n",
    "                    all_array.append(each_array)\n",
    "                else:\n",
    "                    continue\n",
    "            documents_num = np.sum(all_array, axis=0)\n",
    "            return documents_num\n",
    "        \n",
    "        matrx = dict()\n",
    "        xgxx_type = ['ktgg', 'zgcpwsw', 'rmfygg', \n",
    "                             'lending', 'xzcf', 'zhixing', \n",
    "                             'dishonesty', 'jyyc', 'circxzcf', \n",
    "                             'estatus']\n",
    "\n",
    "        for each_distance in xrange(0, 4):\n",
    "            xgxx_num_list = get_certain_distance_all_info(each_distance, \n",
    "                                                                                      xgxx_type)\n",
    "            matrx[each_distance] = dict(zip(xgxx_type, xgxx_num_list))\n",
    "            \n",
    "        return matrx\n",
    "    \n",
    "    @__fault_tolerant\n",
    "    def filter_xgxx_type(cls, tar_xgxx):\n",
    "        '''\n",
    "        过滤相关信息\n",
    "        '''\n",
    "        risk = dict()\n",
    "        for each_distance, xgxx_statistics in cls.xgxx_distribution.iteritems():\n",
    "            tar_xgxx_statistics = dict([\n",
    "                    (each_xgxx_type, each_xgxx_num)\n",
    "                    for each_xgxx_type, each_xgxx_num \n",
    "                    in xgxx_statistics.iteritems() \n",
    "                    if each_xgxx_type in tar_xgxx])\n",
    "            risk[each_distance] = tar_xgxx_statistics\n",
    "        return risk\n",
    "    \n",
    "    @__fault_tolerant\n",
    "    def get_feature_10(cls):\n",
    "        '''\n",
    "        法律诉讼风险：开庭公告、裁判文书、法院公告、民间借贷\n",
    "        '''\n",
    "        tar_xgxx = ['ktgg', 'zgcpwsw', 'rmfygg', 'lending']\n",
    "        risk = cls.filter_xgxx_type(tar_xgxx)\n",
    "        \n",
    "        risk['r'] = round(\n",
    "            np.dot(\n",
    "                map(sum, [\n",
    "                        risk[each_distance].values() \n",
    "                        for each_distance in xrange(0, 4)]), \n",
    "                [1., 1/2., 1/3., 1/4.]), 2)\n",
    "        \n",
    "        if risk['r'] == 0:\n",
    "            risk['z'] = 10\n",
    "        elif 0 < risk['r'] <= 10:\n",
    "            risk['z'] = 30\n",
    "        elif 10 < risk['r'] <= 30:\n",
    "            risk['z'] = 60\n",
    "        elif 30 < risk['r']:\n",
    "            risk['z'] = 100\n",
    "        \n",
    "        return risk\n",
    "            \n",
    "    @__fault_tolerant\n",
    "    def get_feature_11(cls):\n",
    "        '''\n",
    "        行政处罚风险\n",
    "        '''\n",
    "        tar_xgxx = ['xzcf']\n",
    "        risk = cls.filter_xgxx_type(tar_xgxx)\n",
    "        \n",
    "        risk['r'] = round(\n",
    "            np.dot(\n",
    "                map(sum, [risk[each_distance].values() \n",
    "                        for each_distance in xrange(0, 4)]), \n",
    "                [1., 1/2., 1/3., 1/4.]), 2)\n",
    "        \n",
    "        if risk['r'] == 0:\n",
    "            risk['z'] = 10\n",
    "        elif 0 < risk['r'] <= 10:\n",
    "            risk['z'] = 30\n",
    "        elif 10 < risk['r'] <= 30:\n",
    "            risk['z'] = 60\n",
    "        elif 30 < risk['r']:\n",
    "            risk['z'] = 100\n",
    "        \n",
    "        return risk\n",
    "    \n",
    "    @__fault_tolerant\n",
    "    def get_feature_12(cls):\n",
    "        '''\n",
    "        被执行风险：被执行、失信被执行\n",
    "        '''\n",
    "        tar_xgxx = ['zhixing', 'dishonesty']\n",
    "        risk = cls.filter_xgxx_type(tar_xgxx)        \n",
    "        \n",
    "        risk['r'] = round(\n",
    "            np.dot(\n",
    "                map(\n",
    "                    lambda x: x['zhixing'] + 2. * x['dishonesty'], \n",
    "                    [risk[each_distance] for each_distance in xrange(0, 4)]), \n",
    "                [1., 1/2., 1/3., 1/4.]), 2)\n",
    "        \n",
    "        if risk['r'] == 0:\n",
    "            risk['z'] = 10\n",
    "        elif 0 < risk['r'] <= 10:\n",
    "            risk['z'] = 30\n",
    "        elif 10 < risk['r'] <= 30:\n",
    "            risk['z'] = 60\n",
    "        elif 30 < risk['r']:\n",
    "            risk['z'] = 100        \n",
    "        \n",
    "        return risk\n",
    "        \n",
    "    @__fault_tolerant\n",
    "    def get_feature_13(cls):\n",
    "        '''\n",
    "        异常经营风险\n",
    "        '''\n",
    "        tar_xgxx = ['jyyc', 'estatus']\n",
    "        risk = cls.filter_xgxx_type(tar_xgxx)        \n",
    "        \n",
    "        risk['r'] = round(\n",
    "            np.dot(\n",
    "                map(\n",
    "                    lambda x: x['jyyc'] + 2. * x['estatus'], \n",
    "                    [risk[each_distance] for each_distance in xrange(0, 4)]), \n",
    "                [1., 1/2., 1/3., 1/4.]), 2)\n",
    "\n",
    "        if risk['r'] == 0:\n",
    "            risk['z'] = 10\n",
    "        elif 0 < risk['r'] <= 10:\n",
    "            risk['z'] = 30\n",
    "        elif 10 < risk['r'] <= 30:\n",
    "            risk['z'] = 60\n",
    "        elif 30 < risk['r']:\n",
    "            risk['z'] = 100\n",
    "        \n",
    "        return risk        \n",
    "        \n",
    "    @__fault_tolerant\n",
    "    def get_feature_14(cls):\n",
    "        '''\n",
    "        银监会行政处罚\n",
    "        '''\n",
    "        tar_xgxx = ['circxzcf']\n",
    "        risk = cls.filter_xgxx_type(tar_xgxx)        \n",
    "        \n",
    "        risk['r'] = round(\n",
    "            np.dot(\n",
    "                map(\n",
    "                    lambda x: x['circxzcf'], \n",
    "                    [risk[each_distance] for each_distance in xrange(0, 4)]), \n",
    "                [1., 1/2., 1/3., 1/4.]), 2)\n",
    "\n",
    "        if risk['r'] == 0:\n",
    "            risk['z'] = 10\n",
    "        elif 0 < risk['r'] <= 10:\n",
    "            risk['z'] = 30\n",
    "        elif 10 < risk['r'] <= 30:\n",
    "            risk['z'] = 60\n",
    "        elif 30 < risk['r']:\n",
    "            risk['z'] = 100        \n",
    "    \n",
    "        return risk            \n",
    "        \n",
    "    @__fault_tolerant\n",
    "    def get_feature_15(cls):\n",
    "        '''\n",
    "        实际控制人风险\n",
    "        '''\n",
    "        def get_degree_distribution(is_human):\n",
    "            some_person_sets = [\n",
    "                node\n",
    "                for node, attr in cls.DIG.nodes_iter(data=True) \n",
    "                if attr['is_human'] == is_human\n",
    "                and attr['distance'] <= 3]\n",
    "            person_out_degree = cls.DIG.out_degree(some_person_sets).values()\n",
    "            if not person_out_degree:\n",
    "                person_out_degree.append(0)\n",
    "            return person_out_degree\n",
    "        \n",
    "        nature_person_distribution = get_degree_distribution(1)\n",
    "        legal_person_distribution = get_degree_distribution(0)\n",
    "        \n",
    "        nature_max_control = max(nature_person_distribution)\n",
    "        legal_max_control = max(legal_person_distribution)        \n",
    "        nature_avg_control = round(np.average(nature_person_distribution), 2)\n",
    "        legal_avg_control = round(np.average(legal_person_distribution), 2)\n",
    "        \n",
    "        total_legal_num = len([\n",
    "                node \n",
    "                for node, attr in cls.DIG.nodes_iter(data=True) \n",
    "                if attr['is_human'] == 0])\n",
    "        \n",
    "        risk = round(((\n",
    "                    2*(nature_max_control + legal_max_control) + \n",
    "                    (nature_avg_control + legal_avg_control)) /\n",
    "                (2*total_legal_num + 0.001)), 2)\n",
    "        \n",
    "        if 0 <= risk <= 0.5:\n",
    "            r = 10\n",
    "        elif 0.5 < risk <= 1:\n",
    "            r = 30\n",
    "        elif 1 < risk <= 3:\n",
    "            r = 60  \n",
    "        elif 3 < risk:\n",
    "            r = 100\n",
    "        else:\n",
    "            r = 10\n",
    "        \n",
    "        return dict(\n",
    "            x_1=nature_max_control,\n",
    "            x_2=legal_max_control,\n",
    "            y_1=nature_avg_control,\n",
    "            y_2=legal_avg_control,\n",
    "            z=total_legal_num,\n",
    "            s=risk,\n",
    "            r=r\n",
    "        )\n",
    "    \n",
    "    @__fault_tolerant\n",
    "    def get_feature_16(cls):\n",
    "        '''\n",
    "        公司扩张路径风险\n",
    "        '''\n",
    "        def get_node_set(is_human):\n",
    "            return Counter([\n",
    "                    attr['distance']\n",
    "                    for node, attr in cls.DIG.nodes_iter(data=True)\n",
    "                    if attr['is_human'] == is_human])\n",
    "        \n",
    "        nature_person_distribution = get_node_set(1)\n",
    "        legal_person_distribution = get_node_set(0)\n",
    "        \n",
    "        nature_person_num = [\n",
    "            nature_person_distribution.get(each_distance, 0)\n",
    "            for each_distance in range(1, 4)]\n",
    "        legal_person_num = [\n",
    "            legal_person_distribution.get(each_distance, 0)\n",
    "            for each_distance in range(1, 4)]\n",
    "        \n",
    "        risk = round(np.sum(\n",
    "                np.divide(\n",
    "                    [\n",
    "                        np.divide(\n",
    "                            np.sum(nature_person_num[:each_distance]), \n",
    "                            np.sum(legal_person_num[:each_distance]), \n",
    "                            dtype=float)\n",
    "                        for each_distance in range(1, 4)], \n",
    "                    np.array([1, 2, 3], dtype=float))), 2)\n",
    "        \n",
    "        if 0 <= risk <= 1:\n",
    "            r = 10\n",
    "        elif 1 < risk <= 3:\n",
    "            r = 30\n",
    "        elif 3 < risk <= 5:\n",
    "            r = 60  \n",
    "        elif 5 < risk:\n",
    "            r = 100\n",
    "        else:\n",
    "            r = 10\n",
    "        \n",
    "        return dict(\n",
    "            x_1=legal_person_num[0],\n",
    "            x_2=legal_person_num[1],\n",
    "            x_3=legal_person_num[2],\n",
    "            y_1=nature_person_num[0],\n",
    "            y_2=nature_person_num[1],\n",
    "            y_3=nature_person_num[2],\n",
    "            z=risk if risk < 10000 else 0,\n",
    "            r=r\n",
    "        )\n",
    "\n",
    "\n",
    "    @__fault_tolerant\n",
    "    def get_feature_17(cls):\n",
    "        '''\n",
    "        关联方中心集聚风险\n",
    "        '''\n",
    "        one_relation_set = [\n",
    "            node for node, attr in cls.DIG.nodes_iter(data=True) \n",
    "            if attr['distance'] == 1]\n",
    "        legal_person_shareholder = len([\n",
    "                src_node \n",
    "                for src_node, des_node, edge_attr \n",
    "                in cls.DIG.edges_iter(one_relation_set, data=True) \n",
    "                if des_node == cls.tarcompany \n",
    "                and edge_attr['is_invest'] == 'INVEST'\n",
    "                and cls.DIG.node[src_node]['is_human'] == 0])\n",
    "        legal_person_subsidiary = len([\n",
    "                des_node \n",
    "                for src_node, des_node, edge_attr \n",
    "                in cls.DIG.edges_iter(one_relation_set, data=True) \n",
    "                if src_node == cls.tarcompany \n",
    "                and edge_attr['is_invest'] == 'INVEST'\n",
    "                and cls.DIG.node[des_node]['is_human'] == 0])\n",
    "        \n",
    "        risk =(\n",
    "            legal_person_subsidiary -\n",
    "            legal_person_shareholder ) \n",
    "        \n",
    "        if risk < 0:\n",
    "            r = 10\n",
    "        elif risk == 0:\n",
    "            r = 30\n",
    "        elif 0 < risk <= 3:\n",
    "            r = 60\n",
    "        elif 3 < risk:\n",
    "            r = 100\n",
    "        else:\n",
    "            r = 100        \n",
    "        \n",
    "        return dict(\n",
    "            x=legal_person_subsidiary,\n",
    "            y=legal_person_shareholder,\n",
    "            z=risk,\n",
    "            r=r\n",
    "        )\n",
    "\n",
    "    @__fault_tolerant\n",
    "    def get_feature_18(cls):\n",
    "        '''\n",
    "        分支机构过度扩张风险\n",
    "        '''\n",
    "        risk = cls.resultiterable.data[0].a_fzjg\n",
    "        risk = risk if risk is not None else 0\n",
    "\n",
    "        if  risk == 0:\n",
    "            r = 10\n",
    "        elif risk == 1:\n",
    "            r = 30\n",
    "        elif 1 < risk <= 3:\n",
    "            r = 60  \n",
    "        elif 3 < risk:\n",
    "            r = 100\n",
    "        else:\n",
    "            r = 10\n",
    "        \n",
    "        return dict(\n",
    "            d=risk if risk < 100 else 100, \n",
    "            z=r\n",
    "        )\n",
    "    \n",
    "    @__fault_tolerant\n",
    "    def get_feature_19(cls):\n",
    "        '''\n",
    "        关联方结构稳定风险\n",
    "        '''\n",
    "        def get_relation_num():\n",
    "            return Counter([\n",
    "                    attr['distance']\n",
    "                    for node, attr in cls.DIG.nodes(data=True)])\n",
    "        \n",
    "        #目标企业各个度的节点的数量\n",
    "        relations_num = get_relation_num()\n",
    "        relation_three_num = relations_num.get(3, 0)\n",
    "        relation_two_num = relations_num.get(2, 0)\n",
    "        relation_one_num = relations_num.get(1, 0)\n",
    "        relation_zero_num = relations_num.get(0, 1)\n",
    "        \n",
    "        x = np.array([\n",
    "                relation_zero_num, \n",
    "                relation_one_num, \n",
    "                relation_two_num, \n",
    "                relation_three_num]).astype(float)\n",
    "        \n",
    "        y_2 = x[2] / (x[1]+x[2])\n",
    "        y_3 = x[3] / (x[1]+x[2]+x[3])\n",
    "        risk = y_2/2 + y_3/3\n",
    "        \n",
    "\n",
    "        \n",
    "        if 0 <= risk <= 0.1:\n",
    "            r = 10\n",
    "        elif 0.1 < risk <= 0.3:\n",
    "            r = 30\n",
    "        elif 0.3 < risk <= 0.5:\n",
    "            r = 60  \n",
    "        elif 0.5 < risk:\n",
    "            r = 100      \n",
    "        else:\n",
    "            r = 10\n",
    "        \n",
    "        return dict(\n",
    "            x_1=x[1],\n",
    "            x_2=x[2],\n",
    "            x_3=x[3],\n",
    "            y_2=y_2,\n",
    "            y_3=y_3,\n",
    "            z=risk if risk else 0,\n",
    "            r=r\n",
    "        )        \n",
    "    \n",
    "    @__fault_tolerant\n",
    "    def get_feature_20(cls):\n",
    "        '''\n",
    "        潜在违规融资风险\n",
    "        '''\n",
    "\n",
    "        \n",
    "        def get_relation_risk_num(keyword_type):\n",
    "            return Counter([attr['distance']\n",
    "                    for node, attr in cls.DIG.nodes_iter(data=True) \n",
    "                    if attr['is_human'] == 0 and \n",
    "                    attr['opescope'] == keyword_type])\n",
    "        \n",
    "        k_2_num = get_relation_risk_num('k_2')\n",
    "        k_1_num = get_relation_risk_num('k_1')\n",
    "        \n",
    "        x = np.array([\n",
    "                k_2_num.get(each_distance, 0) \n",
    "                for each_distance in xrange(1, 4)])\n",
    "        y = np.array([\n",
    "                k_1_num.get(each_distance, 0) \n",
    "                for each_distance in xrange(1, 4)])\n",
    "    \n",
    "        risk = round(\n",
    "            np.sum(\n",
    "                np.divide(np.add(x, y), [1, 2, 3], dtype=float)), 2)\n",
    "    \n",
    "        if  risk == 0:\n",
    "            r = 10\n",
    "        elif 0 < risk < 5:\n",
    "            r = 30\n",
    "        elif 5 <= risk < 10:\n",
    "            r = 60  \n",
    "        elif 10 <= risk:\n",
    "            r = 100     \n",
    "        else:\n",
    "            r = 10\n",
    "    \n",
    "        return dict(\n",
    "            x_1=x[0],\n",
    "            x_2=x[1],\n",
    "            x_3=x[2],\n",
    "            y_1=y[0],\n",
    "            y_2=y[1],\n",
    "            y_3=y[2],\n",
    "            g=r\n",
    "        )\n",
    "    \n",
    "    @__fault_tolerant\n",
    "    def get_feature_21(cls):\n",
    "        '''\n",
    "        关联方地址集中度风险\n",
    "        '''\n",
    "        legal_person_address = [\n",
    "            attr['address'] \n",
    "            for node, attr in cls.DIG.nodes_iter(data=True) \n",
    "            if attr['distance'] <= 3 \n",
    "            and attr['is_human'] == 0]\n",
    "        c = Counter(\n",
    "            filter(\n",
    "                lambda x: x is not None and len(x) >= 21,  \n",
    "                legal_person_address))\n",
    "        n = c.most_common(1)\n",
    "        n= n[0][1] if len(n) > 0 else 1\n",
    "        risk = n -1\n",
    "        \n",
    "        if  risk == 0:\n",
    "            z = 10\n",
    "        elif risk == 1:\n",
    "            z = 30\n",
    "        elif risk == 2:\n",
    "            z = 60  \n",
    "        elif 2 < risk:\n",
    "            z = 100             \n",
    "        \n",
    "        return dict(\n",
    "            n=risk,\n",
    "            y=risk,\n",
    "            z=z\n",
    "        )\n",
    "    \n",
    "    @__fault_tolerant\n",
    "    def get_feature_22(cls):\n",
    "        '''\n",
    "        利益一致行动法人风险\n",
    "        '''\n",
    "        def is_similarity(str_1, str_2):\n",
    "            '''\n",
    "            判断两个字符串是否有连续2个字相同\n",
    "            '''\n",
    "            try:\n",
    "                token_1 = [\n",
    "                    str_1[index] + str_1[index+1] \n",
    "                    for index,data in enumerate(str_1) \n",
    "                    if index < len(str_1) - 1]\n",
    "                is_similarity = sum([\n",
    "                        1 for each_token in token_1 \n",
    "                        if each_token in str_2])\n",
    "                return True if is_similarity > 0 else False        \n",
    "            except:\n",
    "                return False\n",
    "            \n",
    "        #目标公司的名称frag\n",
    "        tar_company = cls.resultiterable.data[0].a_name\n",
    "        tar_company_frag = cls.resultiterable.data[0].a_namefrag\n",
    "        #三度以内，所有关联方的节点集合(不包含自身)\n",
    "        relation_set = [\n",
    "                attr['name'] \n",
    "                for node, attr in cls.DIG.nodes_iter(data=True) \n",
    "                if attr['distance'] <= 3]\n",
    "        #relation_set.remove(tar_company)\n",
    "        common_interests_list = [\n",
    "            1 \n",
    "            for node_name in relation_set \n",
    "            if is_similarity(tar_company_frag, node_name)]\n",
    "        \n",
    "        risk = sum(common_interests_list)\n",
    "        if risk == 0:\n",
    "            z = 10\n",
    "        elif risk == 1:\n",
    "            z = 30\n",
    "        elif 1 < risk <= 3:\n",
    "            z = 60\n",
    "        elif 3 < risk:\n",
    "            z = 100\n",
    "        else:\n",
    "            z = 10\n",
    "        \n",
    "        return dict(\n",
    "            d=risk, \n",
    "            z=z\n",
    "        )        \n",
    "        \n",
    "    @__fault_tolerant\n",
    "    def get_feature_23(cls):\n",
    "        '''\n",
    "        关联方信誉风险\n",
    "        '''\n",
    "        def get_black_num(distance):\n",
    "            '''\n",
    "            某一度黑企业的总数\n",
    "            '''\n",
    "            return sum([\n",
    "                    1\n",
    "                    for node, attr in cls.DIG.nodes_iter(data=True)\n",
    "                    if (attr['is_black'] and attr['distance'] == distance \n",
    "                            and attr['is_human'] == 0)\n",
    "            ])\n",
    "        \n",
    "        black_relation_set =[\n",
    "            get_black_num(each_distance) \n",
    "            for each_distance in range(1, 4)]\n",
    "        risk = np.dot(black_relation_set, [1, 1/2., 1/3.])\n",
    "        if risk == 0:\n",
    "            z = 10\n",
    "        elif 0 < risk < 0.5:\n",
    "            z = 30\n",
    "        elif 0.5 <= risk < 1:\n",
    "            z = 60\n",
    "        elif 1 <= risk:\n",
    "            z =100\n",
    "        else:\n",
    "            z = 10\n",
    "        \n",
    "        return dict(\n",
    "            b_1=black_relation_set[0],\n",
    "            b_2=black_relation_set[1],\n",
    "            b_3=black_relation_set[2],\n",
    "            y=risk,\n",
    "            z=z\n",
    "        )\n",
    "    \n",
    "    @__fault_tolerant\n",
    "    def get_feature_24(cls):\n",
    "        '''\n",
    "        短期逐利风险\n",
    "        '''\n",
    "        \n",
    "        def get_max_established(distance, timedelta):\n",
    "            '''\n",
    "            获取在某distance关联方企业中，某企业在任意timedelta内投资成立公司数量的最大值\n",
    "            '''\n",
    "            #用于获取最大连续时间数，这里i的取值要仔细琢磨一下，这里输入一个时间差序列与时间差\n",
    "            def get_date_density(difference_list, timedelta):\n",
    "                time_density_list = []\n",
    "                for index, date in enumerate(difference_list):\n",
    "                    if date < 15:\n",
    "                        s = 0\n",
    "                        i = 1\n",
    "                        while(s < 15 and i <= len(difference_list) - index):\n",
    "                            i += 1\n",
    "                            s = sum(difference_list[index:index+i])\n",
    "                        time_density_list.append(i)\n",
    "                    else:\n",
    "                        continue\n",
    "                return max(time_density_list) if len(time_density_list) >0 else 0            \n",
    "\n",
    "            #distance所有节点集合\n",
    "            relation_set = [\n",
    "                node \n",
    "                for node, attr in cls.DIG.nodes(data=True) \n",
    "                if attr['distance'] == distance]\n",
    "            investment_dict = defaultdict(lambda : [])     \n",
    "            \n",
    "            if len(cls.DIG.edge) > 1:\n",
    "                for src_node, des_node, edge_attr in (\n",
    "                        cls.DIG.edges_iter(relation_set, data=True)):\n",
    "                    if (edge_attr.get('is_invest', 0) == 'INVEST' \n",
    "                            and cls.DIG.node[des_node]['distance'] == distance\n",
    "                            and cls.DIG.node[des_node]['esdate'] is not None \n",
    "                            and cls.DIG.node[src_node]['is_human'] == 0):\n",
    "                        #将所有节点投资的企业的成立时间加进列表中\n",
    "                        investment_dict[src_node].append(cls.DIG.node[des_node]['esdate'])\n",
    "             \n",
    "            #目标企业所有节点所投资的企业时间密度字典\n",
    "            all_date_density_dict = {}\n",
    "            \n",
    "            for node, date_list in investment_dict.iteritems():\n",
    "                #构建按照时间先后排序的序列\n",
    "                date_strp_list = sorted(date_list)\n",
    "                if len(date_strp_list) > 1:\n",
    "                    #构建时间差的序列，例如：[256, 4, 5, 1, 2, 33, 6, 5, 4, 73]\n",
    "                    date_difference_list = [\n",
    "                        (date_strp_list[index + 1] - date_strp_list[index]).days \n",
    "                        for index in range(len(date_strp_list) -1)]\n",
    "                    #计算某法人节点在timedelta天之内有多少家公司成立\n",
    "                    es_num = get_date_density(date_difference_list, timedelta)\n",
    "                    if all_date_density_dict.has_key(es_num):\n",
    "                        all_date_density_dict[es_num].append(node)\n",
    "                    else:\n",
    "                        all_date_density_dict[es_num] = [node]              \n",
    "                else:\n",
    "                    continue\n",
    "            keys = all_date_density_dict.keys()        \n",
    "            max_num = max(keys) if len(keys) > 0 else 0\n",
    "            \n",
    "            return max_num\n",
    "        \n",
    "        x = [\n",
    "            get_max_established(each_distance, 180) \n",
    "            for each_distance in xrange(1, 4)]\n",
    "        \n",
    "        risk = round(\n",
    "            np.sum(\n",
    "                np.divide(x, [1, 2, 3], dtype=float)), 2)\n",
    "        \n",
    "        if  risk == 0:\n",
    "            r = 10\n",
    "        elif 0 < risk <= 0.7:\n",
    "            r = 30\n",
    "        elif 0.7 < risk <= 1:\n",
    "            r = 60  \n",
    "        elif 1 < risk:\n",
    "            r = 100       \n",
    "        else:\n",
    "            r = 10\n",
    "        \n",
    "        return dict(\n",
    "            x_1=x[0],\n",
    "            x_2=x[1],\n",
    "            x_3=x[2],\n",
    "            z=r,\n",
    "            d=risk\n",
    "        )\n",
    "    \n",
    "\n",
    "    @classmethod\n",
    "    def get_some_feature(cls, resultiterable, feature_nums):\n",
    "        #创建类变量\n",
    "        cls.resultiterable = resultiterable\n",
    "        cls.tarcompany = cls.resultiterable.data[0].a\n",
    "        cls.DIG = cls.create_graph_udf(cls.resultiterable, 1)\n",
    "        cls.xgxx_distribution = cls.get_feature_xgxx()\n",
    "        \n",
    "        feature_list = [\n",
    "            ('feature_{0}'.format(feature_index), \n",
    "                     eval('cls.get_feature_{0}()'.format(feature_index)))\n",
    "            for feature_index in feature_nums]\n",
    "        feature_list.append(('bbd_qyxx_id', cls.tarcompany))\n",
    "        feature_list.append(('company_name', cls.resultiterable.data[0].a_name))\n",
    "        \n",
    "        return json.dumps(dict(feature_list), ensure_ascii=False)      #\n",
    "\n",
    "    \n",
    "def spark_data_flow(tidversion):\n",
    "    '''\n",
    "    dataflow\n",
    "    '''\n",
    "    conf = SparkConf()\n",
    "    conf.setMaster('yarn-client')\n",
    "    conf.set(\"spark.yarn.am.cores\", 7)\n",
    "    conf.set(\"spark.executor.memory\", \"50g\")\n",
    "    conf.set(\"spark.executor.instances\", 20)\n",
    "    conf.set(\"spark.executor.cores\", 10)\n",
    "    conf.set(\"spark.python.worker.memory\", \"2g\")\n",
    "    conf.set(\"spark.default.parallelism\", 1000)\n",
    "    conf.set(\"spark.sql.shuffle.partitions\", 1000)\n",
    "    conf.set(\"spark.broadcast.blockSize\", 1024)\n",
    "    conf.set(\"spark.executor.extraJavaOptions\",\"-XX:+PrintGCDetails -XX:+PrintGCTimeStamps\")    \n",
    "    conf.set(\"spark.shuffle.file.buffer\", '512k')\n",
    "    \n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"glf\") \\\n",
    "        .config(conf = conf) \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()  \n",
    "    \n",
    "    #输入\n",
    "    #算法设计直接用rdd\n",
    "    tid_df = spark.read.parquet(\"/user/antifraud/jinli/tiddata/relation_company_info_merge/{version}\".format(version=tidversion))\n",
    "    tid_rdd = tid_df.rdd\n",
    "        \n",
    "    #最终计算流程\n",
    "    tid_rdd_2 = tid_rdd.map(lambda row: (row.a_name, row)) \\\n",
    "        .groupByKey() \\\n",
    "        .repartition(1000) \\\n",
    "        .filter(lambda r: len(r[1].data) <= 100000) \\\n",
    "        .cache()\n",
    "\n",
    "    import time\n",
    "    import signal\n",
    "    def handler(signum, frame):\n",
    "        raise AssertionError\n",
    "\n",
    "    def fault_tolerant(func):\n",
    "        def wappen(*args):\n",
    "            try:\n",
    "                return func(*args)\n",
    "            except:\n",
    "                return 'acer'\n",
    "        return wappen\n",
    "            \n",
    "    @fault_tolerant\n",
    "    def time_out(data):\n",
    "        signal.signal(signal.SIGALRM, handler)\n",
    "        signal.alarm(30)\n",
    "        return FeatureConstruction.get_some_feature(data, [_ for _ in range(1, 25)])\n",
    "\n",
    "    def calculate(data):\n",
    "        return FeatureConstruction.get_some_feature(data, [_ for _ in range(1, 25)])\n",
    "\n",
    "    feature_list = tid_rdd_2.mapValues(\n",
    "        time_out\n",
    "    ).map(\n",
    "        itemgetter(1)\n",
    "    )   \n",
    "    \n",
    "    return feature_list\n",
    "\n",
    "def collect_to_driver(version, out_path, tidversion):\n",
    "    '''\n",
    "    格式化输出\n",
    "    '''\n",
    "    src_time = time.time()\n",
    "    pd_df = spark_data_flow(tidversion)\n",
    "    os.system(\"hadoop fs -rmr {path}/{version}/prd_jinli_feature_score_distribution_{version}\".format(path=OUTPATH, version=version))\n",
    "    pd_df.saveAsTextFile(\"{path}/{version}/prd_jinli_feature_score_distribution_{version}\".format(path=OUTPATH, version=version))\n",
    "    des_time = time.time()    \n",
    "    \n",
    "    print 'SUCCESS !! \\n 耗时：{0}'.format(des_time - src_time)\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':  \n",
    "    #输入参数\n",
    "    VERSION = \"20170417\"\n",
    "    OUTPATH = \"/user/antifraud/jinli/tmpdata\"    \n",
    "\n",
    "    collect_to_driver(\n",
    "        version=VERSION,\n",
    "        out_path=OUTPATH,\n",
    "        tidversion='20170403'\n",
    "    )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /data5/antifraud/source/tmp_20170413/hongjing_jinli_dynamic_kpi.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /data5/antifraud/source/tmp_20170413/hongjing_jinli_dynamic_kpi.py\n",
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "动态风险：需要2个时间节点的中间数据\n",
    "'''\n",
    "import datetime\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "from operator import itemgetter\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as fun\n",
    "from pyspark.sql import types as tp\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pandas.io.json import json_normalize\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class FeatureConstruction(object):\n",
    "    '''\n",
    "    计算特征的函数集\n",
    "    '''      \n",
    "\n",
    "    def __fault_tolerant(func):\n",
    "        '''\n",
    "        一个用于容错的装饰器\n",
    "        '''\n",
    "        @classmethod\n",
    "        def wappen(cls, *args, **kwargs):\n",
    "            try:\n",
    "                return func(cls, *args, **kwargs)\n",
    "            except Exception, e:\n",
    "                return (\n",
    "                    \"{func_name} has a errr : {excp}\"\n",
    "                ).format(func_name=func.__name__, excp=e)\n",
    "        return wappen    \n",
    "    \n",
    "    @__fault_tolerant    \n",
    "    def create_graph_udf(cls, relations, is_directed):\n",
    "        '''\n",
    "        根据关联方的结构创建有向、无向图\n",
    "        '''\n",
    "        def init_graph(edge_list, node_list, is_directed=0):\n",
    "            #网络初始化\n",
    "            G = nx.DiGraph() if is_directed == 1 else nx.Graph()    \n",
    "            #增加带属性的节点\n",
    "            for node in node_list:\n",
    "                G.add_node(node[0], attr_dict=node[1])\n",
    "            #增加带属性的边\n",
    "            G.add_edges_from(edge_list)\n",
    "            return G\n",
    "            \n",
    "        #生成一个图\n",
    "        company_correlative_edges = [\n",
    "            (row.b, row.c, {'is_invest': row.bc_relation}) for row in relations]\n",
    "                                     \n",
    "        company_correlative_nodes = [(\n",
    "                row.b, \n",
    "                dict(\n",
    "                    is_human=row.b_isperson,\n",
    "                    is_black=row.b_is_black_company,\n",
    "                    distance = row.b_degree,\n",
    "                    name = row.b_name,\n",
    "                    isSOcompany = row.b_isSOcompany,\n",
    "                    esdate = row.b_regtime,\n",
    "                    ktgg = row.b_ktgg,\n",
    "                    zgcpwsw = row.b_zgcpwsw,\n",
    "                    rmfygg = row.b_rmfygg,\n",
    "                    lending = row.b_lending,\n",
    "                    xzcf = row.b_xzcf,\n",
    "                    zhixing = row.b_zhixing,\n",
    "                    dishonesty = row.b_dishonesty,\n",
    "                    jyyc = row.b_jyyc,\n",
    "                    circxzcf = row.b_circxzcf,\n",
    "                    opescope = row.b_opescope,\n",
    "                    address = row.b_address,\n",
    "                    estatus = row.b_estatus,\n",
    "                    province = row.b_province\n",
    "                ))  for row in relations] + [(\n",
    "                row.c, \n",
    "                dict(\n",
    "                    is_human=row.c_isperson,\n",
    "                    is_black=row.c_is_black_company,\n",
    "                    distance = row.c_degree,\n",
    "                    name = row.c_name,\n",
    "                    isSOcompany = row.c_isSOcompany,\n",
    "                    esdate = row.c_regtime,\n",
    "                    ktgg = row.c_ktgg,\n",
    "                    zgcpwsw = row.c_zgcpwsw,\n",
    "                    rmfygg = row.c_rmfygg,\n",
    "                    lending = row.c_lending,\n",
    "                    xzcf = row.c_xzcf,\n",
    "                    zhixing = row.c_zhixing,\n",
    "                    dishonesty = row.c_dishonesty,\n",
    "                    jyyc = row.c_jyyc,\n",
    "                    circxzcf = row.c_circxzcf,\n",
    "                    opescope = row.c_opescope,\n",
    "                    address = row.c_address,\n",
    "                    estatus = row.c_estatus,\n",
    "                    province = row.c_province\n",
    "                )) for row in relations]\n",
    "        \n",
    "        if is_directed == 1:\n",
    "            g = init_graph(\n",
    "                company_correlative_edges,\n",
    "                company_correlative_nodes, is_directed = 1) \n",
    "        else:\n",
    "            g = init_graph(\n",
    "                company_correlative_edges, \n",
    "                company_correlative_nodes, is_directed = 0)\n",
    "        return g\n",
    "\n",
    "    @__fault_tolerant\n",
    "    def get_feature_1(cls):\n",
    "        '''\n",
    "        企业背景风险\n",
    "        '''\n",
    "        one_relation_set = [\n",
    "            node for node, attr in cls.DIG.nodes_iter(data=True) if attr['distance'] == 1]\n",
    "        shareholder = [\n",
    "            src_node for src_node, des_node, edge_attr \n",
    "            in cls.DIG.edges_iter(one_relation_set, data=True) \n",
    "            if des_node == cls.tarcompany \n",
    "            and edge_attr['is_invest'] == 'INVEST']\n",
    "        os_shareholder = [\n",
    "            node for node in shareholder \n",
    "            if cls.DIG.node[node]['isSOcompany'] is True \n",
    "            and cls.DIG.node[node]['is_human'] == 0]\n",
    "        anti_os_shareholder = [\n",
    "            node for node in shareholder \n",
    "            if cls.DIG.node[node]['isSOcompany'] is False \n",
    "            and cls.DIG.node[node]['is_human'] == 0]\n",
    "        nature_shareholder = [\n",
    "            node for node in shareholder \n",
    "            if cls.DIG.node[node]['is_human'] == 1]\n",
    "        is_ipo_tarcompany = 1 if cls.resultiterable.data[0].a_isIPOcompany is True else 0\n",
    "    \n",
    "        x = len(os_shareholder)\n",
    "        y = len(anti_os_shareholder)\n",
    "        z = len(nature_shareholder)\n",
    "        w = len(shareholder)\n",
    "        r_i = is_ipo_tarcompany\n",
    "        r = (0.2*x + 0.5*y + z) * 100. * (2 - r_i) / (2*w + 0.001) \n",
    "        \n",
    "        return dict(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            z=z,\n",
    "            w=w,\n",
    "            r_i=r_i,\n",
    "            r=round(r, 2) if r else 100\n",
    "        )\n",
    "\n",
    "\n",
    "    @__fault_tolerant\n",
    "    def get_feature_2(cls):\n",
    "        '''\n",
    "        资本风险\n",
    "        '''\n",
    "        x = cls.resultiterable.data[0].a_regcap \n",
    "        y = cls.resultiterable.data[0].a_realcap\n",
    "        \n",
    "        x = x / 10000 if x != 'NULL' and x is not None else 0\n",
    "        y = y / 10000 if y != 'NULL' and y is not None else 0\n",
    "        \n",
    "        if 0 <= x < 500:\n",
    "            c_i = 100\n",
    "        elif 500 <= x < 1000:\n",
    "            c_i = 60\n",
    "        elif 1000 <= x < 5000:\n",
    "            c_i = 30\n",
    "        elif 5000 <= x:\n",
    "            c_i = 10\n",
    "        \n",
    "        p_i = 1 if y / (x + 0.001) >= 0.5 else 0\n",
    "        \n",
    "        return  dict(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            c=c_i * (1 - p_i/2.)\n",
    "        )\n",
    "    \n",
    "    @__fault_tolerant\n",
    "    def get_feature_3(cls):\n",
    "        '''\n",
    "        公司地域风险\n",
    "        '''\n",
    "        province_black_num = cls.resultiterable.data[0].a_province_black_num\n",
    "        province_leijinrong_num = cls.resultiterable.data[0].a_province_leijinrong_num\n",
    "        province_black_num = province_black_num if province_black_num else 0\n",
    "        province_leijinrong_num = province_leijinrong_num if province_leijinrong_num else 0\n",
    "        \n",
    "        return dict(\n",
    "            j_1=province_black_num,\n",
    "            j_2=province_leijinrong_num,\n",
    "            j=round(province_black_num / (province_leijinrong_num + 0.0001), 4)\n",
    "        )\n",
    "    \n",
    "    @__fault_tolerant\n",
    "    def get_feature_4(cls):\n",
    "        '''\n",
    "        知识产权风险（专利和商标）\n",
    "        '''\n",
    "        k_1 = cls.resultiterable.data[0].a_zhuanli\n",
    "        k_2 = cls.resultiterable.data[0].a_shangbiao \n",
    "        k_1 = k_1 if k_1 != 'NULL' and k_1 is not None else 0\n",
    "        k_2 = k_2 if k_1 != 'NULL' and k_2 is not None else 0\n",
    "        k_3 = k_1 + k_2\n",
    "        \n",
    "        if k_3 == 0:\n",
    "            k = 100\n",
    "        elif 1 <= k_3 < 5:\n",
    "            k = 60\n",
    "        elif 5 <= k_3 < 10:\n",
    "            k = 30\n",
    "        elif 10 <= k_3:\n",
    "            k = 10\n",
    "        \n",
    "        return dict(\n",
    "            k_1=k_1,\n",
    "            k_2=k_2,\n",
    "            k=k\n",
    "        )\n",
    "        \n",
    "    @__fault_tolerant\n",
    "    def get_feature_5(cls):\n",
    "        '''\n",
    "        域名备案风险\n",
    "        '''\n",
    "        icp = cls.resultiterable.data[0].a_ICP\n",
    "        url = cls.resultiterable.data[0].a_url\n",
    "        \n",
    "        c_i = 1 if icp != 'NULL' and icp is not None else 0   \n",
    "        try:\n",
    "            if url is not None and url != 'NULL':\n",
    "                status_code = requests.get(url, allow_redirects=False).status_code \n",
    "                p_i = 1 if status_code == 200 else 0\n",
    "            else:\n",
    "                p_i = 0\n",
    "        except:\n",
    "            p_i = 0\n",
    "        \n",
    "        l = (1 - c_i/2.) * (1 - p_i/2.) * 100\n",
    "        \n",
    "        return dict(\n",
    "            c_i=c_i,\n",
    "            p_i=p_i,\n",
    "            l=l\n",
    "        )\n",
    "        \n",
    "    @__fault_tolerant\n",
    "    def get_feature_6(cls):\n",
    "        '''\n",
    "        工商变更风险\n",
    "        '''\n",
    "        def get_bgxx_symbol(bgxx_name):\n",
    "            if u'法定代表人' in bgxx_name:\n",
    "                return 'c_1'\n",
    "            elif u'股东' in bgxx_name:\n",
    "                return 'c_2'\n",
    "            elif u'注册资本' in bgxx_name:\n",
    "                return 'c_3'\n",
    "            elif u'高管' in bgxx_name:\n",
    "                return 'c_4'\n",
    "            elif u'经营范围' in bgxx_name:\n",
    "                return 'c_5'\n",
    "            else:\n",
    "                return 'c_6'\n",
    "        \n",
    "        default_result = OrderedDict([\n",
    "                ('c_1', 0),\n",
    "                ('c_2', 0),\n",
    "                ('c_3', 0),\n",
    "                ('c_4', 0),\n",
    "                ('c_5', 0),\n",
    "                ('c_6', 0),\n",
    "                ('r', 0),\n",
    "                ('z', 10)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        if cls.resultiterable.data[0].a_bgxx is not None:\n",
    "            for bgxx_name, bgxx_num in cls.resultiterable.data[0].a_bgxx.iteritems():\n",
    "                default_result[get_bgxx_symbol(bgxx_name)] += int(bgxx_num)\n",
    "            default_result['r'] = np.dot(default_result.values(), [2, 1, 1, 1, 2, 0, 0, 0])\n",
    "        \n",
    "            if default_result['r'] == 0:\n",
    "                default_result['z'] = 10\n",
    "            elif 1 <= default_result['r'] < 5:\n",
    "                default_result['z'] = 30\n",
    "            elif 5 <= default_result['r'] < 10:\n",
    "                default_result['z'] = 60\n",
    "            elif 10 <= default_result['r']:\n",
    "                default_result['z'] = 100\n",
    "                \n",
    "        default_result.pop('c_6')\n",
    "        \n",
    "        return dict(default_result)\n",
    "        \n",
    "    @__fault_tolerant\n",
    "    def get_feature_7(cls):\n",
    "        '''\n",
    "        人才结构风险\n",
    "        '''        \n",
    "        def get_recruit_num(each_recruit):\n",
    "            if u'本科' in each_recruit:\n",
    "                return 'e_2'\n",
    "            elif u'硕士' in each_recruit:\n",
    "                return 'e_3'\n",
    "            elif u'博士' in each_recruit:\n",
    "                return 'e_3'\n",
    "            else:\n",
    "                return 'e_1'\n",
    "            \n",
    "        default_result = OrderedDict([\n",
    "                ('e_1', 0),\n",
    "                ('e_2', 0),\n",
    "                ('e_3', 0),\n",
    "                ('e', 0),\n",
    "                ('r', 0),\n",
    "                ('z', 10)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        if cls.resultiterable.data[0].a_recruit is not None:\n",
    "            for education_name, education_num in cls.resultiterable.data[0].a_recruit.iteritems():\n",
    "                default_result[get_recruit_num(education_name)] += int(education_num)\n",
    "            default_result['e'] = sum(\n",
    "                map(int ,cls.resultiterable.data[0].a_recruit.values()))\n",
    "            default_result['r'] = round(\n",
    "                np.dot(default_result.values(), [100., 60., 30., 0, 0, 0]) / default_result['e'], 2)\n",
    "            \n",
    "            if default_result['r'] == 0:\n",
    "                default_result['z'] = 10\n",
    "            elif 0 < default_result['r'] < 60:\n",
    "                default_result['z'] = 30\n",
    "            elif 60 <= default_result['r'] < 80:\n",
    "                default_result['z'] = 60\n",
    "            elif 80 <= default_result['r']:\n",
    "                default_result['z'] = 100         \n",
    "            \n",
    "        return dict(default_result)\n",
    "        \n",
    "    @__fault_tolerant\n",
    "    def get_feature_8(cls):\n",
    "        '''\n",
    "        公司运营持续风险\n",
    "        '''\n",
    "        esdate_relation_set = [\n",
    "            (datetime.date.today() - attr['esdate']).days \n",
    "            for node, attr in cls.DIG.nodes_iter(data=True) \n",
    "            if attr['distance'] <= 3 \n",
    "            and attr['is_human'] == 0 \n",
    "            and attr['esdate'] is not None]\n",
    "        t = round(np.average(esdate_relation_set), 2)\n",
    "        \n",
    "        if 0 <= t < 365:\n",
    "            y = 100\n",
    "        elif 365 <= t < 1825:\n",
    "            y = 60\n",
    "        elif 1825 <= t < 3650:\n",
    "            y = 30\n",
    "        elif 3650 <= t:\n",
    "            y = 10\n",
    "        else:\n",
    "            y = 100\n",
    "        return dict(\n",
    "            t=t if t else 0,\n",
    "            y=y\n",
    "        )\n",
    "\n",
    "        \n",
    "    @__fault_tolerant\n",
    "    def get_feature_9(cls):\n",
    "        '''\n",
    "        招标和中标风险\n",
    "        '''\n",
    "        n_1 = cls.resultiterable.data[0].a_zhaobiao\n",
    "        n_2 = cls.resultiterable.data[0].a_zhongbiao\n",
    "        n_1 = n_1 if n_1 is not None else 0\n",
    "        n_2 = n_2 if n_2 is not None else 0\n",
    "        \n",
    "        n = n_1 + n_2\n",
    "        \n",
    "        if n == 0:\n",
    "            n = 100\n",
    "        elif 1 <= n < 5:\n",
    "            n = 60\n",
    "        elif 5<= n < 10:\n",
    "            n = 30\n",
    "        else:\n",
    "            n = 10\n",
    "        \n",
    "        return  dict(\n",
    "            n_1=n_1,\n",
    "            n_2=n_2,\n",
    "            n=n\n",
    "        )\n",
    "        \n",
    "    @__fault_tolerant\n",
    "    def get_feature_xgxx(cls):\n",
    "        '''\n",
    "        【计算基础数据】\n",
    "        法律诉讼风险：开庭公告、裁判文书、法院公告、民间借贷\n",
    "        行政处罚\n",
    "        被执行风险\n",
    "        异常经营风险：经营异常、吊销&注销\n",
    "        银监会行政处罚\n",
    "        ''' \n",
    "        def get_certain_distance_all_info(distance, document_types):\n",
    "            all_array = []\n",
    "            #处理某一个distance不存在节点的情况\n",
    "            all_array.append([0]*len(document_types))\n",
    "            for node, attr in cls.DIG.nodes_iter(data=True):\n",
    "                if attr['is_human'] == 0  and attr['distance'] == distance:\n",
    "                    each_array = map(lambda x: x if x else 0, \n",
    "                                                  [attr[each_document] \n",
    "                                                      for each_document in document_types])\n",
    "                    all_array.append(each_array)\n",
    "                else:\n",
    "                    continue\n",
    "            documents_num = np.sum(all_array, axis=0)\n",
    "            return documents_num\n",
    "        \n",
    "        matrx = dict()\n",
    "        xgxx_type = ['ktgg', 'zgcpwsw', 'rmfygg', \n",
    "                             'lending', 'xzcf', 'zhixing', \n",
    "                             'dishonesty', 'jyyc', 'circxzcf', \n",
    "                             'estatus']\n",
    "\n",
    "        for each_distance in xrange(0, 4):\n",
    "            xgxx_num_list = get_certain_distance_all_info(each_distance, \n",
    "                                                                                      xgxx_type)\n",
    "            matrx[each_distance] = dict(zip(xgxx_type, xgxx_num_list))\n",
    "            \n",
    "        return matrx\n",
    "    \n",
    "    @__fault_tolerant\n",
    "    def filter_xgxx_type(cls, tar_xgxx):\n",
    "        '''\n",
    "        过滤相关信息\n",
    "        '''\n",
    "        risk = dict()\n",
    "        for each_distance, xgxx_statistics in cls.xgxx_distribution.iteritems():\n",
    "            tar_xgxx_statistics = dict([\n",
    "                    (each_xgxx_type, each_xgxx_num)\n",
    "                    for each_xgxx_type, each_xgxx_num \n",
    "                    in xgxx_statistics.iteritems() \n",
    "                    if each_xgxx_type in tar_xgxx])\n",
    "            risk[each_distance] = tar_xgxx_statistics\n",
    "        return risk\n",
    "    \n",
    "    @__fault_tolerant\n",
    "    def get_feature_10(cls):\n",
    "        '''\n",
    "        法律诉讼风险：开庭公告、裁判文书、法院公告、民间借贷\n",
    "        '''\n",
    "        tar_xgxx = ['ktgg', 'zgcpwsw', 'rmfygg', 'lending']\n",
    "        risk = cls.filter_xgxx_type(tar_xgxx)\n",
    "        \n",
    "        risk['r'] = round(\n",
    "            np.dot(\n",
    "                map(sum, [\n",
    "                        risk[each_distance].values() \n",
    "                        for each_distance in xrange(0, 4)]), \n",
    "                [1., 1/2., 1/3., 1/4.]), 2)\n",
    "        \n",
    "        if risk['r'] == 0:\n",
    "            risk['z'] = 10\n",
    "        elif 0 < risk['r'] <= 10:\n",
    "            risk['z'] = 30\n",
    "        elif 10 < risk['r'] <= 30:\n",
    "            risk['z'] = 60\n",
    "        elif 30 < risk['r']:\n",
    "            risk['z'] = 100\n",
    "        \n",
    "        return risk\n",
    "            \n",
    "    @__fault_tolerant\n",
    "    def get_feature_11(cls):\n",
    "        '''\n",
    "        行政处罚风险\n",
    "        '''\n",
    "        tar_xgxx = ['xzcf']\n",
    "        risk = cls.filter_xgxx_type(tar_xgxx)\n",
    "        \n",
    "        risk['r'] = round(\n",
    "            np.dot(\n",
    "                map(sum, [risk[each_distance].values() \n",
    "                        for each_distance in xrange(0, 4)]), \n",
    "                [1., 1/2., 1/3., 1/4.]), 2)\n",
    "        \n",
    "        if risk['r'] == 0:\n",
    "            risk['z'] = 10\n",
    "        elif 0 < risk['r'] <= 10:\n",
    "            risk['z'] = 30\n",
    "        elif 10 < risk['r'] <= 30:\n",
    "            risk['z'] = 60\n",
    "        elif 30 < risk['r']:\n",
    "            risk['z'] = 100\n",
    "        \n",
    "        return risk\n",
    "    \n",
    "    @__fault_tolerant\n",
    "    def get_feature_12(cls):\n",
    "        '''\n",
    "        被执行风险：被执行、失信被执行\n",
    "        '''\n",
    "        tar_xgxx = ['zhixing', 'dishonesty']\n",
    "        risk = cls.filter_xgxx_type(tar_xgxx)        \n",
    "        \n",
    "        risk['r'] = round(\n",
    "            np.dot(\n",
    "                map(\n",
    "                    lambda x: x['zhixing'] + 2. * x['dishonesty'], \n",
    "                    [risk[each_distance] for each_distance in xrange(0, 4)]), \n",
    "                [1., 1/2., 1/3., 1/4.]), 2)\n",
    "        \n",
    "        if risk['r'] == 0:\n",
    "            risk['z'] = 10\n",
    "        elif 0 < risk['r'] <= 10:\n",
    "            risk['z'] = 30\n",
    "        elif 10 < risk['r'] <= 30:\n",
    "            risk['z'] = 60\n",
    "        elif 30 < risk['r']:\n",
    "            risk['z'] = 100        \n",
    "        \n",
    "        return risk\n",
    "        \n",
    "    @__fault_tolerant\n",
    "    def get_feature_13(cls):\n",
    "        '''\n",
    "        异常经营风险\n",
    "        '''\n",
    "        tar_xgxx = ['jyyc', 'estatus']\n",
    "        risk = cls.filter_xgxx_type(tar_xgxx)        \n",
    "        \n",
    "        risk['r'] = round(\n",
    "            np.dot(\n",
    "                map(\n",
    "                    lambda x: x['jyyc'] + 2. * x['estatus'], \n",
    "                    [risk[each_distance] for each_distance in xrange(0, 4)]), \n",
    "                [1., 1/2., 1/3., 1/4.]), 2)\n",
    "\n",
    "        if risk['r'] == 0:\n",
    "            risk['z'] = 10\n",
    "        elif 0 < risk['r'] <= 10:\n",
    "            risk['z'] = 30\n",
    "        elif 10 < risk['r'] <= 30:\n",
    "            risk['z'] = 60\n",
    "        elif 30 < risk['r']:\n",
    "            risk['z'] = 100\n",
    "        \n",
    "        return risk        \n",
    "        \n",
    "    @__fault_tolerant\n",
    "    def get_feature_14(cls):\n",
    "        '''\n",
    "        银监会行政处罚\n",
    "        '''\n",
    "        tar_xgxx = ['circxzcf']\n",
    "        risk = cls.filter_xgxx_type(tar_xgxx)        \n",
    "        \n",
    "        risk['r'] = round(\n",
    "            np.dot(\n",
    "                map(\n",
    "                    lambda x: x['circxzcf'], \n",
    "                    [risk[each_distance] for each_distance in xrange(0, 4)]), \n",
    "                [1., 1/2., 1/3., 1/4.]), 2)\n",
    "\n",
    "        if risk['r'] == 0:\n",
    "            risk['z'] = 10\n",
    "        elif 0 < risk['r'] <= 10:\n",
    "            risk['z'] = 30\n",
    "        elif 10 < risk['r'] <= 30:\n",
    "            risk['z'] = 60\n",
    "        elif 30 < risk['r']:\n",
    "            risk['z'] = 100        \n",
    "    \n",
    "        return risk            \n",
    "        \n",
    "    @__fault_tolerant\n",
    "    def get_feature_15(cls):\n",
    "        '''\n",
    "        实际控制人风险\n",
    "        '''\n",
    "        def get_degree_distribution(is_human):\n",
    "            some_person_sets = [\n",
    "                node\n",
    "                for node, attr in cls.DIG.nodes_iter(data=True) \n",
    "                if attr['is_human'] == is_human\n",
    "                and attr['distance'] <= 3]\n",
    "            person_out_degree = cls.DIG.out_degree(some_person_sets).values()\n",
    "            if not person_out_degree:\n",
    "                person_out_degree.append(0)\n",
    "            return person_out_degree\n",
    "        \n",
    "        nature_person_distribution = get_degree_distribution(1)\n",
    "        legal_person_distribution = get_degree_distribution(0)\n",
    "        \n",
    "        nature_max_control = max(nature_person_distribution)\n",
    "        legal_max_control = max(legal_person_distribution)        \n",
    "        nature_avg_control = round(np.average(nature_person_distribution), 2)\n",
    "        legal_avg_control = round(np.average(legal_person_distribution), 2)\n",
    "        \n",
    "        total_legal_num = len([\n",
    "                node \n",
    "                for node, attr in cls.DIG.nodes_iter(data=True) \n",
    "                if attr['is_human'] == 0])\n",
    "        \n",
    "        risk = round(((\n",
    "                    2*(nature_max_control + legal_max_control) + \n",
    "                    (nature_avg_control + legal_avg_control)) /\n",
    "                (2*total_legal_num + 0.001)), 2)\n",
    "        \n",
    "        if 0 <= risk <= 0.5:\n",
    "            r = 10\n",
    "        elif 0.5 < risk <= 1:\n",
    "            r = 30\n",
    "        elif 1 < risk <= 3:\n",
    "            r = 60  \n",
    "        elif 3 < risk:\n",
    "            r = 100\n",
    "        else:\n",
    "            r = 10\n",
    "        \n",
    "        return dict(\n",
    "            x_1=nature_max_control,\n",
    "            x_2=legal_max_control,\n",
    "            y_1=nature_avg_control,\n",
    "            y_2=legal_avg_control,\n",
    "            z=total_legal_num,\n",
    "            s=risk,\n",
    "            r=r\n",
    "        )\n",
    "    \n",
    "    @__fault_tolerant\n",
    "    def get_feature_16(cls):\n",
    "        '''\n",
    "        公司扩张路径风险\n",
    "        '''\n",
    "        def get_node_set(is_human):\n",
    "            return Counter([\n",
    "                    attr['distance']\n",
    "                    for node, attr in cls.DIG.nodes_iter(data=True)\n",
    "                    if attr['is_human'] == is_human])\n",
    "        \n",
    "        nature_person_distribution = get_node_set(1)\n",
    "        legal_person_distribution = get_node_set(0)\n",
    "        \n",
    "        nature_person_num = [\n",
    "            nature_person_distribution.get(each_distance, 0)\n",
    "            for each_distance in range(1, 4)]\n",
    "        legal_person_num = [\n",
    "            legal_person_distribution.get(each_distance, 0)\n",
    "            for each_distance in range(1, 4)]\n",
    "        \n",
    "        risk = round(np.sum(\n",
    "                np.divide(\n",
    "                    [\n",
    "                        np.divide(\n",
    "                            np.sum(nature_person_num[:each_distance]), \n",
    "                            np.sum(legal_person_num[:each_distance]), \n",
    "                            dtype=float)\n",
    "                        for each_distance in range(1, 4)], \n",
    "                    np.array([1, 2, 3], dtype=float))), 2)\n",
    "        \n",
    "        if 0 <= risk <= 1:\n",
    "            r = 10\n",
    "        elif 1 < risk <= 3:\n",
    "            r = 30\n",
    "        elif 3 < risk <= 5:\n",
    "            r = 60  \n",
    "        elif 5 < risk:\n",
    "            r = 100\n",
    "        else:\n",
    "            r = 10\n",
    "        \n",
    "        return dict(\n",
    "            x_1=legal_person_num[0],\n",
    "            x_2=legal_person_num[1],\n",
    "            x_3=legal_person_num[2],\n",
    "            y_1=nature_person_num[0],\n",
    "            y_2=nature_person_num[1],\n",
    "            y_3=nature_person_num[2],\n",
    "            z=risk if risk < 10000 else 0,\n",
    "            r=r\n",
    "        )\n",
    "\n",
    "    @__fault_tolerant\n",
    "    def get_feature_17(cls):\n",
    "        '''\n",
    "        关联方中心集聚风险\n",
    "        '''\n",
    "        one_relation_set = [\n",
    "            node for node, attr in cls.DIG.nodes_iter(data=True) \n",
    "            if attr['distance'] == 1]\n",
    "        legal_person_shareholder = len([\n",
    "                src_node \n",
    "                for src_node, des_node, edge_attr \n",
    "                in cls.DIG.edges_iter(one_relation_set, data=True) \n",
    "                if des_node == cls.tarcompany \n",
    "                and edge_attr['is_invest'] == 'INVEST'\n",
    "                and cls.DIG.node[src_node]['is_human'] == 0])\n",
    "        legal_person_subsidiary = len([\n",
    "                des_node \n",
    "                for src_node, des_node, edge_attr \n",
    "                in cls.DIG.edges_iter(one_relation_set, data=True) \n",
    "                if src_node == cls.tarcompany \n",
    "                and edge_attr['is_invest'] == 'INVEST'\n",
    "                and cls.DIG.node[des_node]['is_human'] == 0])\n",
    "        \n",
    "        risk =(\n",
    "            legal_person_subsidiary -\n",
    "            legal_person_shareholder ) \n",
    "        \n",
    "        if risk < 0:\n",
    "            r = 10\n",
    "        elif risk == 0:\n",
    "            r = 30\n",
    "        elif 0 < risk <= 3:\n",
    "            r = 60\n",
    "        elif 3 < risk:\n",
    "            r = 100\n",
    "        else:\n",
    "            r = 100        \n",
    "        \n",
    "        return dict(\n",
    "            x=legal_person_subsidiary,\n",
    "            y=legal_person_shareholder,\n",
    "            z=risk,\n",
    "            r=r\n",
    "        )\n",
    "\n",
    "    @__fault_tolerant\n",
    "    def get_feature_18(cls):\n",
    "        '''\n",
    "        分支机构过度扩张风险\n",
    "        '''\n",
    "        risk = cls.resultiterable.data[0].a_fzjg\n",
    "        risk = risk if risk is not None else 0\n",
    "\n",
    "        if  risk == 0:\n",
    "            r = 10\n",
    "        elif risk == 1:\n",
    "            r = 30\n",
    "        elif 1 < risk <= 3:\n",
    "            r = 60  \n",
    "        elif 3 < risk:\n",
    "            r = 100\n",
    "        else:\n",
    "            r = 10\n",
    "        \n",
    "        return dict(\n",
    "            d=risk if risk < 100 else 100, \n",
    "            z=r\n",
    "        )    \n",
    "    \n",
    "    @__fault_tolerant\n",
    "    def get_feature_19(cls):\n",
    "        '''\n",
    "        关联方结构稳定风险\n",
    "        '''\n",
    "        def get_relation_num():\n",
    "            return Counter([\n",
    "                    attr['distance']\n",
    "                    for node, attr in cls.DIG.nodes(data=True)])\n",
    "        \n",
    "        #目标企业各个度的节点的数量\n",
    "        relations_num = get_relation_num()\n",
    "        relation_three_num = relations_num.get(3, 0)\n",
    "        relation_two_num = relations_num.get(2, 0)\n",
    "        relation_one_num = relations_num.get(1, 0)\n",
    "        relation_zero_num = relations_num.get(0, 1)\n",
    "        \n",
    "        x = np.array([\n",
    "                relation_zero_num, \n",
    "                relation_one_num, \n",
    "                relation_two_num, \n",
    "                relation_three_num]).astype(float)\n",
    "        \n",
    "        y_2 = x[2] / (x[1]+x[2])\n",
    "        y_3 = x[3] / (x[1]+x[2]+x[3])\n",
    "        risk = y_2/2 + y_3/3\n",
    "        \n",
    "\n",
    "        \n",
    "        if 0 <= risk <= 0.1:\n",
    "            r = 10\n",
    "        elif 0.1 < risk <= 0.3:\n",
    "            r = 30\n",
    "        elif 0.3 < risk <= 0.5:\n",
    "            r = 60  \n",
    "        elif 0.5 < risk:\n",
    "            r = 100      \n",
    "        else:\n",
    "            r = 10\n",
    "        \n",
    "        return dict(\n",
    "            x_1=x[1],\n",
    "            x_2=x[2],\n",
    "            x_3=x[3],\n",
    "            y_2=y_2,\n",
    "            y_3=y_3,\n",
    "            z=risk if risk else 0,\n",
    "            r=r\n",
    "        )        \n",
    "    \n",
    "    @__fault_tolerant\n",
    "    def get_feature_20(cls):\n",
    "        '''\n",
    "        潜在违规融资风险\n",
    "        '''\n",
    "\n",
    "        \n",
    "        def get_relation_risk_num(keyword_type):\n",
    "            return Counter([attr['distance']\n",
    "                    for node, attr in cls.DIG.nodes_iter(data=True) \n",
    "                    if attr['is_human'] == 0 and \n",
    "                    attr['opescope'] == keyword_type])\n",
    "        \n",
    "        k_2_num = get_relation_risk_num('k_2')\n",
    "        k_1_num = get_relation_risk_num('k_1')\n",
    "        \n",
    "        x = np.array([\n",
    "                k_2_num.get(each_distance, 0) \n",
    "                for each_distance in xrange(1, 4)])\n",
    "        y = np.array([\n",
    "                k_1_num.get(each_distance, 0) \n",
    "                for each_distance in xrange(1, 4)])\n",
    "    \n",
    "        risk = round(\n",
    "            np.sum(\n",
    "                np.divide(np.add(x, y), [1, 2, 3], dtype=float)), 2)\n",
    "    \n",
    "        if  risk == 0:\n",
    "            r = 10\n",
    "        elif 0 < risk < 5:\n",
    "            r = 30\n",
    "        elif 5 <= risk < 10:\n",
    "            r = 60  \n",
    "        elif 10 <= risk:\n",
    "            r = 100     \n",
    "        else:\n",
    "            r = 10\n",
    "    \n",
    "        return dict(\n",
    "            x_1=x[0],\n",
    "            x_2=x[1],\n",
    "            x_3=x[2],\n",
    "            y_1=y[0],\n",
    "            y_2=y[1],\n",
    "            y_3=y[2],\n",
    "            g=r\n",
    "        )\n",
    "    \n",
    "    @__fault_tolerant\n",
    "    def get_feature_21(cls):\n",
    "        '''\n",
    "        关联方地址集中度风险\n",
    "        '''\n",
    "        legal_person_address = [\n",
    "            attr['address'] \n",
    "            for node, attr in cls.DIG.nodes_iter(data=True) \n",
    "            if attr['distance'] <= 3 \n",
    "            and attr['is_human'] == 0]\n",
    "        c = Counter(\n",
    "            filter(\n",
    "                lambda x: x is not None and len(x) >= 21,  \n",
    "                legal_person_address))\n",
    "        n = c.most_common(1)\n",
    "        n= n[0][1] if len(n) > 0 else 1\n",
    "        risk = n -1\n",
    "        \n",
    "        if  risk == 0:\n",
    "            z = 10\n",
    "        elif risk == 1:\n",
    "            z = 30\n",
    "        elif risk == 2:\n",
    "            z = 60  \n",
    "        elif 2 < risk:\n",
    "            z = 100             \n",
    "        \n",
    "        return dict(\n",
    "            n=risk,\n",
    "            y=risk,\n",
    "            z=z\n",
    "        )\n",
    "    \n",
    "    @__fault_tolerant\n",
    "    def get_feature_22(cls):\n",
    "        '''\n",
    "        利益一致行动法人风险\n",
    "        '''\n",
    "        def is_similarity(str_1, str_2):\n",
    "            '''\n",
    "            判断两个字符串是否有连续2个字相同\n",
    "            '''\n",
    "            try:\n",
    "                token_1 = [\n",
    "                    str_1[index] + str_1[index+1] \n",
    "                    for index,data in enumerate(str_1) \n",
    "                    if index < len(str_1) - 1]\n",
    "                is_similarity = sum([\n",
    "                        1 for each_token in token_1 \n",
    "                        if each_token in str_2])\n",
    "                return True if is_similarity > 0 else False        \n",
    "            except:\n",
    "                return False\n",
    "            \n",
    "        #目标公司的名称frag\n",
    "        tar_company = cls.resultiterable.data[0].a_name\n",
    "        tar_company_frag = cls.resultiterable.data[0].a_namefrag\n",
    "        #三度以内，所有关联方的节点集合(不包含自身)\n",
    "        relation_set = [\n",
    "                attr['name'] \n",
    "                for node, attr in cls.DIG.nodes_iter(data=True) \n",
    "                if attr['distance'] <= 3]\n",
    "        #relation_set.remove(tar_company)\n",
    "        common_interests_list = [\n",
    "            1 \n",
    "            for node_name in relation_set \n",
    "            if is_similarity(tar_company_frag, node_name)]\n",
    "        \n",
    "        risk = sum(common_interests_list)\n",
    "        if risk == 0:\n",
    "            z = 10\n",
    "        elif risk == 1:\n",
    "            z = 30\n",
    "        elif 1 < risk <= 3:\n",
    "            z = 60\n",
    "        elif 3 < risk:\n",
    "            z = 100\n",
    "        else:\n",
    "            z = 10\n",
    "        \n",
    "        return dict(\n",
    "            d=risk, \n",
    "            z=z\n",
    "        )        \n",
    "        \n",
    "    @__fault_tolerant\n",
    "    def get_feature_23(cls):\n",
    "        '''\n",
    "        关联方信誉风险\n",
    "        '''\n",
    "        def get_black_num(distance):\n",
    "            '''\n",
    "            某一度黑企业的总数\n",
    "            '''\n",
    "            return sum([\n",
    "                    1\n",
    "                    for node, attr in cls.DIG.nodes_iter(data=True)\n",
    "                    if (attr['is_black'] and attr['distance'] == distance \n",
    "                            and attr['is_human'] == 0)\n",
    "            ])\n",
    "        \n",
    "        black_relation_set =[\n",
    "            get_black_num(each_distance) \n",
    "            for each_distance in range(1, 4)]\n",
    "        risk = np.dot(black_relation_set, [1, 1/2., 1/3.])\n",
    "        if risk == 0:\n",
    "            z = 10\n",
    "        elif 0 < risk < 0.5:\n",
    "            z = 30\n",
    "        elif 0.5 <= risk < 1:\n",
    "            z = 60\n",
    "        elif 1 <= risk:\n",
    "            z =100\n",
    "        else:\n",
    "            z = 10\n",
    "        \n",
    "        return dict(\n",
    "            b_1=black_relation_set[0],\n",
    "            b_2=black_relation_set[1],\n",
    "            b_3=black_relation_set[2],\n",
    "            y=risk,\n",
    "            z=z\n",
    "        )\n",
    "    \n",
    "    @__fault_tolerant\n",
    "    def get_feature_24(cls):\n",
    "        '''\n",
    "        短期逐利风险\n",
    "        '''\n",
    "        \n",
    "        def get_max_established(distance, timedelta):\n",
    "            '''\n",
    "            获取在某distance关联方企业中，某企业在任意timedelta内投资成立公司数量的最大值\n",
    "            '''\n",
    "            #用于获取最大连续时间数，这里i的取值要仔细琢磨一下，这里输入一个时间差序列与时间差\n",
    "            def get_date_density(difference_list, timedelta):\n",
    "                time_density_list = []\n",
    "                for index, date in enumerate(difference_list):\n",
    "                    if date < 15:\n",
    "                        s = 0\n",
    "                        i = 1\n",
    "                        while(s < 15 and i <= len(difference_list) - index):\n",
    "                            i += 1\n",
    "                            s = sum(difference_list[index:index+i])\n",
    "                        time_density_list.append(i)\n",
    "                    else:\n",
    "                        continue\n",
    "                return max(time_density_list) if len(time_density_list) >0 else 0            \n",
    "\n",
    "            #distance所有节点集合\n",
    "            relation_set = [\n",
    "                node \n",
    "                for node, attr in cls.DIG.nodes(data=True) \n",
    "                if attr['distance'] == distance]\n",
    "            investment_dict = defaultdict(lambda : [])     \n",
    "            \n",
    "            if len(cls.DIG.edge) > 1:\n",
    "                for src_node, des_node, edge_attr in (\n",
    "                        cls.DIG.edges_iter(relation_set, data=True)):\n",
    "                    if (edge_attr.get('is_invest', 0) == 'INVEST' \n",
    "                            and cls.DIG.node[des_node]['distance'] == distance\n",
    "                            and cls.DIG.node[des_node]['esdate'] is not None \n",
    "                            and cls.DIG.node[src_node]['is_human'] == 0):\n",
    "                        #将所有节点投资的企业的成立时间加进列表中\n",
    "                        investment_dict[src_node].append(cls.DIG.node[des_node]['esdate'])\n",
    "             \n",
    "            #目标企业所有节点所投资的企业时间密度字典\n",
    "            all_date_density_dict = {}\n",
    "            \n",
    "            for node, date_list in investment_dict.iteritems():\n",
    "                #构建按照时间先后排序的序列\n",
    "                date_strp_list = sorted(date_list)\n",
    "                if len(date_strp_list) > 1:\n",
    "                    #构建时间差的序列，例如：[256, 4, 5, 1, 2, 33, 6, 5, 4, 73]\n",
    "                    date_difference_list = [\n",
    "                        (date_strp_list[index + 1] - date_strp_list[index]).days \n",
    "                        for index in range(len(date_strp_list) -1)]\n",
    "                    #计算某法人节点在timedelta天之内有多少家公司成立\n",
    "                    es_num = get_date_density(date_difference_list, timedelta)\n",
    "                    if all_date_density_dict.has_key(es_num):\n",
    "                        all_date_density_dict[es_num].append(node)\n",
    "                    else:\n",
    "                        all_date_density_dict[es_num] = [node]              \n",
    "                else:\n",
    "                    continue\n",
    "            keys = all_date_density_dict.keys()        \n",
    "            max_num = max(keys) if len(keys) > 0 else 0\n",
    "            \n",
    "            return max_num\n",
    "        \n",
    "        x = [\n",
    "            get_max_established(each_distance, 180) \n",
    "            for each_distance in xrange(1, 4)]\n",
    "        \n",
    "        risk = round(\n",
    "            np.sum(\n",
    "                np.divide(x, [1, 2, 3], dtype=float)), 2)\n",
    "        \n",
    "        if  risk == 0:\n",
    "            r = 10\n",
    "        elif 0 < risk <= 0.7:\n",
    "            r = 30\n",
    "        elif 0.7 < risk <= 1:\n",
    "            r = 60  \n",
    "        elif 1 < risk:\n",
    "            r = 100       \n",
    "        else:\n",
    "            r = 10\n",
    "        \n",
    "        return dict(\n",
    "            x_1=x[0],\n",
    "            x_2=x[1],\n",
    "            x_3=x[2],\n",
    "            z=r,\n",
    "            d=risk\n",
    "        )\n",
    "    \n",
    "\n",
    "    @classmethod\n",
    "    def get_some_feature(cls, resultiterable, feature_nums):\n",
    "        #创建类变量\n",
    "        cls.resultiterable = resultiterable\n",
    "        cls.tarcompany = cls.resultiterable.data[0].a\n",
    "        cls.DIG = cls.create_graph_udf(cls.resultiterable, 1)\n",
    "        cls.xgxx_distribution = cls.get_feature_xgxx()\n",
    "        \n",
    "        feature_list = [\n",
    "            ('feature_{0}'.format(feature_index), \n",
    "                     eval('cls.get_feature_{0}()'.format(feature_index)))\n",
    "            for feature_index in feature_nums]\n",
    "        feature_list.append(('bbd_qyxx_id', cls.tarcompany))\n",
    "        feature_list.append(('company_name', cls.resultiterable.data[0].a_name))\n",
    "        \n",
    "        return json.dumps(dict(feature_list), ensure_ascii=False)      #\n",
    "\n",
    "class DynamicFeatureConstruction(FeatureConstruction):\n",
    "    '''\n",
    "    计算特征的函数集\n",
    "    '''      \n",
    "\n",
    "    def __fault_tolerant(func):\n",
    "        '''\n",
    "        一个用于容错的装饰器\n",
    "        '''\n",
    "        @classmethod\n",
    "        def wappen(cls, *args, **kwargs):\n",
    "            try:\n",
    "                return func(cls, *args, **kwargs)\n",
    "            except Exception, e:\n",
    "                return (\n",
    "                    \"{func_name} has a errr : {excp}\"\n",
    "                ).format(func_name=func.__name__, excp=e)\n",
    "        return wappen    \n",
    "    \n",
    "    @classmethod\n",
    "    def get_feature_25(cls):\n",
    "        '''\n",
    "        平台稳态运营风险\n",
    "        '''\n",
    "        nature_person_num = sum([\n",
    "                1\n",
    "                for node, attr in  cls.DIG.nodes_iter(data=True)\n",
    "                if (attr['is_human'] == 1\n",
    "                        and attr['distance'] <= 3)\n",
    "        ])\n",
    "        legal_person_num = sum([\n",
    "                1\n",
    "                for node, attr in  cls.DIG.nodes_iter(data=True)\n",
    "                if (attr['is_human'] == 0\n",
    "                        and attr['distance'] <= 3)\n",
    "        ])\n",
    "        return dict(\n",
    "            nature_person_num=nature_person_num,\n",
    "            legal_person_num=legal_person_num\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def get_feature_26(cls):\n",
    "        '''\n",
    "        平台核心资本运作风险\n",
    "        '''\n",
    "        some_person_sets = [\n",
    "            node\n",
    "            for node, attr in cls.DIG.nodes_iter(data=True) \n",
    "            if (attr['is_human'] == 1\n",
    "                    and attr['distance'] <= 3)]\n",
    "        person_out_degree = cls.DIG.out_degree(\n",
    "            some_person_sets).values()\n",
    "        person_out_degree.sort(reverse=True)\n",
    "        if not person_out_degree:\n",
    "            person_out_degree.append(0)\n",
    "\n",
    "        return dict(\n",
    "            kernel_control_num=sum(person_out_degree[:3])\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def get_feature_27(cls):\n",
    "        '''\n",
    "        可持续性风险\n",
    "        '''\n",
    "        def is_similarity(str_1, str_2):\n",
    "            '''\n",
    "            判断两个字符串是否有连续2个字相同\n",
    "            '''\n",
    "            try:\n",
    "                token_1 = [\n",
    "                    str_1[index] + str_1[index+1] \n",
    "                    for index,data in enumerate(str_1) \n",
    "                    if index < len(str_1) - 1]\n",
    "                is_similarity = sum([\n",
    "                        1 for each_token in token_1 \n",
    "                        if each_token in str_2])\n",
    "                return True if is_similarity > 0 else False        \n",
    "            except:\n",
    "                return False\n",
    "            \n",
    "        #目标公司的名称frag\n",
    "        tar_company = cls.resultiterable.data[0].a_name\n",
    "        tar_company_frag = cls.resultiterable.data[0].a_namefrag\n",
    "        #三度以内，所有关联方的节点集合(不包含自身)\n",
    "        relation_set = [\n",
    "                attr['name']\n",
    "                for node, attr in cls.DIG.nodes_iter(data=True) \n",
    "                if attr['distance'] <= 3]\n",
    "        #relation_set.remove(tar_company)\n",
    "        common_interests_num = sum([\n",
    "                1 \n",
    "                for node_name in relation_set \n",
    "                if is_similarity(tar_company_frag, node_name)\n",
    "        ])\n",
    "        total_legal_person_num = sum([\n",
    "                1\n",
    "                for node, attr in cls.DIG.nodes_iter(data=True) \n",
    "                if (attr['distance'] <= 3 and \n",
    "                       attr['is_human'] == 0)\n",
    "        ])\n",
    "        if total_legal_person_num:\n",
    "            ratio = round(\n",
    "                common_interests_num / total_legal_person_num, 2\n",
    "            )\n",
    "        else:\n",
    "            ratio = 0\n",
    "            \n",
    "        return dict(\n",
    "            ratio=ratio\n",
    "        )\n",
    "        \n",
    "    @classmethod\n",
    "    def get_feature_28(cls):\n",
    "        '''\n",
    "        平台跨区域舞弊风险\n",
    "        '''\n",
    "        province_list = filter(\n",
    "            None, [\n",
    "                attr['province']\n",
    "                for node, attr in cls.DIG.nodes_iter(data=True)\n",
    "                if (attr['distance'] <= 3 and \n",
    "                       attr['is_human'] == 0)]\n",
    "        )\n",
    "        province_distribution = Counter(province_list)\n",
    "        province_top4_num = sum(\n",
    "            map(lambda (k, v): v, province_distribution.most_common(4)))\n",
    "        \n",
    "        return dict(\n",
    "            province_top4_num=province_top4_num\n",
    "        )\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def get_some_feature(cls, resultiterable, feature_nums):\n",
    "        #创建类变量\n",
    "        cls.resultiterable = resultiterable\n",
    "        cls.tarcompany = cls.resultiterable.data[0].a\n",
    "        cls.DIG = cls.create_graph_udf(cls.resultiterable, 1)\n",
    "        cls.G = cls.create_graph_udf(cls.resultiterable, 0)\n",
    "        \n",
    "        feature_list = [\n",
    "            ('feature_{0}'.format(feature_index), \n",
    "                     eval('cls.get_feature_{0}()'.format(feature_index)))\n",
    "            for feature_index in feature_nums]\n",
    "        feature_list.append(('bbd_qyxx_id', cls.tarcompany))\n",
    "        feature_list.append(('company_name', cls.resultiterable.data[0].a_name))\n",
    "        \n",
    "        return dict(feature_list) #json.dumps(dict(feature_list), ensure_ascii=False)      \n",
    "\n",
    "    \n",
    "def get_dynamic_risk(data):\n",
    "    '''\n",
    "    计算动态风险\n",
    "    '''\n",
    "    old_data, new_data = data\n",
    "    #平台稳态运营风险\n",
    "    if old_data['feature_25']['nature_person_num']:\n",
    "        nature_person_risk = ((new_data['feature_25']['nature_person_num'] - \n",
    "                                            old_data['feature_25']['nature_person_num']) / \n",
    "                                            old_data['feature_25']['nature_person_num'])\n",
    "    else:\n",
    "        nature_person_risk = 0.\n",
    "    if old_data['feature_25']['legal_person_num']:\n",
    "        legal_person_risk = ((new_data['feature_25']['legal_person_num'] - \n",
    "                                          old_data['feature_25']['legal_person_num']) / \n",
    "                                          old_data['feature_25']['legal_person_num'])\n",
    "    else:\n",
    "        legal_person_risk = 0.\n",
    "    feature_25 = 5. * (nature_person_risk + legal_person_risk)\n",
    "    \n",
    "    #平台核心资本运作风险\n",
    "    if old_data['feature_26']['kernel_control_num']:\n",
    "        feature_26 = (15. * \n",
    "                              new_data['feature_26']['kernel_control_num'] / \n",
    "                              old_data['feature_26']['kernel_control_num'])\n",
    "    else:\n",
    "        feature_26 = 0.\n",
    "        \n",
    "    #可持续性风险\n",
    "    if old_data['feature_27']['ratio']:\n",
    "        feature_27 = (4. * \n",
    "                              new_data['feature_27']['ratio'] / \n",
    "                              old_data['feature_27']['ratio'])\n",
    "    else:\n",
    "        feature_27 = 0.\n",
    "        \n",
    "    #平台跨区域舞弊风险\n",
    "    if old_data['feature_28']['province_top4_num']:\n",
    "        feature_28 = (15. * \n",
    "                              new_data['feature_28']['province_top4_num'] / \n",
    "                              old_data['feature_28']['province_top4_num'])\n",
    "    else:\n",
    "        feature_28 = 0.\n",
    "        \n",
    "    return dict(\n",
    "        feature_25={'p': round(feature_25, 2)},\n",
    "        feature_26={'h': round(feature_26, 2)},\n",
    "        feature_27={'c': round(feature_27, 2)},\n",
    "        feature_28={'k': round(feature_28, 2)}\n",
    "    )\n",
    "    \n",
    "def spark_data_flow(tid_old_version, tid_new_version):\n",
    "    '''\n",
    "    dataflow\n",
    "    '''\n",
    "    conf = SparkConf()\n",
    "    conf.setMaster('yarn-client')\n",
    "    conf.set(\"spark.yarn.am.cores\", 7)\n",
    "    conf.set(\"spark.executor.memory\", \"50g\")\n",
    "    conf.set(\"spark.executor.instances\", 20)\n",
    "    conf.set(\"spark.executor.cores\", 10)\n",
    "    conf.set(\"spark.python.worker.memory\", \"2g\")\n",
    "    conf.set(\"spark.default.parallelism\", 1000)\n",
    "    conf.set(\"spark.sql.shuffle.partitions\", 1000)\n",
    "    conf.set(\"spark.broadcast.blockSize\", 1024)\n",
    "    conf.set(\"spark.executor.extraJavaOptions\",\"-XX:+PrintGCDetails -XX:+PrintGCTimeStamps\")    \n",
    "    conf.set(\"spark.shuffle.file.buffer\", '512k')\n",
    "    \n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"glf\") \\\n",
    "        .config(conf = conf) \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()  \n",
    "    \n",
    "    #输入\n",
    "    #算法设计直接用rdd\n",
    "    tid_old_df = spark.read.parquet(\n",
    "        \"/user/antifraud/jinli/tiddata/relation_company_info_merge/{version}\".format(version=tid_old_version))\n",
    "    tid_old_rdd = tid_old_df.rdd\n",
    "    tid_new_df = spark.read.parquet(\n",
    "        \"/user/antifraud/jinli/tiddata/relation_company_info_merge/{version}\".format(version=tid_new_version))\n",
    "    tid_new_rdd = tid_new_df.rdd    \n",
    "        \n",
    "    #最终计算流程\n",
    "    tid_new_rdd_2 = tid_new_rdd.map(lambda row: (row.a_name, row)) \\\n",
    "        .groupByKey() \\\n",
    "        .repartition(600) \\\n",
    "        .filter(lambda r: len(r[1].data) <= 150000) \\\n",
    "        .cache()\n",
    "    tid_old_rdd_2 = tid_old_rdd.map(lambda row: (row.a_name, row)) \\\n",
    "        .groupByKey() \\\n",
    "        .repartition(600) \\\n",
    "        .filter(lambda r: len(r[1].data) <= 150000) \\\n",
    "        .cache()\n",
    "\n",
    "    feature_new_list = tid_new_rdd_2.mapValues(\n",
    "        lambda v: DynamicFeatureConstruction.get_some_feature(\n",
    "            v, [_ for _ in range(25, 29)]))\n",
    "    feature_old_list = tid_old_rdd_2.mapValues(\n",
    "        lambda v: DynamicFeatureConstruction.get_some_feature(\n",
    "            v, [_ for _ in range(25, 29)]))\n",
    "    \n",
    "    dynamic_risk_data = feature_old_list.join(\n",
    "        feature_new_list\n",
    "    ).mapValues(\n",
    "        get_dynamic_risk\n",
    "    ).map(\n",
    "        lambda (k, v): dict({'company_name': k}, **v)\n",
    "    ).map(\n",
    "        lambda data: json.dumps(data, ensure_ascii=False)\n",
    "    )\n",
    "\n",
    "    return dynamic_risk_data\n",
    "\n",
    "def collect_to_driver(version, out_path, old_version, new_version):\n",
    "    '''\n",
    "    格式化输出\n",
    "    '''\n",
    "    src_time = time.time()\n",
    "    dynamic_risk_data = spark_data_flow(old_version, new_version)\n",
    "    os.system(\"hadoop fs -rmr {path}/{version}/prd_jinli_dynamic_feature_score_distribution_{version}\".format(path=OUTPATH, version=VERSION))\n",
    "    dynamic_risk_data.saveAsTextFile(\"{path}/{version}/prd_jinli_dynamic_feature_score_distribution_{version}\".format(path=OUTPATH, version=VERSION))\n",
    "\n",
    "    des_time = time.time()    \n",
    "    \n",
    "    print 'SUCCESS !! \\n 耗时：{0}'.format(des_time - src_time)\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':  \n",
    "    #输出参数\n",
    "    VERSION = \"20170417\"\n",
    "    OUTPATH = \"/user/antifraud/jinli/tmpdata\"\n",
    "    \n",
    "    collect_to_driver(\n",
    "        version=VERSION,\n",
    "        out_path=OUTPATH,\n",
    "        old_version='20170117',\n",
    "        new_version='20170403'\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|feature|\n",
      "+-------+\n",
      "| [0.42]|\n",
      "| [0.46]|\n",
      "| [0.55]|\n",
      "| [0.58]|\n",
      "| [0.25]|\n",
      "| [0.16]|\n",
      "|  [0.7]|\n",
      "| [0.25]|\n",
      "| [0.32]|\n",
      "| [0.55]|\n",
      "| [0.25]|\n",
      "| [0.73]|\n",
      "| [0.61]|\n",
      "| [0.13]|\n",
      "| [0.24]|\n",
      "| [0.51]|\n",
      "| [0.73]|\n",
      "|  [0.5]|\n",
      "| [0.56]|\n",
      "| [0.44]|\n",
      "| [0.59]|\n",
      "|  [0.1]|\n",
      "| [0.52]|\n",
      "|  [0.7]|\n",
      "| [0.15]|\n",
      "| [0.45]|\n",
      "|  [0.0]|\n",
      "| [0.48]|\n",
      "| [0.39]|\n",
      "| [0.33]|\n",
      "|  [0.0]|\n",
      "| [0.66]|\n",
      "| [0.38]|\n",
      "| [0.45]|\n",
      "| [0.57]|\n",
      "| [0.17]|\n",
      "| [0.25]|\n",
      "| [0.38]|\n",
      "| [0.61]|\n",
      "| [0.45]|\n",
      "| [0.65]|\n",
      "|  [0.0]|\n",
      "| [0.17]|\n",
      "|  [0.5]|\n",
      "|  [0.7]|\n",
      "| [0.41]|\n",
      "| [0.67]|\n",
      "| [0.17]|\n",
      "| [0.36]|\n",
      "| [0.26]|\n",
      "| [0.33]|\n",
      "| [0.32]|\n",
      "|  [0.2]|\n",
      "|  [0.0]|\n",
      "|  [0.7]|\n",
      "| [0.25]|\n",
      "|  [0.3]|\n",
      "| [0.63]|\n",
      "| [0.36]|\n",
      "| [0.25]|\n",
      "| [0.19]|\n",
      "| [0.28]|\n",
      "| [0.69]|\n",
      "| [0.49]|\n",
      "| [0.54]|\n",
      "| [0.39]|\n",
      "| [0.71]|\n",
      "| [0.42]|\n",
      "| [0.36]|\n",
      "| [0.77]|\n",
      "| [0.66]|\n",
      "| [0.18]|\n",
      "| [0.57]|\n",
      "| [0.42]|\n",
      "| [0.36]|\n",
      "|  [0.7]|\n",
      "|  [0.6]|\n",
      "| [0.74]|\n",
      "| [0.48]|\n",
      "| [0.68]|\n",
      "| [0.66]|\n",
      "| [0.63]|\n",
      "| [0.65]|\n",
      "| [0.19]|\n",
      "|  [0.3]|\n",
      "| [0.21]|\n",
      "| [0.65]|\n",
      "| [0.25]|\n",
      "| [0.36]|\n",
      "|  [0.0]|\n",
      "| [0.13]|\n",
      "|  [0.0]|\n",
      "| [0.75]|\n",
      "| [0.38]|\n",
      "| [0.36]|\n",
      "| [0.24]|\n",
      "| [0.69]|\n",
      "| [0.29]|\n",
      "| [0.17]|\n",
      "| [0.74]|\n",
      "+-------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "from pyspark.sql import Row\n",
    "\n",
    "schema = StructType(\n",
    "    [StructField('feature_19',FloatType(),True)])\n",
    "\n",
    "rdd = spark.sparkContext.textFile(\"/user/antifraud/jinli/tmpdata/20170413/prd_jinli_feature_score_distribution_20170413\")\n",
    "rdd = rdd.map(\n",
    "    json.loads\n",
    ").filter(\n",
    "    lambda r: isinstance(r['feature_19']['z'], float)\n",
    ").map(\n",
    "    lambda r: (Vectors.dense([round(np.nan_to_num(r['feature_19']['z']), 2)]), )\n",
    ").cache()\n",
    "\n",
    "#rdd.collect()\n",
    "df = spark.createDataFrame(rdd, ['feature'])\n",
    "df.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|feature|              scaled|\n",
      "+-------+--------------------+\n",
      "| [0.42]|[0.04718465138528...|\n",
      "| [0.46]|[0.2481008360802646]|\n",
      "| [0.55]|[0.7001622516439626]|\n",
      "| [0.58]|[0.8508493901651948]|\n",
      "| [0.25]|[-0.8067091335683...|\n",
      "| [0.16]|[-1.2587705491320...|\n",
      "|  [0.7]|[1.4535979442501252]|\n",
      "| [0.25]|[-0.8067091335683...|\n",
      "| [0.32]|[-0.4551058103521...|\n",
      "| [0.55]|[0.7001622516439626]|\n",
      "| [0.25]|[-0.8067091335683...|\n",
      "| [0.73]| [1.604285082771358]|\n",
      "| [0.61]|[1.0015365286864275]|\n",
      "| [0.13]|[-1.4094576876532...|\n",
      "| [0.24]|[-0.8569381797421...|\n",
      "| [0.51]|[0.49924606694898...|\n",
      "| [0.73]| [1.604285082771358]|\n",
      "|  [0.5]|[0.4490170207752413]|\n",
      "| [0.56]|[0.7503912978177069]|\n",
      "| [0.44]|[0.1476427437327761]|\n",
      "| [0.59]| [0.901078436338939]|\n",
      "|  [0.1]|[-1.560144826174527]|\n",
      "| [0.52]|[0.5494751131227298]|\n",
      "|  [0.7]|[1.4535979442501252]|\n",
      "| [0.15]|[-1.3089995953058...|\n",
      "| [0.45]|[0.19787178990652...|\n",
      "|  [0.0]|[-2.0624352879119...|\n",
      "| [0.48]|[0.3485589284277528]|\n",
      "| [0.39]|[-0.1035024871359...|\n",
      "| [0.33]|[-0.4048767641784...|\n",
      "|  [0.0]|[-2.0624352879119...|\n",
      "| [0.66]|[1.2526817595551487]|\n",
      "| [0.38]|[-0.1537315333096...|\n",
      "| [0.45]|[0.19787178990652...|\n",
      "| [0.57]|[0.8006203439914505]|\n",
      "| [0.17]|[-1.2085415029583...|\n",
      "| [0.25]|[-0.8067091335683...|\n",
      "| [0.38]|[-0.1537315333096...|\n",
      "| [0.61]|[1.0015365286864275]|\n",
      "| [0.45]|[0.19787178990652...|\n",
      "| [0.65]|[1.2024527133814045]|\n",
      "|  [0.0]|[-2.0624352879119...|\n",
      "| [0.17]|[-1.2085415029583...|\n",
      "|  [0.5]|[0.4490170207752413]|\n",
      "|  [0.7]|[1.4535979442501252]|\n",
      "| [0.41]|[-0.0030443947884...|\n",
      "| [0.67]| [1.302910805728893]|\n",
      "| [0.17]|[-1.2085415029583...|\n",
      "| [0.36]|[-0.2541896256571...|\n",
      "| [0.26]|[-0.7564800873946...|\n",
      "| [0.33]|[-0.4048767641784...|\n",
      "| [0.32]|[-0.4551058103521...|\n",
      "|  [0.2]|[-1.0578543644370...|\n",
      "|  [0.0]|[-2.0624352879119...|\n",
      "|  [0.7]|[1.4535979442501252]|\n",
      "| [0.25]|[-0.8067091335683...|\n",
      "|  [0.3]|[-0.5555639026996...|\n",
      "| [0.63]| [1.101994621033916]|\n",
      "| [0.36]|[-0.2541896256571...|\n",
      "| [0.25]|[-0.8067091335683...|\n",
      "| [0.19]|[-1.108083410610829]|\n",
      "| [0.28]|[-0.6560219950471...|\n",
      "| [0.69]| [1.403368898076381]|\n",
      "| [0.49]|[0.39878797460149...|\n",
      "| [0.54]|[0.6499332054702183]|\n",
      "| [0.39]|[-0.1035024871359...|\n",
      "| [0.71]|[1.5038269904238695]|\n",
      "| [0.42]|[0.04718465138528...|\n",
      "| [0.36]|[-0.2541896256571...|\n",
      "| [0.77]| [1.805201267466335]|\n",
      "| [0.66]|[1.2526817595551487]|\n",
      "| [0.18]|[-1.1583124567845...|\n",
      "| [0.57]|[0.8006203439914505]|\n",
      "| [0.42]|[0.04718465138528...|\n",
      "| [0.36]|[-0.2541896256571...|\n",
      "|  [0.7]|[1.4535979442501252]|\n",
      "|  [0.6]|[0.9513074825126833]|\n",
      "| [0.74]|[1.6545141289451022]|\n",
      "| [0.48]|[0.3485589284277528]|\n",
      "| [0.68]|[1.3531398519026372]|\n",
      "| [0.66]|[1.2526817595551487]|\n",
      "| [0.63]| [1.101994621033916]|\n",
      "| [0.65]|[1.2024527133814045]|\n",
      "| [0.19]|[-1.108083410610829]|\n",
      "|  [0.3]|[-0.5555639026996...|\n",
      "| [0.21]|[-1.0076253182633...|\n",
      "| [0.65]|[1.2024527133814045]|\n",
      "| [0.25]|[-0.8067091335683...|\n",
      "| [0.36]|[-0.2541896256571...|\n",
      "|  [0.0]|[-2.0624352879119...|\n",
      "| [0.13]|[-1.4094576876532...|\n",
      "|  [0.0]|[-2.0624352879119...|\n",
      "| [0.75]|[1.7047431751188464]|\n",
      "| [0.38]|[-0.1537315333096...|\n",
      "| [0.36]|[-0.2541896256571...|\n",
      "| [0.24]|[-0.8569381797421...|\n",
      "| [0.69]| [1.403368898076381]|\n",
      "| [0.29]|[-0.6057929488733...|\n",
      "| [0.17]|[-1.2085415029583...|\n",
      "| [0.74]|[1.6545141289451022]|\n",
      "+-------+--------------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "standardScaler = StandardScaler(inputCol=\"feature\", outputCol=\"scaled\", withStd=True, withMean=True)\n",
    "model = standardScaler.fit(df)\n",
    "scaledData = model.transform(df)\n",
    "scaledData.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04718469]\n",
      " [ 0.24810106]\n",
      " [ 0.70016289]\n",
      " [ 0.85085016]\n",
      " [-0.80670987]\n",
      " [-1.25877169]\n",
      " [ 1.45359926]\n",
      " [-0.80670987]\n",
      " [-0.45510622]\n",
      " [ 0.70016289]\n",
      " [-0.80670987]\n",
      " [ 1.60428654]\n",
      " [ 1.00153744]\n",
      " [-1.40945897]\n",
      " [-0.85693896]\n",
      " [ 0.49924652]\n",
      " [ 1.60428654]\n",
      " [ 0.44901743]\n",
      " [ 0.75039198]\n",
      " [ 0.14764288]\n",
      " [ 0.90107925]\n",
      " [-1.56014624]\n",
      " [ 0.54947561]\n",
      " [ 1.45359926]\n",
      " [-1.30900078]\n",
      " [ 0.19787197]\n",
      " [-2.06243716]\n",
      " [ 0.34855925]\n",
      " [-0.10350258]\n",
      " [-0.40487713]\n",
      " [-2.06243716]\n",
      " [ 1.2526829 ]\n",
      " [-0.15373167]\n",
      " [ 0.19787197]\n",
      " [ 0.80062107]\n",
      " [-1.2085426 ]\n",
      " [-0.80670987]\n",
      " [-0.15373167]\n",
      " [ 1.00153744]\n",
      " [ 0.19787197]\n",
      " [ 1.20245381]\n",
      " [-2.06243716]\n",
      " [-1.2085426 ]\n",
      " [ 0.44901743]\n",
      " [ 1.45359926]\n",
      " [-0.0030444 ]\n",
      " [ 1.30291199]\n",
      " [-1.2085426 ]\n",
      " [-0.25418986]\n",
      " [-0.75648077]\n",
      " [-0.40487713]\n",
      " [-0.45510622]\n",
      " [-1.05785533]\n",
      " [-2.06243716]\n",
      " [ 1.45359926]\n",
      " [-0.80670987]\n",
      " [-0.55556441]\n",
      " [ 1.10199562]\n",
      " [-0.25418986]\n",
      " [-0.80670987]\n",
      " [-1.10808442]\n",
      " [-0.65602259]\n",
      " [ 1.40337017]\n",
      " [ 0.39878834]\n",
      " [ 0.6499338 ]\n",
      " [-0.10350258]\n",
      " [ 1.50382836]\n",
      " [ 0.04718469]\n",
      " [-0.25418986]\n",
      " [ 1.80520291]\n",
      " [ 1.2526829 ]\n",
      " [-1.15831351]\n",
      " [ 0.80062107]\n",
      " [ 0.04718469]\n",
      " [-0.25418986]\n",
      " [ 1.45359926]\n",
      " [ 0.95130835]\n",
      " [ 1.65451563]\n",
      " [ 0.34855925]\n",
      " [ 1.35314108]\n",
      " [ 1.2526829 ]\n",
      " [ 1.10199562]\n",
      " [ 1.20245381]\n",
      " [-1.10808442]\n",
      " [-0.55556441]\n",
      " [-1.00762623]\n",
      " [ 1.20245381]\n",
      " [-0.80670987]\n",
      " [-0.25418986]\n",
      " [-2.06243716]\n",
      " [-1.40945897]\n",
      " [-2.06243716]\n",
      " [ 1.70474472]\n",
      " [-0.15373167]\n",
      " [-0.25418986]\n",
      " [-0.85693896]\n",
      " [ 1.40337017]\n",
      " [-0.6057935 ]\n",
      " [-1.2085426 ]\n",
      " [ 1.65451563]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "arry = df.rdd.map(lambda r: r.feature.toArray()).collect()\n",
    "scaler = preprocessing.StandardScaler().fit(arry)\n",
    "print scaler.transform(arry)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.filter()\n",
    "scaler_a = StandardScaler(inputCol=\"_1\", outputCol=\"scaledFeature\",\n",
    "                        withStd=True, withMean=False)\n",
    "scalerModel = scaler_a.fit(df)\n",
    "scaledData = scalerModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ".map(\n",
    "    lambda r: (Vectors.sparse(1, [(1, round(np.nan_to_num(r['feature_19']['z']), 2))]), )\n",
    ").cache()\n",
    "\n",
    "scaler_a = StandardScaler(inputCol=\"_1\", outputCol=\"scaledFeature\",\n",
    "                        withStd=True, withMean=False)\n",
    "scalerModel = scaler_a.fit(df)\n",
    "scaledData = scalerModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol=\"_1\", outputCol=\"scaledFeature\",\n",
    "                        withStd=True, withMean=False)\n",
    "scalerModel = scaler.fit(df)\n",
    "scaledData = scalerModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'bbd_qyxx_id': u'97e922cfd9f24068b6a289df82ab9a71',\n",
       "  u'company_name': u'\\u5c71\\u897f\\u946b\\u9686\\u5229\\u6e90\\u6295\\u8d44\\u6709\\u9650\\u516c\\u53f8',\n",
       "  u'feature_1': {u'r': 99.98, u'r_i': 0, u'w': 3, u'x': 0, u'y': 0, u'z': 3},\n",
       "  u'feature_10': {u'0': {u'ktgg': 0,\n",
       "    u'lending': 0,\n",
       "    u'rmfygg': 0,\n",
       "    u'zgcpwsw': 0},\n",
       "   u'1': {u'ktgg': 0, u'lending': 0, u'rmfygg': 0, u'zgcpwsw': 0},\n",
       "   u'2': {u'ktgg': 0, u'lending': 0, u'rmfygg': 0, u'zgcpwsw': 0},\n",
       "   u'3': {u'ktgg': 0, u'lending': 0, u'rmfygg': 0, u'zgcpwsw': 0},\n",
       "   u'r': 0.0,\n",
       "   u'z': 10},\n",
       "  u'feature_11': {u'0': {u'xzcf': 0},\n",
       "   u'1': {u'xzcf': 0},\n",
       "   u'2': {u'xzcf': 0},\n",
       "   u'3': {u'xzcf': 0},\n",
       "   u'r': 0.0,\n",
       "   u'z': 10},\n",
       "  u'feature_12': {u'0': {u'dishonesty': 0, u'zhixing': 0},\n",
       "   u'1': {u'dishonesty': 0, u'zhixing': 0},\n",
       "   u'2': {u'dishonesty': 0, u'zhixing': 0},\n",
       "   u'3': {u'dishonesty': 0, u'zhixing': 0},\n",
       "   u'r': 0.0,\n",
       "   u'z': 10},\n",
       "  u'feature_13': {u'0': {u'estatus': 0, u'jyyc': 0},\n",
       "   u'1': {u'estatus': 0, u'jyyc': 0},\n",
       "   u'2': {u'estatus': 2, u'jyyc': 0},\n",
       "   u'3': {u'estatus': 0, u'jyyc': 0},\n",
       "   u'r': 1.33,\n",
       "   u'z': 30},\n",
       "  u'feature_14': {u'0': {u'circxzcf': 0},\n",
       "   u'1': {u'circxzcf': 0},\n",
       "   u'2': {u'circxzcf': 0},\n",
       "   u'3': {u'circxzcf': 0},\n",
       "   u'r': 0.0,\n",
       "   u'z': 10},\n",
       "  u'feature_15': {u'r': 60,\n",
       "   u's': 1.15,\n",
       "   u'x_1': 5,\n",
       "   u'x_2': 0,\n",
       "   u'y_1': 1.5,\n",
       "   u'y_2': 0.0,\n",
       "   u'z': 5},\n",
       "  u'feature_16': {u'r': 100,\n",
       "   u'x_1': 0,\n",
       "   u'x_2': 4,\n",
       "   u'x_3': 0,\n",
       "   u'y_1': 3,\n",
       "   u'y_2': 0,\n",
       "   u'y_3': 5,\n",
       "   u'z': 0},\n",
       "  u'feature_17': {u'd': 0, u'z': 10},\n",
       "  u'feature_18': {u'r': 30, u'x': 0, u'y': 0, u'z': 0},\n",
       "  u'feature_19': {u'r': 60,\n",
       "   u'x_1': 3.0,\n",
       "   u'x_2': 4.0,\n",
       "   u'x_3': 5.0,\n",
       "   u'y_2': 0.5714285714285714,\n",
       "   u'y_3': 0.4166666666666667,\n",
       "   u'z': 0.4246031746031746},\n",
       "  u'feature_2': {u'c': 30.0, u'x': 1000.0, u'y': 0},\n",
       "  u'feature_20': {u'g': 10,\n",
       "   u'x_1': 0,\n",
       "   u'x_2': 0,\n",
       "   u'x_3': 0,\n",
       "   u'y_1': 0,\n",
       "   u'y_2': 0,\n",
       "   u'y_3': 0},\n",
       "  u'feature_21': {u'n': 0, u'y': 0, u'z': 10},\n",
       "  u'feature_22': {u'd': 1, u'z': 30},\n",
       "  u'feature_23': {u'b_1': 0, u'b_2': 0, u'b_3': 0, u'y': 0.0, u'z': 10},\n",
       "  u'feature_24': {u'd': 0.0, u'x_1': 0, u'x_2': 0, u'x_3': 0, u'z': 10},\n",
       "  u'feature_3': {u'j': 0.003, u'j_1': 19, u'j_2': 6382},\n",
       "  u'feature_4': {u'k': 100, u'k_1': 0, u'k_2': 0},\n",
       "  u'feature_5': {u'c_i': 0, u'l': 100.0, u'p_i': 0},\n",
       "  u'feature_6': {u'c_1': 0,\n",
       "   u'c_2': 0,\n",
       "   u'c_3': 0,\n",
       "   u'c_4': 0,\n",
       "   u'c_5': 0,\n",
       "   u'r': 0,\n",
       "   u'z': 10},\n",
       "  u'feature_7': {u'e': 0, u'e_1': 0, u'e_2': 0, u'e_3': 0, u'r': 0, u'z': 10},\n",
       "  u'feature_8': {u't': 4522.8, u'y': 10},\n",
       "  u'feature_9': {u'n': 100, u'n_1': 0, u'n_2': 0}}]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = spark.sparkContext.textFile(\"/user/antifraud/jinli/tmpdata/20170413/prd_jinli_feature_score_distribution_20170413\")\n",
    "rdd.map(\n",
    "    json.loads\n",
    ").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "|  1.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[151,152,153...|\n",
      "|  0.0|(692,[129,130,131...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[99,100,101,...|\n",
      "|  0.0|(692,[154,155,156...|\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[154,155,156...|\n",
      "|  0.0|(692,[153,154,155...|\n",
      "|  0.0|(692,[151,152,153...|\n",
      "|  1.0|(692,[129,130,131...|\n",
      "|  0.0|(692,[154,155,156...|\n",
      "|  1.0|(692,[150,151,152...|\n",
      "|  0.0|(692,[124,125,126...|\n",
      "|  0.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[97,98,99,12...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataFrame = spark.read.format(\"libsvm\").load(\"/user/antifraud/source/tmp_test/tmp_file/sample_libsvm_data.txt\")\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=False)\n",
    "\n",
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel = scaler.fit(dataFrame)\n",
    "\n",
    "# Normalize each feature to have unit standard deviation.\n",
    "scaledData = scalerModel.transform(dataFrame)\n",
    "scaledData.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "dataFrame = spark.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=False)\n",
    "\n",
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel = scaler.fit(dataFrame)\n",
    "\n",
    "# Normalize each feature to have unit standard deviation.\n",
    "scaledData = scalerModel.transform(dataFrame)\n",
    "scaledData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "data = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n",
    "        (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n",
    "        (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "pca = PCA(k=3, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(df)\n",
    "result = model.transform(df).select(\"pcaFeatures\")\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import types as tp, functions as fun\n",
    "\n",
    "def get_num(col):\n",
    "    import re\n",
    "    try:\n",
    "        return re.search('[\\d\\.\\,]+', col).group()\n",
    "    except:\n",
    "        return None\n",
    "get_num_udf = fun.udf(get_num, tp.StringType())\n",
    "\n",
    "df = spark.read.csv(\"/user/antifraud/source/tmp_test/tmp_file/res.csv\", sep=',')\n",
    "df = df.withColumn('_c7', get_num_udf('_c5'))\n",
    "df = df.withColumn('_c8', get_num_udf('_c6'))\n",
    "df.repartition(1).write.csv(\"/user/antifraud/source/tmp_test/tmp_file/res2.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/user/antifraud/hongjing2/dataflow/step_two/tid/nf_feature_preprocessing/20170518\")\n",
    "df.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|c_is_black_company|\n",
      "+------------------+\n",
      "|             false|\n",
      "|             false|\n",
      "|             false|\n",
      "|             false|\n",
      "|             false|\n",
      "|             false|\n",
      "|             false|\n",
      "|             false|\n",
      "|             false|\n",
      "|             false|\n",
      "|             false|\n",
      "|             false|\n",
      "|             false|\n",
      "|             false|\n",
      "|             false|\n",
      "|             false|\n",
      "|             false|\n",
      "|             false|\n",
      "|             false|\n",
      "|             false|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\n",
    "    '/user/antifraud/hongjing2/dataflow/step_one/tid/common_company_info_merge_v2/20170627'\n",
    ")\n",
    "\n",
    "df.select('c_is_black_company').where(df.c_is_black_company.).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      company_name|\n",
      "+------------------+\n",
      "|      北京花果信息技术有限公司|\n",
      "|      北京远山利丰科贸有限公司|\n",
      "|  北京镒金恒睿投资中心（有限合伙）|\n",
      "|        十月投资控股有限公司|\n",
      "|        北京光灿传媒有限公司|\n",
      "|    北京首科文创投资管理有限公司|\n",
      "|      北京成龙华天早餐有限公司|\n",
      "|      北京极客韦尔科技有限公司|\n",
      "|      北京汲古文化科技有限公司|\n",
      "|    宁波首科燕园科技发展有限公司|\n",
      "|  北京海科健融医疗投资管理有限公司|\n",
      "|    北京文信时空文化发展有限公司|\n",
      "|宁波首科淮海投资合伙企业（有限合伙）|\n",
      "|      中瑞富信资产管理有限公司|\n",
      "|      北京圣云奥海科技有限公司|\n",
      "|    博思智胜（北京）咨询有限公司|\n",
      "|发现创合（北京）文化传播有限责任公司|\n",
      "|     北京恒基发投资有限责任公司|\n",
      "|    北京锐圣尚坤科技发展有限公司|\n",
      "|      北京镒金投资管理有限公司|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw = spark.read.csv(\n",
    "    \"/user/antifraud/jinli/rawdata/tar_company/raw_jl_company_list_20171015_1.data\"\n",
    ").withColumnRenamed(\"_c0\", \"company_name\")\n",
    "\n",
    "raw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+--------------------+--------+----+------+--------+------------+--------------------+---------+------------+--------------------+--------------------+\n",
      "|        company_name|         bbd_qyxx_id|           risk_tags|risk_index|    risk_composition|province|city|county|is_black|company_type|           xgxx_info|risk_rank|data_version|          gmt_create|          gmt_update|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------+----+------+--------+------------+--------------------+---------+------------+--------------------+--------------------+\n",
      "|        上海枘琦信息科技有限公司|003c81736bc0467cb...|{\"--\": {\"静态关联方风险\"...|      30.0|{\"--\": {\"静态关联方风险\"...|     上海市| 上海市|   奉贤区|    null|        新兴金融|{\"行政处罚\": 0, \"分支机构...|     持续监控|    20170924|2017-10-11 18:37:...|2017-10-11 18:37:...|\n",
      "|     广西龙佳旺酒店投资有限责任公司|01eb7b691ad1457b9...|{\"--\": {\"静态关联方风险\"...|      30.1|{\"--\": {\"静态关联方风险\"...| 广西壮族自治区|防城港市|   港口区|    null|        新兴金融|{\"行政处罚\": 0, \"分支机构...|     重点关注|    20170924|2017-10-11 18:37:...|2017-10-11 18:37:...|\n",
      "|      北京康家智联科技发展有限公司|03b12d933162426d9...|{\"--\": {\"静态关联方风险\"...|      32.6|{\"--\": {\"静态关联方风险\"...|     北京市| 北京市|   海淀区|    null|        新兴金融|{\"行政处罚\": 0, \"分支机构...|     重点关注|    20170924|2017-10-11 18:37:...|2017-10-11 18:37:...|\n",
      "|    北京小牛网金信息服务有限责任公司|06216cb2d5b845bbb...|{\"--\": {\"静态关联方风险\"...|      30.9|{\"--\": {\"静态关联方风险\"...|     北京市| 北京市|   海淀区|    null|        新兴金融|{\"行政处罚\": 0, \"分支机构...|     重点关注|    20170924|2017-10-11 18:37:...|2017-10-11 18:37:...|\n",
      "|        上海仁今商务咨询有限公司|07256736b98f42f5b...|{\"--\": {\"静态关联方风险\"...|      39.4|{\"--\": {\"静态关联方风险\"...|     上海市| 上海市|  浦东新区|    null|        新兴金融|{\"行政处罚\": 0, \"分支机构...|     重点关注|    20170924|2017-10-11 18:37:...|2017-10-11 18:37:...|\n",
      "|      泉州市丰泽联利昌商贸有限公司|0880254881de49b38...|{\"--\": {\"静态关联方风险\"...|      30.0|{\"--\": {\"静态关联方风险\"...|     福建省| 泉州市|   丰泽区|    null|        新兴金融|{\"行政处罚\": 0, \"分支机构...|     持续监控|    20170924|2017-10-11 18:37:...|2017-10-11 18:37:...|\n",
      "|南京丹青一三四六号文化艺术品投资合...|0aad93868e664dbc9...|{\"--\": {\"静态关联方风险\"...|      30.0|{\"--\": {\"静态关联方风险\"...|     江苏省| 南京市|   秦淮区|    null|        新兴金融|{\"行政处罚\": 0, \"分支机构...|     持续监控|    20170924|2017-10-11 18:37:...|2017-10-11 18:37:...|\n",
      "|南京显龙九六二号文化艺术品投资合伙...|0be1ebd22fb149e29...|{\"--\": {\"静态关联方风险\"...|      30.0|{\"--\": {\"静态关联方风险\"...|     江苏省| 南京市|   秦淮区|    null|        新兴金融|{\"行政处罚\": 0, \"分支机构...|     持续监控|    20170924|2017-10-11 18:37:...|2017-10-11 18:37:...|\n",
      "|        合肥鼎南资产管理有限公司|0ce1d374f24349f98...|{\"--\": {\"静态关联方风险\"...|      30.0|{\"--\": {\"静态关联方风险\"...|     安徽省| 合肥市|   蜀山区|    null|        新兴金融|{\"行政处罚\": 0, \"分支机构...|     持续监控|    20170924|2017-10-11 18:37:...|2017-10-11 18:37:...|\n",
      "|    上海恒淳投资管理中心（有限合伙）|0e31cfef3b2947569...|{\"--\": {\"静态关联方风险\"...|      30.0|{\"--\": {\"静态关联方风险\"...|     上海市| 上海市|   崇明区|    null|        新兴金融|{\"行政处罚\": 0, \"分支机构...|     持续监控|    20170924|2017-10-11 18:37:...|2017-10-11 18:37:...|\n",
      "|     北京亚泰锋清雨投资管理有限公司|0fac2096a15447528...|{\"--\": {\"静态关联方风险\"...|      48.4|{\"--\": {\"静态关联方风险\"...|     北京市| 北京市|   朝阳区|    null|        新兴金融|{\"行政处罚\": 0, \"分支机构...|     重点关注|    20170924|2017-10-11 18:37:...|2017-10-11 18:37:...|\n",
      "|       云南鑫企安融资担保有限公司|11aeeea11dd54affb...|{\"--\": {\"静态关联方风险\"...|      36.7|{\"--\": {\"静态关联方风险\"...|     云南省| 昆明市|   官渡区|    null|        融资担保|{\"行政处罚\": 0, \"分支机构...|     重点关注|    20170924|2017-10-11 18:37:...|2017-10-11 18:37:...|\n",
      "|        四川道承投资管理有限公司|133c708049eb43a98...|{\"--\": {\"静态关联方风险\"...|      40.2|{\"--\": {\"静态关联方风险\"...|     四川省| 成都市|   金牛区|    null|        新兴金融|{\"行政处罚\": 0, \"分支机构...|     重点关注|    20170924|2017-10-11 18:37:...|2017-10-11 18:37:...|\n",
      "|        合肥汇量网络科技有限公司|1495965a6e804452b...|{\"--\": {\"静态关联方风险\"...|      30.0|{\"--\": {\"静态关联方风险\"...|     安徽省| 合肥市|   庐阳区|    null|        新兴金融|{\"行政处罚\": 0, \"分支机构...|     持续监控|    20170924|2017-10-11 18:37:...|2017-10-11 18:37:...|\n",
      "|        北京安立瑞达科技有限公司|15e161f7c6f043f19...|{\"--\": {\"静态关联方风险\"...|      30.0|{\"--\": {\"静态关联方风险\"...|     北京市| 北京市|   海淀区|    null|        新兴金融|{\"行政处罚\": 0, \"分支机构...|     持续监控|    20170924|2017-10-11 18:37:...|2017-10-11 18:37:...|\n",
      "|          河北鼎信投资有限公司|182e423747444dffb...|{\"--\": {\"静态关联方风险\"...|      39.0|{\"--\": {\"静态关联方风险\"...|     河北省|石家庄市|   桥西区|    null|        新兴金融|{\"行政处罚\": 0, \"分支机构...|     重点关注|    20170924|2017-10-11 18:37:...|2017-10-11 18:37:...|\n",
      "|      北京盛世中融投资咨询有限公司|1a986718708144f29...|{\"--\": {\"静态关联方风险\"...|      30.0|{\"--\": {\"静态关联方风险\"...|     北京市| 北京市|   丰台区|    null|        新兴金融|{\"行政处罚\": 0, \"分支机构...|     持续监控|    20170924|2017-10-11 18:37:...|2017-10-11 18:37:...|\n",
      "|       汕头市地衣资产管理有限公司|1d16f908f1cc46b4a...|{\"--\": {\"静态关联方风险\"...|      30.0|{\"--\": {\"静态关联方风险\"...|     广东省| 汕头市|   龙湖区|    null|        新兴金融|{\"行政处罚\": 0, \"分支机构...|     持续监控|    20170924|2017-10-11 18:37:...|2017-10-11 18:37:...|\n",
      "|     深圳市世纪银企投资咨询有限公司|1eeece97ffc34987a...|{\"--\": {\"静态关联方风险\"...|      30.0|{\"--\": {\"静态关联方风险\"...|     广东省| 深圳市|   福田区|    null|        新兴金融|{\"行政处罚\": 0, \"分支机构...|     持续监控|    20170924|2017-10-11 18:37:...|2017-10-11 18:37:...|\n",
      "|    运城市运隆晟小额贷款有限责任公司|21401c9661a946b9a...|{\"--\": {\"静态关联方风险\"...|      47.5|{\"--\": {\"静态关联方风险\"...|     山西省| 运城市|   盐湖区|    null|        小额贷款|{\"行政处罚\": 0, \"分支机构...|     重点关注|    20170924|2017-10-11 18:37:...|2017-10-11 18:37:...|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------+----+------+--------+------------+--------------------+---------+------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"/user/antifraud/hongjing2/dataflow/step_three/prd/all_company_info/20170924\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.join(\n",
    "    raw,\n",
    "    'company_name'\n",
    ").select(\n",
    "    'company_name',\n",
    "    'risk_index'\n",
    ").write.csv(\n",
    "    '/user/antifraud/source/tmp_test/tmp_file/tmp_20171015'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(company_name=u'\\u4e50\\u89c6\\u7f51\\u4fe1\\u606f\\u6280\\u672f\\uff08\\u5317\\u4eac\\uff09\\u80a1\\u4efd\\u6709\\u9650\\u516c\\u53f8', bbd_qyxx_id=u'ba369113a4c244608fb3541a4a5e6074'),\n",
       " Row(company_name=u'\\u4e39\\u4e1c\\u6e2f\\u96c6\\u56e2\\u6709\\u9650\\u516c\\u53f8', bbd_qyxx_id=u'ea17b8bac2bc48b093e52b754265a127')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "company = [\n",
    "    u'丹东港集团有限公司',\n",
    "    u'乐视网信息技术（北京）股份有限公司'\n",
    "]\n",
    "\n",
    "tar_df = spark.sparkContext.parallelize(company).map(lambda r: Row(company_name=r)).toDF()\n",
    "\n",
    "basic_df = spark.sql(\n",
    "    '''\n",
    "    select company_name,bbd_qyxx_id from dw.qyxx_basic where dt='20171030'\n",
    "    '''\n",
    ")\n",
    "\n",
    "tar_df.join(\n",
    "    basic_df,\n",
    "    'company_name'\n",
    ").take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "version_list = [\n",
    "    '20170117'\n",
    "    ,'20170221'\n",
    "    ,'20170325'\n",
    "    ,'20170424'\n",
    "    ,'20170518'\n",
    "    ,'20170627'\n",
    "    ,'20170720'\n",
    "    ,'20170825'\n",
    "    ,'20170925'\n",
    "    ,'20171025'\n",
    "]\n",
    "\n",
    "for version in  version_list:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+---------+-------+--------------------+--------+\n",
      "|       id|         bbd_qyxx_id|         bbd_xgxx_id|bbd_table|id_type|         create_time|      dt|\n",
      "+---------+--------------------+--------------------+---------+-------+--------------------+--------+\n",
      "|159236463|d289e76df0534ec2b...|056e6b1d04cd8743a...|     ktgg|      0|2016-08-04 10:14:...|20171204|\n",
      "|159236465|a802b9a2f43b4f68a...|056e6b1d04cd8743a...|     ktgg|      0|2016-08-04 10:14:...|20171204|\n",
      "|159236466|7a7be27abff74a038...|056e6b1d04cd8743a...|     ktgg|      0|2016-08-04 10:14:...|20171204|\n",
      "|159236467|996f4bbd84df4073b...|056e6b1d04cd8743a...|     ktgg|      1|2016-08-04 10:14:...|20171204|\n",
      "+---------+--------------------+--------------------+---------+-------+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\n",
    "    '''\n",
    "    select * from ods.xgxx_relation where dt=20171204\n",
    "    '''\n",
    ")\n",
    "\n",
    "df.where(\n",
    "    df.bbd_xgxx_id == '056e6b1d04cd8743a93601d08565d9d9'\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+---------+-------+--------+----------+---------------+\n",
      "| id|         bbd_qyxx_id|         bbd_xgxx_id|bbd_table|id_type|      dt|event_time|event_timestamp|\n",
      "+---+--------------------+--------------------+---------+-------+--------+----------+---------------+\n",
      "|   |d289e76df0534ec2b...|056e6b1d04cd8743a...|     ktgg|      0|20171117|2016-06-24|     1466697600|\n",
      "+---+--------------------+--------------------+---------+-------+--------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet('/user/wanxiang/tmpdata/tid_xgxx_relation_df/20171117')\n",
    "df.where(\n",
    "    df.bbd_xgxx_id == '056e6b1d04cd8743a93601d08565d9d9'\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+---+--------------------+----+\n",
      "|                 _c0|       _c1|_c2|                 _c3| _c4|\n",
      "+--------------------+----------+---+--------------------+----+\n",
      "|d289e76df0534ec2b...|1512462194|  0|056e6b1d04cd8743a...|KTGG|\n",
      "|7a7be27abff74a038...|1512462194|  0|056e6b1d04cd8743a...|KTGG|\n",
      "|996f4bbd84df4073b...|1512462194|  1|056e6b1d04cd8743a...|KTGG|\n",
      "|a802b9a2f43b4f68a...|1512462194|  0|056e6b1d04cd8743a...|KTGG|\n",
      "+--------------------+----------+---+--------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"/user/wanxiang/step_two/20171204/event_edge\")\n",
    "\n",
    "df.where(\n",
    "    df._c3 == '056e6b1d04cd8743a93601d08565d9d9'\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 10.0, 5000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 104.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), bbd_qyxx_id=u'07ca6b04163e4807b212fb9b96b48f87', company_name=u'\\u8fdc\\u76db\\u519c\\u7267\\u79d1\\u6280\\uff08\\u6d77\\u5357\\uff09\\u6709\\u9650\\u516c\\u53f8', scaledFeatures=DenseVector([0.0008, -0.1215, -0.0651, -0.1261, 0.0, 0.0, 0.0, -0.0328, -0.0781, -0.1616, -0.1825, -0.0607, -0.1455, 0.0, -0.1965, -0.0279, -0.0349, -0.1828, -0.0447, -0.1252, -0.1857, -0.0221, -0.2233, -0.0522, -0.2197, 1.0626, -0.1962, -0.1999, -0.7406, -0.2111, -1.272, -1.1416, -1.6513, 0.0258, -0.0033, -0.2311, -0.1021, -0.0414, -0.0728, -0.2198, -0.1111, -0.1661, -0.2198, -0.3353, -0.15, -0.3425, -0.1347, -0.2773, -0.3826, -0.5324, 0.0, 0.0, 0.0, 0.0, -0.9742, -1.0223, -0.4698, -0.072, -0.0074, -0.0467, 0.35])),\n",
       " Row(features=DenseVector([1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.01, 11.0, 2.25, 0.0, 11.0, 0.8462, 0.4348, 100.0, 10.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 191.0, 2946.82, 0.0, 0.0, 0.0, 0.0, 0.0]), bbd_qyxx_id=u'142117c4cdb54966b21c412ca03e0929', company_name=u'\\u5370\\u5373\\uff08\\u4e0a\\u6d77\\uff09\\u751f\\u7269\\u79d1\\u6280\\u6709\\u9650\\u516c\\u53f8', scaledFeatures=DenseVector([0.0008, -0.1215, -0.0651, -0.1261, 0.0, 0.0, 0.0, -0.0328, -0.0781, -0.1616, -0.1825, -0.0607, -0.1455, 0.0, 0.8151, -0.0279, -0.0349, -0.1828, -0.0447, -0.1252, -0.1857, -0.0221, -0.2233, -0.0522, -0.2197, 0.0263, 0.1116, 0.2588, -0.7406, -0.0441, 1.2414, 0.234, 0.8441, -0.0536, -0.0033, -0.2311, -0.1021, -0.0414, -0.0728, -0.2198, -0.1111, -0.1661, -0.2198, -0.3353, -0.15, -0.3425, -0.1347, -0.2773, -0.3826, -0.5324, 0.0, 0.0, 0.0, 0.0, -0.9117, 0.8965, -0.4698, -0.072, -0.0074, -0.0467, 0.35])),\n",
       " Row(features=DenseVector([1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 30.0, 1000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 202.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), bbd_qyxx_id=u'24e552f1f5934193849ceadf1c2a0be7', company_name=u'\\u53a6\\u95e8\\u6052\\u6cb3\\u6c99\\u6570\\u636e\\u670d\\u52a1\\u6709\\u9650\\u516c\\u53f8', scaledFeatures=DenseVector([0.0008, -0.1215, -0.0651, -0.1261, 0.0, 0.0, 0.0, -0.0328, -0.0781, -0.1616, -0.1825, -0.0607, -0.1455, 0.0, -0.1965, -0.0279, -0.0349, -0.1828, -0.0447, -0.1252, -0.1857, -0.0221, -0.2233, -0.0522, -0.2197, 1.0626, -0.1962, -0.1999, -0.7406, -0.2111, -1.272, -1.1416, -1.0968, -0.0379, -0.0033, -0.2311, -0.1021, -0.0414, -0.0728, -0.2198, -0.1111, -0.1661, -0.2198, -0.3353, -0.15, -0.3425, -0.1347, -0.2773, -0.3826, -0.5324, 0.0, 0.0, 0.0, 0.0, -0.9038, -1.0223, -0.4698, -0.072, -0.0074, -0.0467, 0.35]))]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.parquet(\"/user/antifraud/hongjing2/dataflow/step_two/tid/nf_feature_preprocessing/20180227\")\n",
    "df.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------------+------------------+----------------------------+---------------------------+--------------------+--------------------+------------------+\n",
      "|  GM_behavior_risk|GM_company_strength_risk|    GM_credit_risk|GM_dynamic_relationship_risk|GM_static_relationship_risk|         bbd_qyxx_id|        company_name|       total_score|\n",
      "+------------------+------------------------+------------------+----------------------------+---------------------------+--------------------+--------------------+------------------+\n",
      "| 46.98555658016041|       63.31955352863837|46.910781430052324|           49.99924375714049|         43.039050757196605|4d0d6afe64bd40748...|      永安市安立信企业管理有限公司| 43.34999493748262|\n",
      "|51.924864700431236|      38.944979187839515|44.672319498943686|          49.353070100043986|          63.49531613502696|4d0d6e2e15fb4be4b...|       温州心净莲文化发展有限公司| 33.40753364228069|\n",
      "|  52.6784356378773|       69.89132298589371| 84.06120723053986|          48.532138964951564|          45.08357354810399|4d0d96652aaf4b16a...|    昆明市晋宁县宇生小额贷款有限公司| 61.85203596048979|\n",
      "| 56.84684231970897|       38.94551002744298|44.672319498943686|                50.175617401|          57.62726445209665|4d0d988f96c646c7b...|        上海银瑞投资咨询有限公司|39.100824070650596|\n",
      "| 55.13771134228986|       38.94484647844122| 45.13475558737514|           42.07482875468524|          44.71462357578691|4d0dd012dd7d4aae9...|    澈明（上海）投资中心（有限合伙）|30.092188633172718|\n",
      "| 48.48820716260281|        38.9448862912396|44.672319498943686|                50.175617401|          63.49531613502696|4d0dd638ceb649b19...|        青岛鑫淼商务咨询有限公司| 41.81526052064274|\n",
      "|  57.8281092962452|       38.94750070460119|44.672319498943686|          49.846433322363914|          59.89616907183463|4d0de2fa9656456b8...|        杭州瓦图投资管理有限公司|52.773810265040254|\n",
      "| 47.34276384768837|       48.32650107542851| 44.67263676748066|           50.26005476116092|          31.00633026942211|4d0deb3a3370443ba...|      重庆第一国际教育投资有限公司|27.865114858255566|\n",
      "| 52.00498862108086|       38.94484647844122|44.672319498943686|                50.175617401|          42.75111393278897|4d0dee15d96841fb8...|南京任聚五九三号文化艺术品投资合伙...| 35.89192952844869|\n",
      "| 45.97303839453419|       52.83969865331003|44.672319498943686|            66.3058455212141|           64.8363414550072|4d0e1ed845634407a...|       山东普渡号文化发展有限公司|41.729635000379716|\n",
      "| 56.62525187541664|       38.94551002744298|44.672319498943686|                50.175617401|          61.29587007904915|4d0e38749be743938...|       东莞市搏轩实业投资有限公司| 50.35535473556839|\n",
      "| 44.18467841197039|       38.94551002744298| 69.04084594232492|           49.76416982871659|         63.190927559761235|4d0e4a25ebbd43969...|        桑植县撒禾坪煤业有限公司|42.774561246115965|\n",
      "|53.922293759812526|      38.946173581470354|44.672319498943686|          48.532138964951564|          61.29587007904915|4d0e55f96a824950a...|      曲靖捷铭经济信息咨询有限公司|36.045265922502416|\n",
      "|58.138000867391405|        69.9161511729537| 66.03236631800695|                50.175617401|          65.12216553606632|4d0e5659172441c98...|        中金普华资产管理有限公司| 68.90267766859122|\n",
      "| 52.14038614684845|       63.31955352863837|44.672319498943686|                50.175617401|          61.29587007904915|4d0e616211144f0f9...|    辽宁众祥普惠金融服务外包有限公司| 38.71307996734338|\n",
      "|  72.0265973321604|        69.9161511729537|44.672319498943686|                50.175617401|          61.61660125372346|4d0e67736fa74e4fb...|        北京距石投资管理有限公司| 71.86431895796832|\n",
      "|51.050863261403144|       63.34667361510804|48.611690470743156|           50.34894889496293|          62.71595798579609|4d0e80e7a5734d6c9...|         山东金利联投资有限公司| 58.23658904229396|\n",
      "| 53.66412229400335|       63.33311447694408|  74.8984088604162|           48.87555914522209|          38.47581228772502|4d0e83270d744bd8a...|        宁夏红阳小额贷款有限公司|  51.3033161856515|\n",
      "| 48.53421635256243|       38.94484647844122|44.672319498943686|                50.175617401|          61.29587007904915|4d0e984bb74944d48...|华泰证券股份有限公司麻城融辉路证券营业部| 38.94625694658525|\n",
      "| 43.48093446771931|      38.946173581470354| 45.90033032685248|           66.80842014868364|          36.48433284420107|4d0eab9510b7483fa...|    幸福侯彩擂（上海）茶文化有限公司|29.135898862840353|\n",
      "+------------------+------------------------+------------------+----------------------------+---------------------------+--------------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"/user/antifraud/hongjing2/dataflow/step_two/prd/nf_feature_risk_score/20180227\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+------------------+-----------+---------+-----+-------------------+-----------------+--------------------+\n",
      "|                   b|                   c|              b_name|            c_name|bc_relation|role_name|ratio|regno_or_creditcode|            regno|                name|\n",
      "+--------------------+--------------------+--------------------+------------------+-----------+---------+-----+-------------------+-----------------+--------------------+\n",
      "|4b747cf333044a4ea...|82e3b988d9784d3fb...|  西安市木可焊接科技有限公司第一分公司|     西安市木可焊接科技有限公司|     BRANCH|     分支机构|    -| 91610139MA6TX14L4G|  610139200000809|  西安市木可焊接科技有限公司第一分公司|\n",
      "|9e6300948e004b0f9...|f47ed50a4903448c8...|重庆群联物业管理有限责任公司南坪直...|    重庆群联物业管理有限责任公司|     BRANCH|     分支机构|    -|                   |  500108300020536|重庆群联物业管理有限责任公司南坪直...|\n",
      "|e5a4d96186f344e88...|f7f47551076f430ea...|    浙江民政礼仪用品经营服务公司分部|    浙江民政礼仪用品经营服务公司|     BRANCH|     分支机构|    -|      3301001703411|    3301001703411|    浙江民政礼仪用品经营服务公司分部|\n",
      "|a1bfc5342a524064b...|c01f3060c8904cbf9...|北京中复电讯设备有限责任公司增光路...|    北京中复电讯设备有限责任公司|     BRANCH|     分支机构|    -|                   |  110108005646123|北京中复电讯设备有限责任公司增光路...|\n",
      "|99f2b075ae65433ca...|94bc13ae9722497d9...|  祥云县汇优快递有限公司祥云一部分公司|       祥云县汇优快递有限公司|     BRANCH|     分支机构|    -|                   |                 |  祥云县汇优快递有限公司祥云一部分公司|\n",
      "|3e5385d5a1ea421b9...|6199ddee1c3848e1a...|中国石化销售有限公司河南开封第十六加油站|        中国石化销售有限公司|     BRANCH|     分支机构|    -|    410202002027116|  410202002027116|中国石化销售有限公司河南开封第十六加油站|\n",
      "|                None|e8766e8650ff4c729...|    福州多妮妮洗涤有限公司湖前经营部|       福州多妮妮洗涤有限公司|     BRANCH|     分支机构|    -|                   |                 |    福州多妮妮洗涤有限公司湖前经营部|\n",
      "|00d6a66607004f1cb...|65f8fc4f65ff435eb...| 安徽新建基础工程有限责任公司安庆分公司|    安徽新建基础工程有限责任公司|     BRANCH|     分支机构|    -|                   |  340800000009574| 安徽新建基础工程有限责任公司安庆分公司|\n",
      "|52eb583de14648b3a...|82a1f93ef4a944458...|广州市国营珠江华侨农工商联合公司南...|  广州市国营珠江华侨农工商联合公司|     BRANCH|     分支机构|    -|      4401101400070|    4401101400070|广州市国营珠江华侨农工商联合公司南...|\n",
      "|e32c1446c1c0490f9...|25a705357bcb4a8cb...|       海南电教设备器材公司配件部|        海南电教设备器材公司|     BRANCH|     分支机构|    -|                   |       98408237-8|       海南电教设备器材公司配件部|\n",
      "|f27e7564897d440ca...|dc37d89f3bfe4389b...|郑州市全日鲜生活便利店有限公司济源分公司|   郑州市全日鲜生活便利店有限公司|     BRANCH|     分支机构|    -|      4108812600111|    4108812600111|郑州市全日鲜生活便利店有限公司济源分公司|\n",
      "|561a810301214b93b...|3cba3778b0b6479fb...|深圳中诚信达股权投资基金管理有限公...|深圳中诚信达股权投资基金管理有限公司|     BRANCH|     分支机构|    -|                   |                 |深圳中诚信达股权投资基金管理有限公...|\n",
      "|e3b14b39fc8a4e078...|264c6bb771ed492da...|   丹东东源运输有限公司辽F10592|        丹东东源运输有限公司|     BRANCH|     分支机构|    -|    210600600033138|  210600600033138|   丹东东源运输有限公司辽F10592|\n",
      "|c49748a456ec4a019...|e82019f2142b47888...|     衡阳市联友贸易有限公司一分公司|       衡阳市联友贸易有限公司|     BRANCH|     分支机构|    -|      4304002500255|    4304002500255|     衡阳市联友贸易有限公司一分公司|\n",
      "|                None|e2479117cbfe4d158...|     福州创想科技有限公司建瓯分公司|        福州创想科技有限公司|     BRANCH|     分支机构|    -|                   |                 |     福州创想科技有限公司建瓯分公司|\n",
      "|3ce11f51eecf4b958...|f357bc42a5b14d7c9...|北京美佳基业房地产经纪有限公司第五分公司|   北京美佳基业房地产经纪有限公司|     BRANCH|     分支机构|    -|    110112013291620|  110112013291620|北京美佳基业房地产经纪有限公司第五分公司|\n",
      "|7ed42bbf9edc46c8b...|a3d5301b54a391c98...|        湖北省烟草公司荆州市公司|           湖北省烟草公司|     BRANCH|     分支机构|    -|                   |  421000000000359|        湖北省烟草公司荆州市公司|\n",
      "|bc4e3af814874b8f8...|1c0c436febce49a2b...|     上海豪坤装潢材料有限公司分公司|      上海豪坤装潢材料有限公司|     BRANCH|     分支机构|    -|                   |31022720270500001|     上海豪坤装潢材料有限公司分公司|\n",
      "|92be51ba8011480b8...|2c21686a626847e8a...|  北京东方海航商贸有限责任公司七分公司|    北京东方海航商贸有限责任公司|     BRANCH|     分支机构|    -| 911101057921081740|  110105009852196|  北京东方海航商贸有限责任公司七分公司|\n",
      "|                None|624d2bdad04442928...| 上海国际医学中心有限公司欣健医药分公司|      上海国际医学中心有限公司|     BRANCH|     分支机构|    -| 91310115MA1H9A5D9H|                 | 上海国际医学中心有限公司欣健医药分公司|\n",
      "+--------------------+--------------------+--------------------+------------------+-----------+---------+-----+-------------------+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "XGXX_RELATION = '20180305'\n",
    "raw_qyxx_fzjg_merge = spark.sql(\n",
    "    '''\n",
    "    SELECT\n",
    "    bbd_branch_id b,\n",
    "    bbd_qyxx_id c,\n",
    "    name b_name,\n",
    "    company_name c_name,\n",
    "    'BRANCH' bc_relation,\n",
    "    '分支机构' role_name,\n",
    "    '-' ratio,\n",
    "    regno_or_creditcode,\n",
    "    regno, \n",
    "    name\n",
    "    FROM\n",
    "    dw.qyxx_fzjg_merge\n",
    "    WHERE\n",
    "    dt='{version}'\n",
    "    '''.format(version=XGXX_RELATION)\n",
    ")\n",
    "raw_qyxx_fzjg_merge.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "from functools import partial\n",
    "import hashlib\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import functions as fun, types as tp\n",
    "\n",
    "def filter_comma(col):\n",
    "    '''ID中逗号或为空值，则将该记录删除'''\n",
    "    if not col or ',' in col or u'\\uff0c' in col:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def filter_chinaese(col):\n",
    "    '''字段中只要包含中文，将其过滤'''\n",
    "    if col:\n",
    "        match = re.search(ur'[\\u4e00-\\u9fa5]', col)\n",
    "        return False if match else True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_id(src, des, relation, position):\n",
    "    '''\n",
    "    生成规则：md5（起点name+终点id+关系类型+关系名）\n",
    "    由于源ID有中文，因此这里需要做容错\n",
    "    '''\n",
    "    try:\n",
    "        role_id = hashlib.md5(src.encode('utf-8') + \n",
    "                              des.encode('utf-8') +\n",
    "                              relation.encode('utf-8') +\n",
    "                              position.encode('utf-8'))\n",
    "        return role_id.hexdigest()\n",
    "    except:\n",
    "        return ''\n",
    "        \n",
    "def get_branch_id(regno_or_creditcode, \n",
    "                  regno, \n",
    "                  name):\n",
    "    '''\n",
    "    生成规则：md5（起点name+终点id+关系类型+关系名）\n",
    "    由于源ID有中文，因此这里需要做容错\n",
    "    '''\n",
    "    try:\n",
    "        role_id = hashlib.md5(regno_or_creditcode.encode('utf-8') + \n",
    "                              regno.encode('utf-8') +\n",
    "                              name.encode('utf-8') +\n",
    "                              \"qyxx_fzjg_merge\")\n",
    "        return role_id.hexdigest()\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "def is_invest(col):\n",
    "    if 'INVEST' in col:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_role_label(label):\n",
    "    return 'Entity;Role;{0}'.format(label.lower().capitalize())\n",
    "\n",
    "def get_chinese(string):\n",
    "    '''获得中文字符串'''\n",
    "    try:\n",
    "        result = re.findall(ur'[\\u4e00-\\u9fa5]', string)\n",
    "        if result:\n",
    "            return ''.join(result)\n",
    "        else:\n",
    "            return '-'\n",
    "    except:\n",
    "        return '-'\n",
    "\n",
    "\n",
    "filter_chinaese_udf = fun.udf(filter_chinaese, tp.BooleanType())\n",
    "filter_comma_udf = fun.udf(filter_comma, tp.BooleanType())\n",
    "get_role_label_udf = fun.udf(get_role_label, \n",
    "                             tp.StringType())\n",
    "get_isinvest_label_udf = fun.udf(\n",
    "    partial(get_role_label, 'Isinvest'), tp.StringType())\n",
    "get_id_udf = fun.udf(get_id, tp.StringType())\n",
    "get_branch_id_udf = fun.udf(get_branch_id, tp.StringType())\n",
    "get_isinvest_id_udf = fun.udf(\n",
    "    partial(get_id, relation='Isinvest', position=''), tp.StringType())\n",
    "is_invest_udf = fun.udf(is_invest, tp.BooleanType())\n",
    "get_chinese_udf = fun.udf(get_chinese, tp.StringType())\n",
    "\n",
    "get_relation_label_3_udf = fun.udf(\n",
    "        partial(lambda r: r, 'VIRTUAL'), tp.StringType())    \n",
    "\n",
    "\n",
    "get_relation_label_3_udf = fun.udf(\n",
    "    partial(lambda r: r, 'VIRTUAL'), tp.StringType())    \n",
    "\n",
    "tid_qyxx_fzjg_merge = raw_qyxx_fzjg_merge.where(\n",
    "        filter_comma_udf('b')\n",
    "    ).where(\n",
    "        filter_chinaese_udf('b')\n",
    "    ).where(\n",
    "        filter_comma_udf('c')\n",
    "    ).where(\n",
    "        filter_chinaese_udf('c')\n",
    "    ).where(\n",
    "        filter_comma_udf('b_name')\n",
    "    ).where(\n",
    "        filter_comma_udf('c_name')\n",
    "    ).select(\n",
    "        fun.when(\n",
    "            raw_qyxx_fzjg_merge.b == 'None',\n",
    "            get_branch_id_udf('regno_or_creditcode', 'regno', 'name')\n",
    "        ).otherwise(\n",
    "            raw_qyxx_fzjg_merge.b        \n",
    "        ).alias('b'),\n",
    "        'b_name',\n",
    "        get_id_udf('b_name', 'c', \n",
    "                   'bc_relation', 'role_name').alias('bbd_role_id:ID'),\n",
    "        'c',\n",
    "        'c_name',\n",
    "        'bc_relation',\n",
    "        'role_name',\n",
    "        'ratio',\n",
    "    ).dropDuplicates(\n",
    "        ['b', 'c', 'bc_relation']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5028976"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = spark.read.parquet(\"/user/wanxiang/tmpdata/tid_qyxx_fzjg_merge/20180304\")\n",
    "tmp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+-------------+----------+------------+----------+-----------+-------------------+--------------+----------------+-------------------------+----------------+------------+----------------+--------------+------------------+-----------------+----------+----+------+--------------------+----------+----------------+------------------+-----------+----------+----------+--------------------+----------------+-----------+-------+--------------+----------------+--------------------+-------------+---------------+---------+---------------+-------------------+-----------+----------+----+----------------+------------+-------------+--------------------+---------------+---------------+-------------+--------+\n",
      "|  id|         bbd_qyxx_id|             address|approval_date|bbd_dotime|    bbd_type|bbd_uptime|cancel_date|company_companytype|company_county|company_currency|company_enterprise_status|company_industry|company_name|company_province|company_regorg|       credit_code|enterprise_status|    esdate|form|frname|           frname_id|invest_cap|investcap_amount|investcap_currency|ipo_company|  openfrom|    opento|       operate_scope|operating_period|parent_firm|realcap|realcap_amount|realcap_currency|              regcap|regcap_amount|regcap_currency|regcapcur|          regno|regno_or_creditcode|revoke_date|    regorg|type|bbd_history_name|company_type|frname_compid|         create_time|company_gis_lat|company_gis_lon|bbdgis_dotime|      dt|\n",
      "+----+--------------------+--------------------+-------------+----------+------------+----------+-----------+-------------------+--------------+----------------+-------------------------+----------------+------------+----------------+--------------+------------------+-----------------+----------+----+------+--------------------+----------+----------------+------------------+-----------+----------+----------+--------------------+----------------+-----------+-------+--------------+----------------+--------------------+-------------+---------------+---------+---------------+-------------------+-----------+----------+----+----------------+------------+-------------+--------------------+---------------+---------------+-------------+--------+\n",
      "|null|5e1562de3f1a4386b...|深圳市前海深港合作区前湾一路1号A...|   2018-02-02|2018-03-05|qyxy_element|1520228162|       null|               1100|        440305|             人民币|                       存续|               L|核新产融（深圳）有限公司|              广东|        440300|91440300MA5DCTNGX2|           在营（开业）|2016-05-17|null|    雷达|dd6c0c9edbe491328...|      null|            null|              null|       null|2016-05-17|9999-12-31|许可经营项目：投资咨询、经济信息咨...|            null|       null|   []万元|          null|             人民币|5000.000000万元（币种：...|        5.0E7|            人民币|     人民币元|440301116176780|               null|       null|深圳市市场监督管理局|元素征信|              []|      有限责任公司|            1|2018-03-05 13:36:...|           null|           null|         null|20180305|\n",
      "+----+--------------------+--------------------+-------------+----------+------------+----------+-----------+-------------------+--------------+----------------+-------------------------+----------------+------------+----------------+--------------+------------------+-----------------+----------+----+------+--------------------+----------+----------------+------------------+-----------+----------+----------+--------------------+----------------+-----------+-------+--------------+----------------+--------------------+-------------+---------------+---------+---------------+-------------------+-----------+----------+----+----------------+------------+-------------+--------------------+---------------+---------------+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\n",
    "    '''\n",
    "    select * from dw.qyxx_basic where dt=20180305 and bbd_qyxx_id='5e1562de3f1a4386bad38a0e9128a437'\n",
    "    '''\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|feature_10.0.zgcpwsw_1|\n",
      "+----------------------+\n",
      "|                     0|\n",
      "|                     0|\n",
      "|                     0|\n",
      "|                     0|\n",
      "|                     0|\n",
      "|                     0|\n",
      "|                     0|\n",
      "|                     0|\n",
      "|                     1|\n",
      "|                     0|\n",
      "|                     0|\n",
      "|                     0|\n",
      "|                     0|\n",
      "|                     0|\n",
      "|                     1|\n",
      "|                     0|\n",
      "|                     0|\n",
      "|                     1|\n",
      "|                     0|\n",
      "|                     0|\n",
      "+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kpi = spark.read.json(\"/user/antifraud/jinli/prddata/prd_jinli_feature_score_distribution_20180312_02\")\n",
    "kpi.select(\n",
    "    kpi.feature_10.getField('0').getField('zgcpwsw_1')\n",
    ").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
